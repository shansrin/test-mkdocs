{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Coherence Operator Documentation Oracle enables organizations using Coherence to move their clusters into the cloud. By supporting industry standards, such as Docker and Kubernetes, Oracle facilitates running Coherence on cloud-neutral infrastructure. In addition, Oracle provides an open-source Coherence Operator (\"the operator\"), which implements features to assist with deploying and managing Coherence clusters in a Kubernetes environment. You can: Run Coherence within the de facto standard Kubernetes container orchestration framework, using Docker containers for the members of a Coherence cluster. Use popular industry standard tools such as Grafana , ELK (or more specifically the EFK stack including Fluentd), and Prometheus to monitor the performance, logs and and health of your clusters. Flexibly override and customize cluster configuration. Scale the Coherence deployment. Use Coherence*Extend to access your cluster with a variety of clients. Use Kubernetes Zone information to ensure data stored in Coherence is resilient to loss of a Zone. Coherence goes to great efforts to ensure data is safe across processes, machines, racks and sites. When Coherence is deployed to Kubernetes with the Coherence Operator, data will be spread across zones to ensure this underlying principle is supported; thus by default, loss of any zone is a tolerated failure mode. This is reflected in the StatusHA value (SITE-SAFE) for partitioned services, in addition to the member level site information that is equivalent to the kubernetes zone label on the associated pod. Start clusters based on declarative startup parameters and desired states. Use Kubernetes persistent volumes when using Coherence's disk-based storage features Elastic Data or Persistence. Deploy custom code for your EntryProcessor classes and other server-side Coherence constructs. The fastest way to experience the operator is to follow the Quick Start guide , or try out the samples . About This Documentation This documentation includes sections targeted to different audiences. To help you find what you are looking for more easily, consult this table of contents: The Quick Start Guide explains how to quickly get Coherence running on Kubernetes, using the defaults, nothing special. The User Guide contains detailed usage information on the Coherence Operator, including how to install and configure the operator and several common use cases. The Samples provide detailed example code and instructions that show you how to perform various tasks related to the operator. The Developer Guide provides details for users who want to understand how the operator is built, tested, and so on. Those who wish to contribute to the operator code will find useful information here. The Access the EFK (Elasticsearch, Fluentd and Kibana) Stack to View Logs page describes how to enable log capture, and manage data logging through the EFK stack to view logs. The Monitor Coherence Services via Grafana Dashboards page explains how to configure the Prometheus Operator and monitor Coherence services through Grafana dashboards. User Guide The User Guide provides detailed information on all aspects of using the operator including: Installing and configuring the operator. Using the operator to create and manage Coherence clusters. Manually creating Coherence clusters to be managed by the operator. Configuring Elasticsearch and Kibana to access the operator's log files. Shutting down clusters. And much more! Samples Refer to our samples for information about the available sample code. Things to Keep In Mind for Existing Coherence Users Software running in Kubernetes must provide \"health checks\" so that Kubernetes can make informed decisions about starting, stopping, or even killing, the containers running the software. The operator provides everything required to do this for Coherence. Keep in mind that these health checks cause frequent MemberJoined and MemberLeft events to happen. If these events refer to something like OracleCoherenceK8sPodChecker , they are normal and be safely ignored. Need more help? Have a suggestion? Come and say \"Hello!\" Join Our Public Slack Channel We have a public Slack channel where you can get in touch with us to ask questions about using the operator or give us feedback or suggestions about what features and improvements you would like to see. We would love to hear from you. To join our channel, please visit this site to get an invitation . The invitation email will include details of how to access our Slack workspace. After you are logged in, please come to #operator and say, \"hello!\"","title":"Home"},{"location":"#coherence-operator-documentation","text":"Oracle enables organizations using Coherence to move their clusters into the cloud. By supporting industry standards, such as Docker and Kubernetes, Oracle facilitates running Coherence on cloud-neutral infrastructure. In addition, Oracle provides an open-source Coherence Operator (\"the operator\"), which implements features to assist with deploying and managing Coherence clusters in a Kubernetes environment. You can: Run Coherence within the de facto standard Kubernetes container orchestration framework, using Docker containers for the members of a Coherence cluster. Use popular industry standard tools such as Grafana , ELK (or more specifically the EFK stack including Fluentd), and Prometheus to monitor the performance, logs and and health of your clusters. Flexibly override and customize cluster configuration. Scale the Coherence deployment. Use Coherence*Extend to access your cluster with a variety of clients. Use Kubernetes Zone information to ensure data stored in Coherence is resilient to loss of a Zone. Coherence goes to great efforts to ensure data is safe across processes, machines, racks and sites. When Coherence is deployed to Kubernetes with the Coherence Operator, data will be spread across zones to ensure this underlying principle is supported; thus by default, loss of any zone is a tolerated failure mode. This is reflected in the StatusHA value (SITE-SAFE) for partitioned services, in addition to the member level site information that is equivalent to the kubernetes zone label on the associated pod. Start clusters based on declarative startup parameters and desired states. Use Kubernetes persistent volumes when using Coherence's disk-based storage features Elastic Data or Persistence. Deploy custom code for your EntryProcessor classes and other server-side Coherence constructs. The fastest way to experience the operator is to follow the Quick Start guide , or try out the samples .","title":"Coherence Operator Documentation"},{"location":"#about-this-documentation","text":"This documentation includes sections targeted to different audiences. To help you find what you are looking for more easily, consult this table of contents: The Quick Start Guide explains how to quickly get Coherence running on Kubernetes, using the defaults, nothing special. The User Guide contains detailed usage information on the Coherence Operator, including how to install and configure the operator and several common use cases. The Samples provide detailed example code and instructions that show you how to perform various tasks related to the operator. The Developer Guide provides details for users who want to understand how the operator is built, tested, and so on. Those who wish to contribute to the operator code will find useful information here. The Access the EFK (Elasticsearch, Fluentd and Kibana) Stack to View Logs page describes how to enable log capture, and manage data logging through the EFK stack to view logs. The Monitor Coherence Services via Grafana Dashboards page explains how to configure the Prometheus Operator and monitor Coherence services through Grafana dashboards.","title":"About This Documentation"},{"location":"#user-guide","text":"The User Guide provides detailed information on all aspects of using the operator including: Installing and configuring the operator. Using the operator to create and manage Coherence clusters. Manually creating Coherence clusters to be managed by the operator. Configuring Elasticsearch and Kibana to access the operator's log files. Shutting down clusters. And much more!","title":"User Guide"},{"location":"#samples","text":"Refer to our samples for information about the available sample code.","title":"Samples"},{"location":"#things-to-keep-in-mind-for-existing-coherence-users","text":"Software running in Kubernetes must provide \"health checks\" so that Kubernetes can make informed decisions about starting, stopping, or even killing, the containers running the software. The operator provides everything required to do this for Coherence. Keep in mind that these health checks cause frequent MemberJoined and MemberLeft events to happen. If these events refer to something like OracleCoherenceK8sPodChecker , they are normal and be safely ignored.","title":"Things to Keep In Mind for Existing Coherence Users"},{"location":"#need-more-help-have-a-suggestion-come-and-say-hello","text":"","title":"Need more help? Have a suggestion? Come and say \"Hello!\""},{"location":"#join-our-public-slack-channel","text":"We have a public Slack channel where you can get in touch with us to ask questions about using the operator or give us feedback or suggestions about what features and improvements you would like to see. We would love to hear from you. To join our channel, please visit this site to get an invitation . The invitation email will include details of how to access our Slack workspace. After you are logged in, please come to #operator and say, \"hello!\"","title":"Join Our Public Slack Channel"},{"location":"about/","text":"","title":"About"},{"location":"developer/","text":"Developer Guide The Developer Guide provides information for developers who want to build, install, and test the operator. After successfully completing the steps in this guide, you can execute the instructions in the Quick Start Guide and User Guide . Prerequisites See Before You Begin section in the Quick Start guide. In addition, you require the following software versions for the build environment: Mac OS 10.13.6 Docker Desktop 2.0.0.3 (31259). Channel: stable, 8858db33c8, with Kubernetes v1.10.11. Oracle JDK 11.0.1 2018-10-16 LTS Apache Maven 3.5.4 Note : If you are using other operating systems or Docker versions, you need to make the necessary adjustments to execute the steps in this guide. Verify System Environment Check whether your environment is configured with the following software for building and installing the operator: Software Verify Expected Output Docker $ docker run hello world Hello from Docker! Kubernetes $ kubectl version Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"clean\", BuildDate:\"2019-02-04T04:49:22Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"darwin/amd64\"}<br>Server Version: version.Info{Major:\"1\", Minor:\"10\", GitVersion:\"v1.10.11\", GitCommit:\"637c7e288581ee40ab4ca210618a89a555b6e7e9\", GitTreeState:\"clean\", BuildDate:\"2018-11-26T14:25:46Z\", GoVersion:\"go1.9.3\", Compiler:\"gc\", Platform:\"linux/amd64\"} Helm $ helm version Client: &version.Version{SemVer:\"v2.12.3\", GitCommit:\"eecf22f77df5f65c823aacd2dbd30ae6c65f186e\", GitTreeState:\"clean\"}<br>Server: &version.Version{SemVer:\"v2.12.3\", GitCommit:\"eecf22f77df5f65c823aacd2dbd30ae6c65f186e\", GitTreeState:\"clean\"} Java java version java version \"11.0.1\" 2018-10-16 LTS Maven mvn version Apache Maven 3.5.4 (1edded0938998edf8bf061f1ceb3cfdeccf443fe; 2018-06-17T14:33:14-04:00) <br> Maven home: /Users/username/Downloads/apache-maven-3.5.4 <br> Java version: 11.0.1, vendor: Oracle Corporation, runtime: /Library/Java/JavaVirtualMachines/jdk-11.0.1.jdk/Contents/Home Build the Operator The operator is built using Apache Maven. To build the operator without running any tests, do the following: Clone and check out the current version of the operator from the GitHub repository . Create a Maven settings.xml file. If you already have one, ensure that the following settings are included in your default profile: xml <properties> <test.image.prefix>DOCKER_REPO_HOSTNAME/DOCKER_REPO_PREFIX/dev/DEV_USERNAME/</test.image.prefix> </properties> Note : All the Maven commands in this guide use this settings.xml file. In the settings.xml file: * DOCKER_REPO_HOSTNAME is the hostname of the Docker repository in which you will push your built Docker images. Note : You are not required to push any images when executing the steps in this document. DOCKER_REPO_PREFIX is your organization who owns the repository on Docker Hub. DEV_USERNAME is a username unique to your development environment. In this example, YOUR_test.image.prefix_VALUE is the value used for test.image.prefix property in the settings.xml file. Obtain a Coherence 12.2.1.3.2 Docker image and tag it correctly. Refer to the section Obtain Images from Oracle Container Registry to pull the Coherence Docker image from the Oracle Container Registry. bash docker pull container-registry.oracle.com/middleware/coherence:12.2.1.3.2 Tag the obtained image in the way it is required to build the operator. Obtain the image ID for the Coherence 12.2.1.3.2 Docker image: bash $ docker images | grep 12.2.1.3.2 Tag the obtained image: bash docker tag some_tag YOUR_test.image.prefix_VALUE/oracle/coherence:12.2.1.3.2 In this example, some_tag is the tag given to identify the pulled image. Again execute the command to list the image ID for Coherence: ```bash $ docker images | grep 12.2.1.3.2 YOUR_test.image.prefix_VALUE/oracle/coherence 12.2.1.3.2 7e7feca04384 2 months ago 547MB ``` 5. From the root directory of your cloned repository, build the operator using the following commands: bash $ mvn -DskipTests clean install This produces output similar to the following: bash ... [INFO] ------------------------------------------------------------------------ [INFO] Reactor Summary: [INFO] [INFO] coherence-operator parent VERSION ........... SUCCESS [ 2.487 s] [INFO] coherence-operator ................................. SUCCESS [ 21.651 s] [INFO] coherence-utils .................................... SUCCESS [ 22.868 s] [INFO] coherence-operator-tests VERSION ............ SUCCESS [ 11.468 s] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 58.756 s [INFO] Finished at: 2019-04-17T18:35:14-04:00 [INFO] ------------------------------------------------------------------------ Note : The VERSION in the output will be similar to 1.0.0-SNAPSHOT . mvn -DskipTests generate-resources This produces output similar to the output of the preceding step. mvn -DskipTests -Pdocker clean install The output of this command has the following message: bash ... Successfully built af61471e4774 Successfully tagged YOUR_test.image.prefix_VALUE/oracle/coherence-operator:VERSION ... Successfully built 88495a497a16 Successfully tagged YOUR_test.image.prefix_VALUE/oracle/coherence-utils:VERSION Note : The VERSION in the output will be similar to 1.0.0-SNAPSHOT . Verify that the Docker images have been built and are accessible to your local Docker server. bash $ docker images | grep YOUR_test.image.prefix_VALUE This produces output similar to the following: bash YOUR_test.image.prefix_VALUE/oracle/coherence-utils VERSION 88495a497a16 14 minutes ago 124MB YOUR_test.image.prefix_VALUE/oracle/coherence-operator VERSION af61471e4774 14 minutes ago 537MB YOUR_test.image.prefix_VALUE/oracle/coherence 12.2.1.3.2 7e7feca04384 2 months ago 547MB Note : The VERSION in the output will be similar to 1.0.0-SNAPSHOT . Verify that the Coherence Helm chart and the Coherence Operator Helm chart have been built and are accessible in your work area. bash $ ls -la operator/target | grep \"drw\" | grep coherence This produces output similar to the following: bash drwxr-xr-x 3 username staff 96 Apr 17 18:38 coherence-VERSION-helm drwxr-xr-x 3 username staff 96 Apr 17 18:38 coherence-operator-VERSION-helm If you want to use the build image as the source while executing the steps in the Quick Start Guide and User Guide , replace the Helm repository prefix with the full qualified path: coherence/coherence - /Users/username/workareas/coherence-operator/target/coherence-1.0.0-SNAPSHOT-helm/coherence coherence/coherence-operator - /Users/username/workareas/coherence-operator/target/coherence-operator-1.0.0-SNAPSHOT-helm/coherence-operator Note: It is assumed that the Coherence Operator is built within /Users/username/workareas/coherence-operator . The VERSION in the output will be similar to 1.0.0-SNAPSHOT .","title":"Developer Guide"},{"location":"developer/#developer-guide","text":"The Developer Guide provides information for developers who want to build, install, and test the operator. After successfully completing the steps in this guide, you can execute the instructions in the Quick Start Guide and User Guide .","title":"Developer Guide"},{"location":"developer/#prerequisites","text":"See Before You Begin section in the Quick Start guide. In addition, you require the following software versions for the build environment: Mac OS 10.13.6 Docker Desktop 2.0.0.3 (31259). Channel: stable, 8858db33c8, with Kubernetes v1.10.11. Oracle JDK 11.0.1 2018-10-16 LTS Apache Maven 3.5.4 Note : If you are using other operating systems or Docker versions, you need to make the necessary adjustments to execute the steps in this guide.","title":"Prerequisites"},{"location":"developer/#verify-system-environment","text":"Check whether your environment is configured with the following software for building and installing the operator: Software Verify Expected Output Docker $ docker run hello world Hello from Docker! Kubernetes $ kubectl version Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"clean\", BuildDate:\"2019-02-04T04:49:22Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"darwin/amd64\"}<br>Server Version: version.Info{Major:\"1\", Minor:\"10\", GitVersion:\"v1.10.11\", GitCommit:\"637c7e288581ee40ab4ca210618a89a555b6e7e9\", GitTreeState:\"clean\", BuildDate:\"2018-11-26T14:25:46Z\", GoVersion:\"go1.9.3\", Compiler:\"gc\", Platform:\"linux/amd64\"} Helm $ helm version Client: &version.Version{SemVer:\"v2.12.3\", GitCommit:\"eecf22f77df5f65c823aacd2dbd30ae6c65f186e\", GitTreeState:\"clean\"}<br>Server: &version.Version{SemVer:\"v2.12.3\", GitCommit:\"eecf22f77df5f65c823aacd2dbd30ae6c65f186e\", GitTreeState:\"clean\"} Java java version java version \"11.0.1\" 2018-10-16 LTS Maven mvn version Apache Maven 3.5.4 (1edded0938998edf8bf061f1ceb3cfdeccf443fe; 2018-06-17T14:33:14-04:00) <br> Maven home: /Users/username/Downloads/apache-maven-3.5.4 <br> Java version: 11.0.1, vendor: Oracle Corporation, runtime: /Library/Java/JavaVirtualMachines/jdk-11.0.1.jdk/Contents/Home","title":"Verify System Environment"},{"location":"developer/#build-the-operator","text":"The operator is built using Apache Maven. To build the operator without running any tests, do the following: Clone and check out the current version of the operator from the GitHub repository . Create a Maven settings.xml file. If you already have one, ensure that the following settings are included in your default profile: xml <properties> <test.image.prefix>DOCKER_REPO_HOSTNAME/DOCKER_REPO_PREFIX/dev/DEV_USERNAME/</test.image.prefix> </properties> Note : All the Maven commands in this guide use this settings.xml file. In the settings.xml file: * DOCKER_REPO_HOSTNAME is the hostname of the Docker repository in which you will push your built Docker images. Note : You are not required to push any images when executing the steps in this document. DOCKER_REPO_PREFIX is your organization who owns the repository on Docker Hub. DEV_USERNAME is a username unique to your development environment. In this example, YOUR_test.image.prefix_VALUE is the value used for test.image.prefix property in the settings.xml file. Obtain a Coherence 12.2.1.3.2 Docker image and tag it correctly. Refer to the section Obtain Images from Oracle Container Registry to pull the Coherence Docker image from the Oracle Container Registry. bash docker pull container-registry.oracle.com/middleware/coherence:12.2.1.3.2 Tag the obtained image in the way it is required to build the operator. Obtain the image ID for the Coherence 12.2.1.3.2 Docker image: bash $ docker images | grep 12.2.1.3.2 Tag the obtained image: bash docker tag some_tag YOUR_test.image.prefix_VALUE/oracle/coherence:12.2.1.3.2 In this example, some_tag is the tag given to identify the pulled image. Again execute the command to list the image ID for Coherence: ```bash $ docker images | grep 12.2.1.3.2 YOUR_test.image.prefix_VALUE/oracle/coherence 12.2.1.3.2 7e7feca04384 2 months ago 547MB ``` 5. From the root directory of your cloned repository, build the operator using the following commands: bash $ mvn -DskipTests clean install This produces output similar to the following: bash ... [INFO] ------------------------------------------------------------------------ [INFO] Reactor Summary: [INFO] [INFO] coherence-operator parent VERSION ........... SUCCESS [ 2.487 s] [INFO] coherence-operator ................................. SUCCESS [ 21.651 s] [INFO] coherence-utils .................................... SUCCESS [ 22.868 s] [INFO] coherence-operator-tests VERSION ............ SUCCESS [ 11.468 s] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 58.756 s [INFO] Finished at: 2019-04-17T18:35:14-04:00 [INFO] ------------------------------------------------------------------------ Note : The VERSION in the output will be similar to 1.0.0-SNAPSHOT . mvn -DskipTests generate-resources This produces output similar to the output of the preceding step. mvn -DskipTests -Pdocker clean install The output of this command has the following message: bash ... Successfully built af61471e4774 Successfully tagged YOUR_test.image.prefix_VALUE/oracle/coherence-operator:VERSION ... Successfully built 88495a497a16 Successfully tagged YOUR_test.image.prefix_VALUE/oracle/coherence-utils:VERSION Note : The VERSION in the output will be similar to 1.0.0-SNAPSHOT . Verify that the Docker images have been built and are accessible to your local Docker server. bash $ docker images | grep YOUR_test.image.prefix_VALUE This produces output similar to the following: bash YOUR_test.image.prefix_VALUE/oracle/coherence-utils VERSION 88495a497a16 14 minutes ago 124MB YOUR_test.image.prefix_VALUE/oracle/coherence-operator VERSION af61471e4774 14 minutes ago 537MB YOUR_test.image.prefix_VALUE/oracle/coherence 12.2.1.3.2 7e7feca04384 2 months ago 547MB Note : The VERSION in the output will be similar to 1.0.0-SNAPSHOT . Verify that the Coherence Helm chart and the Coherence Operator Helm chart have been built and are accessible in your work area. bash $ ls -la operator/target | grep \"drw\" | grep coherence This produces output similar to the following: bash drwxr-xr-x 3 username staff 96 Apr 17 18:38 coherence-VERSION-helm drwxr-xr-x 3 username staff 96 Apr 17 18:38 coherence-operator-VERSION-helm If you want to use the build image as the source while executing the steps in the Quick Start Guide and User Guide , replace the Helm repository prefix with the full qualified path: coherence/coherence - /Users/username/workareas/coherence-operator/target/coherence-1.0.0-SNAPSHOT-helm/coherence coherence/coherence-operator - /Users/username/workareas/coherence-operator/target/coherence-operator-1.0.0-SNAPSHOT-helm/coherence-operator Note: It is assumed that the Coherence Operator is built within /Users/username/workareas/coherence-operator . The VERSION in the output will be similar to 1.0.0-SNAPSHOT .","title":"Build the Operator"},{"location":"logcapture/","text":"Access the EFK (Elasticsearch, Fluentd and Kibana) Stack to View Logs The Coherence Operator (the \"operator\") manages Oracle Coherence on Kubernetes. It manages monitoring data through Prometheus, and logging data through the EFK stack. This use case is covered in the samples. Refer to the samples documentation . To access and configure the Kibana user interface(UI), follow the instructions: Install the Charts When you install the coherence-operator and coherence charts, you must specify the following option for helm for both charts. This ensures that the EFK stack is installed and correctly configured. --set logCaptureEnabled=true Port Forward Kibana Once you have installed both charts, use the following script to port forward the Kibana port 5601 : Note : If your chart is installed in a namespace other than the default , then include the --namespace option for both kubectl commands. #!/bin/bash trap \"exit\" INT while : do kubectl port-forward $(kubectl get pods | grep kibana | awk '{print $1}') 5601:5601 done Default Dashboards There are a number of dashboards created via the import process. Coherence Operator Coherence Operator - All Messages - Shows all Coherence Operator messages Coherence Cluster Coherence Cluster - All Messages - Shows all messages Coherence Cluster - Errors and Warnings - Shows only errors and warnings Coherence Cluster - Persistence - Shows persistence related messages Coherence Cluster - Partitions - Shows Ppartition related messages Coherence Cluster - Message Sources - Allows visualization of messages via the message source (Thread) Coherence Cluster - Configuration Messages - Shows configuration related messages Coherence Cluster - Network - Shows network related messages such as communication delays and TCP ring disconnects Default Queries There are many queries related to common Coherence messages, warnings and errors that are loaded and can be accessed via the discover side-bar. Troubleshooting No Default Index Pattern There are two index patterns created via the import process and the coherence-cluster-* pattern will be set as the default. If for some reason this is not the case, then perform the following steps to set coherence-cluster-* as the default: Open the URL: http://127.0.0.1:5601/ Navigate to the Management side-bar. Click Index Patterns , then select the coherence-cluster-* pattern. Click Star on the top right to set as default.","title":"Access the EFK (Elasticsearch, Fluentd and Kibana) Stack to View Logs"},{"location":"logcapture/#access-the-efk-elasticsearch-fluentd-and-kibana-stack-to-view-logs","text":"The Coherence Operator (the \"operator\") manages Oracle Coherence on Kubernetes. It manages monitoring data through Prometheus, and logging data through the EFK stack. This use case is covered in the samples. Refer to the samples documentation . To access and configure the Kibana user interface(UI), follow the instructions:","title":"Access the EFK (Elasticsearch, Fluentd and Kibana) Stack to View Logs"},{"location":"logcapture/#install-the-charts","text":"When you install the coherence-operator and coherence charts, you must specify the following option for helm for both charts. This ensures that the EFK stack is installed and correctly configured. --set logCaptureEnabled=true","title":"Install the Charts"},{"location":"logcapture/#port-forward-kibana","text":"Once you have installed both charts, use the following script to port forward the Kibana port 5601 : Note : If your chart is installed in a namespace other than the default , then include the --namespace option for both kubectl commands. #!/bin/bash trap \"exit\" INT while : do kubectl port-forward $(kubectl get pods | grep kibana | awk '{print $1}') 5601:5601 done","title":"Port Forward Kibana"},{"location":"logcapture/#default-dashboards","text":"There are a number of dashboards created via the import process. Coherence Operator Coherence Operator - All Messages - Shows all Coherence Operator messages Coherence Cluster Coherence Cluster - All Messages - Shows all messages Coherence Cluster - Errors and Warnings - Shows only errors and warnings Coherence Cluster - Persistence - Shows persistence related messages Coherence Cluster - Partitions - Shows Ppartition related messages Coherence Cluster - Message Sources - Allows visualization of messages via the message source (Thread) Coherence Cluster - Configuration Messages - Shows configuration related messages Coherence Cluster - Network - Shows network related messages such as communication delays and TCP ring disconnects","title":"Default Dashboards"},{"location":"logcapture/#default-queries","text":"There are many queries related to common Coherence messages, warnings and errors that are loaded and can be accessed via the discover side-bar.","title":"Default Queries"},{"location":"logcapture/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"logcapture/#no-default-index-pattern","text":"There are two index patterns created via the import process and the coherence-cluster-* pattern will be set as the default. If for some reason this is not the case, then perform the following steps to set coherence-cluster-* as the default: Open the URL: http://127.0.0.1:5601/ Navigate to the Management side-bar. Click Index Patterns , then select the coherence-cluster-* pattern. Click Star on the top right to set as default.","title":"No Default Index Pattern"},{"location":"prometheusoperator/","text":"Monitor Coherence Services via Grafana Dashboards The Coherence Operator (the \"operator\") includes the Prometheus Operator as an optional subchart named, prometheusoperator . To configure the Prometheus Operator and monitor Coherence services via Grafana dashboards, follow the instructions: Note: Use of Prometheus and Grafana is available only when using the operator with Coherence 12.2.1.4. This use-case is covered in the samples. Refer to the samples documentation . Install the Charts When you install the coherence-operator chart, you must specify the following additional set value for helm to install the subchart, prometheusoperator . --set prometheusoperator.enabled=true All coherence charts installed in coherence-operator targetNamespaces are monitored by Prometheus. The servicemonitor, <releasename>-coherence-service-monitor configures Prometheus to scrape all components of coherence-service . Port Forward Grafana After you install the charts, use the following script to port forward the Grafana pod: #!/bin/bash trap \"exit\" INT while : do kubectl port-forward $(kubectl get pods --selector=app=grafana -n namespace --output=jsonpath=\"{.items..metadata.name}\") -n namespace 3000:3000 done Note: Since port forwarding is sometimes unreliable, it should be used only as a development tool, and not in a production environment. Log in to Grafana In a browser, go to the URL, http://127.0.0.1:3000/d/coh-main/coherence-dashboard-main to access the main Coherence dashboard. On the Grafana login page, enter the login id, admin and the password, prom-operator . Click Home on the upper left corner of the page to get a list of preconfigured dashboards. Click Coherence Dashboard Main . Default Dashboards There are a number of dashboards created via the import process. Coherence Dashboard main for inspecting coherence cluster(s) Coherence Cluster Members Summary and Details Coherence Cluster Members Machines Summary Coherence Cache Summary and Details Coherence Services Summary and Details Coherence Proxy Servers Summary and Details Coherence Elastic Data Summary Coherence Cache Persistence Summary Coherence Http Servers Summary Navigate the Dashboards The Grafana dashboards created to monitor Coherence Clusters have some common UI elements and navigation patterns: Variables and Annotations At the top left of the dashboard, variables that are changeable and affect the queries in the dashboards are displayed. Similarly, annotations , that indicate events on the dashboard, can be enabled or disabled. ClusterName is a common variable that can be used to choose the cluster to display information for. Show Cluster Size Changed is an annotation that shows the time over which the cluster size has changed. All annotations appear as a red vertical line as shown below: Access other dashboards On the right of the page, you can click Available Dashboards to view all available dashboards. Configure your Prometheus Operator to Scrape Coherence pods This section assumes that you do not want the coherence-operator's helm subchart, prometheusoperator installed. It provides information on how to configure what is automated by using coherence-operator helm chart parameter prometheusoperator.enabled = true . Refer to the Prometheus Operator documentation to understand how to configure and deploy a service monitor for your own Prometheus Operator installation. This section only describes the service monitor configuration as it relates to the Coherence helm chart. coherence-service-monitor.yaml fragment: ... spec: selector: matchLabels: component: \"coherence-service\" ... endpoints: - port: 9612 If the Coherence helm chart parameter service.metricsHttpPort is set when installing the Coherence helm chart, replace 9612 above with the new value. If the Coherence helm chart parameter store.metrics.ssl.enabled is set to true , then add endpoints.scheme with the value of https to the coherence-service-monitor.yaml fragment. Troubleshoot Helm install of coherence-operator fails, creating a custom resource definition (CRD) The helm installation for coherence-operator fails, creating a custom resource definition (CRD). When you see this error, follow the recommendation in Prometheus Operator: helm fails to create CRDs to manually install the Prometheus Operator CRDs, and then install the coherence-operator chart with these additional set values. --set prometheusoperator.enabled=true --set prometheusoperator.prometheusOperator.createCustomResource=false No datasource found When you see this error, manually create a datasource by clicking Create your first data source on the Grafana Home page, and fill in the following fields: Name: Prometheus HTTP URL: http://prometheus-operated.<namespace>.svc.cluster.local:9090 Ensure that this datasource is set as the default. Click Save & Test .","title":"Monitor Coherence Services via Grafana Dashboards"},{"location":"prometheusoperator/#monitor-coherence-services-via-grafana-dashboards","text":"The Coherence Operator (the \"operator\") includes the Prometheus Operator as an optional subchart named, prometheusoperator . To configure the Prometheus Operator and monitor Coherence services via Grafana dashboards, follow the instructions: Note: Use of Prometheus and Grafana is available only when using the operator with Coherence 12.2.1.4. This use-case is covered in the samples. Refer to the samples documentation .","title":"Monitor Coherence Services via Grafana Dashboards"},{"location":"prometheusoperator/#install-the-charts","text":"When you install the coherence-operator chart, you must specify the following additional set value for helm to install the subchart, prometheusoperator . --set prometheusoperator.enabled=true All coherence charts installed in coherence-operator targetNamespaces are monitored by Prometheus. The servicemonitor, <releasename>-coherence-service-monitor configures Prometheus to scrape all components of coherence-service .","title":"Install the Charts"},{"location":"prometheusoperator/#port-forward-grafana","text":"After you install the charts, use the following script to port forward the Grafana pod: #!/bin/bash trap \"exit\" INT while : do kubectl port-forward $(kubectl get pods --selector=app=grafana -n namespace --output=jsonpath=\"{.items..metadata.name}\") -n namespace 3000:3000 done Note: Since port forwarding is sometimes unreliable, it should be used only as a development tool, and not in a production environment.","title":"Port Forward Grafana"},{"location":"prometheusoperator/#log-in-to-grafana","text":"In a browser, go to the URL, http://127.0.0.1:3000/d/coh-main/coherence-dashboard-main to access the main Coherence dashboard. On the Grafana login page, enter the login id, admin and the password, prom-operator . Click Home on the upper left corner of the page to get a list of preconfigured dashboards. Click Coherence Dashboard Main .","title":"Log in to Grafana"},{"location":"prometheusoperator/#default-dashboards","text":"There are a number of dashboards created via the import process. Coherence Dashboard main for inspecting coherence cluster(s) Coherence Cluster Members Summary and Details Coherence Cluster Members Machines Summary Coherence Cache Summary and Details Coherence Services Summary and Details Coherence Proxy Servers Summary and Details Coherence Elastic Data Summary Coherence Cache Persistence Summary Coherence Http Servers Summary","title":"Default Dashboards"},{"location":"prometheusoperator/#navigate-the-dashboards","text":"The Grafana dashboards created to monitor Coherence Clusters have some common UI elements and navigation patterns: Variables and Annotations At the top left of the dashboard, variables that are changeable and affect the queries in the dashboards are displayed. Similarly, annotations , that indicate events on the dashboard, can be enabled or disabled. ClusterName is a common variable that can be used to choose the cluster to display information for. Show Cluster Size Changed is an annotation that shows the time over which the cluster size has changed. All annotations appear as a red vertical line as shown below: Access other dashboards On the right of the page, you can click Available Dashboards to view all available dashboards.","title":"Navigate the Dashboards"},{"location":"prometheusoperator/#configure-your-prometheus-operator-to-scrape-coherence-pods","text":"This section assumes that you do not want the coherence-operator's helm subchart, prometheusoperator installed. It provides information on how to configure what is automated by using coherence-operator helm chart parameter prometheusoperator.enabled = true . Refer to the Prometheus Operator documentation to understand how to configure and deploy a service monitor for your own Prometheus Operator installation. This section only describes the service monitor configuration as it relates to the Coherence helm chart. coherence-service-monitor.yaml fragment: ... spec: selector: matchLabels: component: \"coherence-service\" ... endpoints: - port: 9612 If the Coherence helm chart parameter service.metricsHttpPort is set when installing the Coherence helm chart, replace 9612 above with the new value. If the Coherence helm chart parameter store.metrics.ssl.enabled is set to true , then add endpoints.scheme with the value of https to the coherence-service-monitor.yaml fragment.","title":"Configure your Prometheus Operator to Scrape Coherence pods"},{"location":"prometheusoperator/#troubleshoot","text":"","title":"Troubleshoot"},{"location":"prometheusoperator/#helm-install-of-coherence-operator-fails-creating-a-custom-resource-definition-crd","text":"The helm installation for coherence-operator fails, creating a custom resource definition (CRD). When you see this error, follow the recommendation in Prometheus Operator: helm fails to create CRDs to manually install the Prometheus Operator CRDs, and then install the coherence-operator chart with these additional set values. --set prometheusoperator.enabled=true --set prometheusoperator.prometheusOperator.createCustomResource=false","title":"Helm install of coherence-operator fails, creating a custom resource definition (CRD)"},{"location":"prometheusoperator/#no-datasource-found","text":"When you see this error, manually create a datasource by clicking Create your first data source on the Grafana Home page, and fill in the following fields: Name: Prometheus HTTP URL: http://prometheus-operated.<namespace>.svc.cluster.local:9090 Ensure that this datasource is set as the default. Click Save & Test .","title":"No datasource found"},{"location":"quickstart/","text":"Quick Start Guide The Quick Start guide provides a simple tutorial to help you get the operator up and running quickly. The Coherence Operator (the \"operator\") manages Coherence through Kubernetes, monitors MBean attributes through Prometheus, and server logs through Elasticsearch and Kibana. Note : Use of Prometheus and Grafana is available only when using the operator with Oracle Coherence 12.2.1.4. Introduction Before You Begin Software Requirements Runtime Environment Requirements Configure the Environment Add the Helm Repository for Coherence Install the Operator Install Coherence Obtain Images from Oracle Container Registry Set Up Secrets to Access the Oracle Container Registry Use Helm to Install Coherence Access the Installed Coherence Remove Coherence and Coherence Operator Introduction Use this Quick Start guide to deploy Coherence applications in a Kubernetes cluster managed by the operator. Note that this guide is for demonstration purposes only, and not sufficiently prescriptive or thorough for a production environment. These instructions assume that you are already familiar with Kubernetes and Helm. If you want to learn more about these two technologies, refer to the Kubernetes and Helm documentation. For more advanced actions, such as accessing Kibana for viewing server logs, see the User Guide . Note : If you have an older version of the operator installed on your cluster, you must remove it before installing the current version. Before You Begin Software Requirements The operator has the following requirements: Kubernetes 1.11.5+, 1.12.3+, 1.13.0+ ( kubectl version ) Docker 18.03.1-ce ( docker version ) Flannel networking v0.10.0-amd64 ( docker images | grep flannel ) Helm 2.12.3 or above, and all of its prerequisites Oracle Coherence 12.2.1.3.2 Runtime Environment Requirements You need a Kubernetes cluster that can pull the docker images required by the operator. Some of the Helm charts in this operator, require configuration on each Kubernetes pod that will run the workloads related to the chart. This configuration currently includes: setting the value of the max_map_count kernel parameter to at least 262144 . This is OS specific and is described in the Docker documentation . If you are running the operator with a local Kubernetes cluster on a developer workstation, ensure that the workstation meets the hardware requirements and the Docker preferences have been tuned to run optimally for the hardware. In particular, ensure that the memory and limits are correctly tuned. Configure the Environment Add the Helm Repository for Coherence Create a coherence helm repository: $ helm repo add coherence https://oracle.github.io/coherence-operator/charts $ helm repo update If you want to build the operator from source, refer to the Developer Guide and ensure that you replace the coherence Helm repository prefix in all samples with the fully qualified directory as described at the end of the guide. Install the Operator Use helm to install the operator: $ helm --debug install coherence/coherence-operator \\ --name sample-coherence-operator \\ --set \"targetNamespaces={}\" \\ --set imagePullSecrets=sample-coherence-secret Note : For all helm install commands, you can leave the --version option off if you want the latest version of the chart to be retrieved. However, if you want to use a specific version, such as 0.9.8 , add --version 0.9.8 to all installs for the coherence-operator and coherence charts. Note : Remove the --debug option if you do not want verbose output. Refer to the values.yaml in the chart for more information about the --set targetNamespaces argument. When the operation completes successfully, the Helm installation produces an output similar to the following: NOTES: Coherence Operator successfully installed. Monitoring namespaces: \"\" Use helm ls to view the installed releases. $ helm ls NAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE sample-coherence-operator 1 Thu May 9 13:59:22 2019 DEPLOYED coherence-operator-0.9.8 0.9.8 default Verify the status of the operator using helm status : $ helm status sample-coherence-operator A successful deployment displays an output similar to the following: LAST DEPLOYED: Thu Feb 7 14:11:17 2019 STATUS: DEPLOYED [...] Install Coherence Obtain Images from Oracle Container Registry By default, the Helm chart pulls the Oracle Coherence Docker image from the Oracle Container Registry. To pull the image correctly for the first time, you must perform the following steps: In a web browser, navigate to Oracle Container Registry and click Sign In . Enter your Oracle credentials or create an account if you don't have one. Search for coherence in the Search Oracle Container Registry field. Click coherence in the search result list. On the Oracle Coherence page, select the language from the drop-down list and click Continue. Click Accept on the Oracle Standard Terms and Conditions page. Subsequently, the image will be pulled automatically using the Kubernetes secret. This action is required to pull the image correctly for the first time. After this, the image will be pulled automatically using the Kubernetes secret. Set Up Secrets to Access the Oracle Container Registry Create a Kubernetes secret with the Oracle Container Registry credentials: bash $ kubectl create secret docker-registry oracle-container-registry-secret \\ --docker-server=container-registry.oracle.com \\ --docker-username='<username>' --docker-password='<password>' \\ --docker-email=`<emailid>` When installing Coherence, refer to the secret and Kubernetes will use that secret when pulling the image. Use Helm to Install Coherence Install the coherence helm chart using the secret 'oracle-container-registry-secret' which was created using the kubectl create secret command. $ helm --debug install coherence/coherence \\ --name sample-coherence \\ --set imagePullSecrets=oracle-container-registry-secret Note : If you want to use a different version of Coherence than the one specified in the coherence Helm chart, supply a --set argument for the coherence.image value: --set coherence.image=\"<prefix>/coherence:<version>\" Use the command helm inspect readme <chart name> to print out the README.md of the chart. For example, helm inspect readme coherence/coherence prints out the README.md for the operator chart. This includes documentation on all the possible values that can be configured with --set options to helm . Refer to the Configuration section of README. When the operation completes successfully, you see output similar to the following. NOTES: 1. Get the application URLs by running these commands: export POD_NAME=$(kubectl get pods -l \"app=coherence,release=sample-coherence\" -o jsonpath=\"{.items[0].metadata.name}\") To forward a local port to the Pod Coherence*Extend port run: kubectl port-forward $POD_NAME 20000:20000 then access the Coherence*Extend endpoint at 127.0.0.1:20000 To forward a local port to the Pod Http port run: kubectl port-forward $POD_NAME 30000:30000 then access the http endpoint at http://127.0.0.1:30000 You can also query the status of the installed Coherence using helm status : $ helm status sample-coherence LAST DEPLOYED: Wed Feb 13 14:51:38 2019 STATUS: DEPLOYED [...] Running helm install creates a helm release . See the Helm Glossary for the definition of Release. Access the Installed Coherence You can access the installed Coherence running within your Kubernetes using the default Coherence*Extend feature. When starting Coherence with no options, the created Coherence cluster has three nodes, and exposes a Coherence*Extend proxy server on port 20000 in the cluster. You must forward this port so that it is available outside the cluster. You can use the kubectl command by supplying the name of the Kubernetes pod that is running the Coherence. Query the Kubernetes cluster to get the name of the first of the three nodes in the Coherence cluster. $ export POD_NAME=$(kubectl get pods -l \"app=coherence,release=sample-coherence\" -o jsonpath=\"{.items[0].metadata.name}\") The name of the cluster will be similar to sample-coherence-65f558c987-5bdxr . Then, map the port 20000 inside the cluster to port 20000 outside the cluster. $ kubectl port-forward $POD_NAME 20000:20000 With this information, you can write a Coherence*Extend client configuration to access the cluster. Save the following XML in a file called example-client-config.xml : <cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"> <caching-scheme-mapping> <cache-mapping> <cache-name>*</cache-name> <scheme-name>thin-remote</scheme-name> </cache-mapping> </caching-scheme-mapping> <caching-schemes> <remote-cache-scheme> <scheme-name>thin-remote</scheme-name> <service-name>Proxy</service-name> <initiator-config> <tcp-initiator> <remote-addresses> <socket-address> <address>127.0.0.1</address> <port>20000</port> </socket-address> </remote-addresses> </tcp-initiator> </initiator-config> </remote-cache-scheme> </caching-schemes> </cache-config> Write a simple Java program to interact with the Coherence cluster. Save this file as HelloCoherence.java in the same directory as the XML file. import com.tangosol.net.CacheFactory; import com.tangosol.net.NamedCache; public class HelloCoherence { public static void main(String[] asArgs) throws Throwable { NamedCache<String, Integer> cache = CacheFactory.getCache(\"HelloCoherence\"); Integer IValue = (Integer) cache.get(\"key\"); IValue = (null == IValue) ? Integer.valueOf(1) : Integer.valueOf(IValue + 1); cache.put(\"key\", IValue); System.out.println(\"The value of the key is \" + IValue); } } With the XML and Java source files in the same directory, and the coherence.jar at ${COHERENCE_HOME}/lib/coherence.jar , compile and run the program: $ javac -cp .:${COHERENCE_HOME}/lib/coherence.jar HelloCoherence.java $ java -cp .:${COHERENCE_HOME}/lib/coherence.jar \\ -Dcoherence.cacheconfig=$PWD/example-client-config.xml HelloCoherence The program produces an output similar to the following: The value of the key is 1 Run the program again to get an output similar to the following: The value of the key is 2 Note : If you are using JDK 11 or later, you can omit the javac command and simply run the program as: $ java -cp $${COHERENCE_HOME}/lib/coherence.jar \\ -Dcoherence.cacheconfig=$PWD/example-client-config.xml HelloCoherence.java Remove Coherence and Coherence Operator Remove the coherence release: $ helm delete --purge sample-coherence sample-coherence-operator","title":"Quick Start"},{"location":"quickstart/#quick-start-guide","text":"The Quick Start guide provides a simple tutorial to help you get the operator up and running quickly. The Coherence Operator (the \"operator\") manages Coherence through Kubernetes, monitors MBean attributes through Prometheus, and server logs through Elasticsearch and Kibana. Note : Use of Prometheus and Grafana is available only when using the operator with Oracle Coherence 12.2.1.4. Introduction Before You Begin Software Requirements Runtime Environment Requirements Configure the Environment Add the Helm Repository for Coherence Install the Operator Install Coherence Obtain Images from Oracle Container Registry Set Up Secrets to Access the Oracle Container Registry Use Helm to Install Coherence Access the Installed Coherence Remove Coherence and Coherence Operator","title":"Quick Start Guide"},{"location":"quickstart/#introduction","text":"Use this Quick Start guide to deploy Coherence applications in a Kubernetes cluster managed by the operator. Note that this guide is for demonstration purposes only, and not sufficiently prescriptive or thorough for a production environment. These instructions assume that you are already familiar with Kubernetes and Helm. If you want to learn more about these two technologies, refer to the Kubernetes and Helm documentation. For more advanced actions, such as accessing Kibana for viewing server logs, see the User Guide . Note : If you have an older version of the operator installed on your cluster, you must remove it before installing the current version.","title":"Introduction"},{"location":"quickstart/#before-you-begin","text":"","title":"Before You Begin"},{"location":"quickstart/#software-requirements","text":"The operator has the following requirements: Kubernetes 1.11.5+, 1.12.3+, 1.13.0+ ( kubectl version ) Docker 18.03.1-ce ( docker version ) Flannel networking v0.10.0-amd64 ( docker images | grep flannel ) Helm 2.12.3 or above, and all of its prerequisites Oracle Coherence 12.2.1.3.2","title":"Software Requirements"},{"location":"quickstart/#runtime-environment-requirements","text":"You need a Kubernetes cluster that can pull the docker images required by the operator. Some of the Helm charts in this operator, require configuration on each Kubernetes pod that will run the workloads related to the chart. This configuration currently includes: setting the value of the max_map_count kernel parameter to at least 262144 . This is OS specific and is described in the Docker documentation . If you are running the operator with a local Kubernetes cluster on a developer workstation, ensure that the workstation meets the hardware requirements and the Docker preferences have been tuned to run optimally for the hardware. In particular, ensure that the memory and limits are correctly tuned.","title":"Runtime Environment Requirements"},{"location":"quickstart/#configure-the-environment","text":"","title":"Configure the Environment"},{"location":"quickstart/#add-the-helm-repository-for-coherence","text":"Create a coherence helm repository: $ helm repo add coherence https://oracle.github.io/coherence-operator/charts $ helm repo update If you want to build the operator from source, refer to the Developer Guide and ensure that you replace the coherence Helm repository prefix in all samples with the fully qualified directory as described at the end of the guide.","title":"Add the Helm Repository for Coherence"},{"location":"quickstart/#install-the-operator","text":"Use helm to install the operator: $ helm --debug install coherence/coherence-operator \\ --name sample-coherence-operator \\ --set \"targetNamespaces={}\" \\ --set imagePullSecrets=sample-coherence-secret Note : For all helm install commands, you can leave the --version option off if you want the latest version of the chart to be retrieved. However, if you want to use a specific version, such as 0.9.8 , add --version 0.9.8 to all installs for the coherence-operator and coherence charts. Note : Remove the --debug option if you do not want verbose output. Refer to the values.yaml in the chart for more information about the --set targetNamespaces argument. When the operation completes successfully, the Helm installation produces an output similar to the following: NOTES: Coherence Operator successfully installed. Monitoring namespaces: \"\" Use helm ls to view the installed releases. $ helm ls NAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE sample-coherence-operator 1 Thu May 9 13:59:22 2019 DEPLOYED coherence-operator-0.9.8 0.9.8 default Verify the status of the operator using helm status : $ helm status sample-coherence-operator A successful deployment displays an output similar to the following: LAST DEPLOYED: Thu Feb 7 14:11:17 2019 STATUS: DEPLOYED [...]","title":"Install the Operator"},{"location":"quickstart/#install-coherence","text":"","title":"Install Coherence"},{"location":"quickstart/#obtain-images-from-oracle-container-registry","text":"By default, the Helm chart pulls the Oracle Coherence Docker image from the Oracle Container Registry. To pull the image correctly for the first time, you must perform the following steps: In a web browser, navigate to Oracle Container Registry and click Sign In . Enter your Oracle credentials or create an account if you don't have one. Search for coherence in the Search Oracle Container Registry field. Click coherence in the search result list. On the Oracle Coherence page, select the language from the drop-down list and click Continue. Click Accept on the Oracle Standard Terms and Conditions page. Subsequently, the image will be pulled automatically using the Kubernetes secret. This action is required to pull the image correctly for the first time. After this, the image will be pulled automatically using the Kubernetes secret.","title":"Obtain Images from Oracle Container Registry"},{"location":"quickstart/#set-up-secrets-to-access-the-oracle-container-registry","text":"Create a Kubernetes secret with the Oracle Container Registry credentials: bash $ kubectl create secret docker-registry oracle-container-registry-secret \\ --docker-server=container-registry.oracle.com \\ --docker-username='<username>' --docker-password='<password>' \\ --docker-email=`<emailid>` When installing Coherence, refer to the secret and Kubernetes will use that secret when pulling the image.","title":"Set Up Secrets to Access the Oracle Container Registry"},{"location":"quickstart/#use-helm-to-install-coherence","text":"Install the coherence helm chart using the secret 'oracle-container-registry-secret' which was created using the kubectl create secret command. $ helm --debug install coherence/coherence \\ --name sample-coherence \\ --set imagePullSecrets=oracle-container-registry-secret Note : If you want to use a different version of Coherence than the one specified in the coherence Helm chart, supply a --set argument for the coherence.image value: --set coherence.image=\"<prefix>/coherence:<version>\" Use the command helm inspect readme <chart name> to print out the README.md of the chart. For example, helm inspect readme coherence/coherence prints out the README.md for the operator chart. This includes documentation on all the possible values that can be configured with --set options to helm . Refer to the Configuration section of README. When the operation completes successfully, you see output similar to the following. NOTES: 1. Get the application URLs by running these commands: export POD_NAME=$(kubectl get pods -l \"app=coherence,release=sample-coherence\" -o jsonpath=\"{.items[0].metadata.name}\") To forward a local port to the Pod Coherence*Extend port run: kubectl port-forward $POD_NAME 20000:20000 then access the Coherence*Extend endpoint at 127.0.0.1:20000 To forward a local port to the Pod Http port run: kubectl port-forward $POD_NAME 30000:30000 then access the http endpoint at http://127.0.0.1:30000 You can also query the status of the installed Coherence using helm status : $ helm status sample-coherence LAST DEPLOYED: Wed Feb 13 14:51:38 2019 STATUS: DEPLOYED [...] Running helm install creates a helm release . See the Helm Glossary for the definition of Release.","title":"Use Helm to Install Coherence"},{"location":"quickstart/#access-the-installed-coherence","text":"You can access the installed Coherence running within your Kubernetes using the default Coherence*Extend feature. When starting Coherence with no options, the created Coherence cluster has three nodes, and exposes a Coherence*Extend proxy server on port 20000 in the cluster. You must forward this port so that it is available outside the cluster. You can use the kubectl command by supplying the name of the Kubernetes pod that is running the Coherence. Query the Kubernetes cluster to get the name of the first of the three nodes in the Coherence cluster. $ export POD_NAME=$(kubectl get pods -l \"app=coherence,release=sample-coherence\" -o jsonpath=\"{.items[0].metadata.name}\") The name of the cluster will be similar to sample-coherence-65f558c987-5bdxr . Then, map the port 20000 inside the cluster to port 20000 outside the cluster. $ kubectl port-forward $POD_NAME 20000:20000 With this information, you can write a Coherence*Extend client configuration to access the cluster. Save the following XML in a file called example-client-config.xml : <cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"> <caching-scheme-mapping> <cache-mapping> <cache-name>*</cache-name> <scheme-name>thin-remote</scheme-name> </cache-mapping> </caching-scheme-mapping> <caching-schemes> <remote-cache-scheme> <scheme-name>thin-remote</scheme-name> <service-name>Proxy</service-name> <initiator-config> <tcp-initiator> <remote-addresses> <socket-address> <address>127.0.0.1</address> <port>20000</port> </socket-address> </remote-addresses> </tcp-initiator> </initiator-config> </remote-cache-scheme> </caching-schemes> </cache-config> Write a simple Java program to interact with the Coherence cluster. Save this file as HelloCoherence.java in the same directory as the XML file. import com.tangosol.net.CacheFactory; import com.tangosol.net.NamedCache; public class HelloCoherence { public static void main(String[] asArgs) throws Throwable { NamedCache<String, Integer> cache = CacheFactory.getCache(\"HelloCoherence\"); Integer IValue = (Integer) cache.get(\"key\"); IValue = (null == IValue) ? Integer.valueOf(1) : Integer.valueOf(IValue + 1); cache.put(\"key\", IValue); System.out.println(\"The value of the key is \" + IValue); } } With the XML and Java source files in the same directory, and the coherence.jar at ${COHERENCE_HOME}/lib/coherence.jar , compile and run the program: $ javac -cp .:${COHERENCE_HOME}/lib/coherence.jar HelloCoherence.java $ java -cp .:${COHERENCE_HOME}/lib/coherence.jar \\ -Dcoherence.cacheconfig=$PWD/example-client-config.xml HelloCoherence The program produces an output similar to the following: The value of the key is 1 Run the program again to get an output similar to the following: The value of the key is 2 Note : If you are using JDK 11 or later, you can omit the javac command and simply run the program as: $ java -cp $${COHERENCE_HOME}/lib/coherence.jar \\ -Dcoherence.cacheconfig=$PWD/example-client-config.xml HelloCoherence.java","title":"Access the Installed Coherence"},{"location":"quickstart/#remove-coherence-and-coherence-operator","text":"Remove the coherence release: $ helm delete --purge sample-coherence sample-coherence-operator","title":"Remove Coherence and Coherence Operator"},{"location":"storage/","text":"Recommended Storage for Coherence Active Persistence For active persistence, local storage is recommended as its IO has a lower latency. Set up Coherence active persistence with local storage using the following steps: 1. Create a Local Storage Class If necessary, create a local storage class, localsc , using YAML. apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"storage.k8s.io/v1beta1\",\"kind\":\"StorageClass\",\"metadata\":{\"annotations\":{\"storageclass.beta.kubernetes.io/is-default-class\":\"false\"},\"name\":\"localsc\",\"namespace\":\"\"},\"provisioner\":\"kubernetes.io/no-provisioner\"} storageclass.beta.kubernetes.io/is-default-class: \"false\" name: localsc selfLink: /apis/storage.k8s.io/v1/storageclasses/localsc provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer 2. Create Local Persistent Volumes Since local volumes can only be used as a statically created PersistentVolume, persistent volumes need to be created before they are used. For information about persistent volumes, read the Kubernetes document . A sample YAML for creating PersistentVolume, mylocalsc-pv0 with label type: local is as follows: kind: PersistentVolume apiVersion: v1 metadata: name: mylocalsc-pv0 labels: type: local spec: storageClassName: mylocalsc capacity: storage: 2Gi accessModes: - ReadWriteOnce hostPath: path: \"/coh/mydata\" The sample YAML file can be used to create PersistentVolume using the following kubectl command: kubectl create -f mypv0.yaml Note : The number of Persistent Volumes created should be the same as the Coherence cluster size. For instance, in case of a cluster of size 3, you must create Persistent Volumes, mylocalsc-pv1 , mylocalsc-pv2 , and mylocalsc-pv3 . 3. Specify Parameters for Coherence Helm Install a. Set nodeSelector for Coherence Clusters In order to use the local storage, the Coherence cluster must run a specified set of nodes. Labels are used to identify the nodes that you want to use. Note that the set of nodes identified must be the same as the Coherence cluster size. Suppose the label name=pool1 is sufficient to identify the nodes that are required, then the following Helm parameter needs to be set: --set nodeSelector.name=pool1 b. Enable Active Persistence in Coherence Set the following Helm parameter to enable Active Persistence in Coherence: --set store.persistence.enabled=true c. Specify Persistent Volumes Used for Coherence Set storage class and set labels for persistent volumes. --set store.persistence.storageClass=mylocalsc --set store.persistence.selector.matchLabels.type=local 4. Uninstall Coherence with Persistence i. Remove the Installation Using Helm Delete ii. Delete the Persistent Volume Claim (PVC) Note : The PVC is created in the same namespace as your Helm installation. The number of PVC equals the Coherence cluster size. Retrieve the name of the PVC as follows: kubectl get pvc -n your_namespace Then, delete the PVC one at a time as follows: kubectl delete pvc -n your_namespace your_pvc_name iii. Delete the Persistent Volume kubectl delete pv mylocalsc-pv0 Snapshot By default, Coherence snapshot uses the same location as active persistence to store snapshot data. If you want to use a different location, then add the following parameter during Helm installation: --set store.snapshot.enabled=true In this case, you should use block storage and configure store.snapshot.storageClass and store.snapshot.selector.matchLabels as in step 3c above. OCI block storage is used by default in an OCI environment. Therefore, it is not required to set the above two properties.","title":"Recommended Storage for Coherence"},{"location":"storage/#recommended-storage-for-coherence","text":"","title":"Recommended Storage for Coherence"},{"location":"storage/#active-persistence","text":"For active persistence, local storage is recommended as its IO has a lower latency. Set up Coherence active persistence with local storage using the following steps:","title":"Active Persistence"},{"location":"storage/#1-create-a-local-storage-class","text":"If necessary, create a local storage class, localsc , using YAML. apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"storage.k8s.io/v1beta1\",\"kind\":\"StorageClass\",\"metadata\":{\"annotations\":{\"storageclass.beta.kubernetes.io/is-default-class\":\"false\"},\"name\":\"localsc\",\"namespace\":\"\"},\"provisioner\":\"kubernetes.io/no-provisioner\"} storageclass.beta.kubernetes.io/is-default-class: \"false\" name: localsc selfLink: /apis/storage.k8s.io/v1/storageclasses/localsc provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer","title":"1. Create a Local Storage Class"},{"location":"storage/#2-create-local-persistent-volumes","text":"Since local volumes can only be used as a statically created PersistentVolume, persistent volumes need to be created before they are used. For information about persistent volumes, read the Kubernetes document . A sample YAML for creating PersistentVolume, mylocalsc-pv0 with label type: local is as follows: kind: PersistentVolume apiVersion: v1 metadata: name: mylocalsc-pv0 labels: type: local spec: storageClassName: mylocalsc capacity: storage: 2Gi accessModes: - ReadWriteOnce hostPath: path: \"/coh/mydata\" The sample YAML file can be used to create PersistentVolume using the following kubectl command: kubectl create -f mypv0.yaml Note : The number of Persistent Volumes created should be the same as the Coherence cluster size. For instance, in case of a cluster of size 3, you must create Persistent Volumes, mylocalsc-pv1 , mylocalsc-pv2 , and mylocalsc-pv3 .","title":"2. Create Local Persistent Volumes"},{"location":"storage/#3-specify-parameters-for-coherence-helm-install","text":"","title":"3. Specify Parameters for Coherence Helm Install"},{"location":"storage/#a-set-nodeselector-for-coherence-clusters","text":"In order to use the local storage, the Coherence cluster must run a specified set of nodes. Labels are used to identify the nodes that you want to use. Note that the set of nodes identified must be the same as the Coherence cluster size. Suppose the label name=pool1 is sufficient to identify the nodes that are required, then the following Helm parameter needs to be set: --set nodeSelector.name=pool1","title":"a. Set nodeSelector for Coherence Clusters"},{"location":"storage/#b-enable-active-persistence-in-coherence","text":"Set the following Helm parameter to enable Active Persistence in Coherence: --set store.persistence.enabled=true","title":"b. Enable Active Persistence in Coherence"},{"location":"storage/#c-specify-persistent-volumes-used-for-coherence","text":"Set storage class and set labels for persistent volumes. --set store.persistence.storageClass=mylocalsc --set store.persistence.selector.matchLabels.type=local","title":"c. Specify Persistent Volumes Used for Coherence"},{"location":"storage/#4-uninstall-coherence-with-persistence","text":"","title":"4. Uninstall Coherence with Persistence"},{"location":"storage/#i-remove-the-installation-using-helm-delete","text":"","title":"i. Remove the Installation Using Helm Delete"},{"location":"storage/#ii-delete-the-persistent-volume-claim-pvc","text":"Note : The PVC is created in the same namespace as your Helm installation. The number of PVC equals the Coherence cluster size. Retrieve the name of the PVC as follows: kubectl get pvc -n your_namespace Then, delete the PVC one at a time as follows: kubectl delete pvc -n your_namespace your_pvc_name","title":"ii. Delete the Persistent Volume Claim (PVC)"},{"location":"storage/#iii-delete-the-persistent-volume","text":"kubectl delete pv mylocalsc-pv0","title":"iii. Delete the Persistent Volume"},{"location":"storage/#snapshot","text":"By default, Coherence snapshot uses the same location as active persistence to store snapshot data. If you want to use a different location, then add the following parameter during Helm installation: --set store.snapshot.enabled=true In this case, you should use block storage and configure store.snapshot.storageClass and store.snapshot.selector.matchLabels as in step 3c above. OCI block storage is used by default in an OCI environment. Therefore, it is not required to set the above two properties.","title":"Snapshot"},{"location":"user-guide/","text":"User Guide The User Guide provides detailed information about how to install Coherence and the Coherence Operator in your Kubernetes cluster, and how to use the operator to manage Coherence clusters. Guide to this Document Before You Begin Common Coherence Tasks Provide Configuration Files and Application Classes to the Coherence Cluster within Kubernetes Deploy JAR Files Create a JAR File Create a Docker image for the Sidecar Use Helm to Install Coherence Create the Local Extend Client Configuration Remove Coherence Deploy Configuration Files Create the Server Side Cache Configuration Create a Docker Image for the Sidecar Run the client program Deploy JAR Containing Application Classes and Configuration files Extract Heap Dump Files from a Kubernetes Coherence Pod Extract Coherence Log Files from Kubernetes Query Elasticsearch Use Java Management Extensions (JMX) to Inspect and Manage Coherence Download the opendmk_jmxremote_optional_jar JAR Run VisualVM with the Additional JAR Manipulate the VisualVM UI to View the Coherence MBeans Provide Arguments to the JVM that Runs Coherence Kubernetes Specific Tasks Using Helm to Scale the Coherence Deployment Perform a Safe Rolling Upgrade Deploy Multiple Coherence Clusters Monitoring Performance and Logging Configuring SSL Endpoints for Management over REST and Metrics Publishing Configuring SSL Endpoints for Management over REST Configuring SSL for Metrics Publishing for Prometheus Guide to this Document The User Guide provides exclusive steps for managing Coherence within Kubernetes. For most of the administrative tasks for managing Kubernetes, refer to Kubernetes Documentation. The information in this guide is organized into sections that are common and Kubernetes specific tasks. Refer to these sections for managing Coherence: * Common Coherence Tasks * Kubernetes Specific Tasks Before You Begin Refere to Before You Begin section in the Quick Start guide. All the examples in this guide are installed in a Kubernetes namespace called sample-coherence-ns . To set this namespace as the active namespace, execute the command: $ kubectl config set-context $(kubectl config current-context) --namespace=sample-coherence-ns Common Coherence Tasks The most common administrative tasks with Coherence are Overriding the Default Cache Configuration File and deploying JARs for Processing Data in a Cache . Most of the administrative tasks to do with Coherence apply when running within Kubernetes. The Oracle Coherence documentation remains a very useful resource. This section covers a few common scenarios that require special treatment regarding Kubernetes. Provide Configuration Files and Application Classes to the Coherence Cluster within Kubernetes This section explains how to make custom configuration and JAR files available to your Coherence cluster running in Kubernetes. This approach can be used for any administrative tasks that require to make JAR, XML, or other configuration files available to the Coherence cluster. You can refer to Add Application Jars/Config to a Coherence Deployment in the Samples. The Coherence Operator uses the sidecar pattern , as recommended by Kubernetes , to make resources available to Coherence within the Kubernetes cluster. Docker containers are the most flexible way to allow interaction with the Coherence cluster running in Kubernetes. The steps to use the sidecar pattern include: Determine the JARs or configuration files that you want to make them available to the cluster. Package the files in a Docker image and deploy that image to a Docker registry accessible to the Kubernetes cluster. Install the docker image using the Helm chart. Deploy JAR Files The concept to create a JAR file is derived from Building Your First Extend Application in Oracle Fusion Middleware Developing Remote Clients for Oracle Coherence for use within Kubernetes. Create a JAR File To create a JAR file: Create a directory for the files: bash $ mkdir -p hello-example/files/lib $ cd hello-example Create a Java program to access the cluster. Save the java file as HelloExample.java in the hello-example directory. ```java import java.io.Serializable; import java.text.SimpleDateFormat; import java.util.Date; import com.tangosol.net.NamedCache; import com.tangosol.net.CacheFactory; public class HelloExample { public static void main(String[] asArgs) throws Throwable { NamedCache cache = CacheFactory.getCache(\"hello-example\"); Timestamp ts = cache.get(\"ts1\"); cache.put(\"ts1\", ts = new Timestamp((null == ts) ? Long.MIN_VALUE : ts.currentTime)); System.out.println(\"The value of the key is \" + ts.toString()); } public static class Timestamp implements Serializable { public long currentTime; public long previousTime; public Timestamp(long previousTime) { this.currentTime = System.currentTimeMillis(); this.previousTime = previousTime; } public String toString() { SimpleDateFormat f = new SimpleDateFormat(\"HH:mm:ss\"); return \"Timestamp: previousTime: \" + f.format(new Date(previousTime)) + \" currentTime: \" + f.format(new Date(currentTime)); } } } This program uses a static inner class, `Timestamp`, to store the values in Coherence. Any Java object that is stored in Coherence must be accessible by Coherence in compiled form. The Java objects are compiled classes in JAR files on the Coherence classpath. Therefore, compile and archive the file: ``` $ javac -cp .:${COHERENCE_HOME}/lib/coherence.jar HelloExample.java $ jar -cf files/lib/hello-example.jar *.class ``` #### Create a Docker image for the Sidecar Package the created JAR file within the sidecar Docker image: 1. Create a `Dockerfile` with the following contents: ```bash FROM oraclelinux:7-slim RUN mkdir -p /files/lib COPY files/lib/hello-example.jar files/lib ``` Note that the JAR file is placed in the `files/lib` directory relative to the root of the Docker image. This is the default location where Coherence looks for JAR files to add to the classpath. Any JAR files in `files/lib` are added to the classpath. You can change the location where Coherence must look for JARs to add to the classpath. 2. Ensure that the Docker is running on the current host. If not, refer to [ Get Started](https://docs.docker.com/get-started/) in Docker documentation. 3. Build and tag a Docker image for `hello-example-sidecar`: ```bash $ docker build -t \"hello-example-sidecar:1.0.0-SNAPSHOT\" . ``` The trailing dot \".\" in the command refers to run the build relative to the current directory. 4. Push the created Docker image to the Docker registry which the Kubernetes cluster can access. The [Quick Start](quickstart.md#obtain-images-from-oracle-container-registry) guide describes how to make the Kubernetes cluster can pull the images using the Docker credentials. > **Note:** If you are using a local Kubernetes, you can omit this step, since the Kubernetes pulls from the same Docker server as the one to which the local build command built the image. #### Use Helm to Install Coherence Install Coherence using the Helm chart with the details of the sidecar image arguments: ```bash $ helm --debug install coherence/coherence --name hello-example \\ --set userArtifacts.image=hello-example-sidecar:1.0.0-SNAPSHOT \\ --set imagePullSecrets=sample-coherence-secret Note: If the JAR files are in a different location within the sidecar Docker image, use the --set userArtifacts.libDir=<absolute path within docker image> argument to helm install to configure the correct location. In a new terminal window, set up a Kubernetes port forward to expose the Extend port so that your local client can use it: $ export POD_NAME=$(kubectl get pods --namespace default -l \\ \"app=coherence,release=hello-example\" -o jsonpath=\"{.items[0].metadata.name}\") $ kubectl --namespace default port-forward $POD_NAME 20000:20000 This prints the following output and blocks the shell: Forwarding from 127.0.0.1:20000 -> 20000 Forwarding from [::1]:20000 -> 20000 Create the Local Extend Client Configuration A local client configuration is necessary because the local client connects to the service through Coherence*Extend. Create a file named hello-client-config.xml with the following contents: <cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"> <caching-scheme-mapping> <cache-mapping> <cache-name>*</cache-name> <scheme-name>thin-remote</scheme-name> </cache-mapping> </caching-scheme-mapping> <caching-schemes> <remote-cache-scheme> <scheme-name>thin-remote</scheme-name> <service-name>Proxy</service-name> <initiator-config> <tcp-initiator> <remote-addresses> <socket-address> <address>127.0.0.1</address> <port>20000</port> </socket-address> </remote-addresses> </tcp-initiator> </initiator-config> </remote-cache-scheme> </caching-schemes> </cache-config> Run the client using the following command: $ java -cp files/lib/hello-example.jar:${COHERENCE_HOME}/lib/coherence.jar \\ -Dcoherence.log.level=1 -Dcoherence.distributed.localstorage=false \\ -Dcoherence.cacheconfig=$PWD/hello-client-config.xml HelloExample An output similar to the following is displayed: The value of the key is Timestamp: previousTime: 11:47:04 currentTime: 16:09:30 Run the command again and it shows the updated Timestamp : The value of the key is Timestamp: previousTime: 16:09:30 currentTime: 16:10:20 Remove Coherence $ helm delete --purge hello-example Deploy Configuration Files The similar sidecar approach is used to deploy configuration files to Coherence inside Kubernetes. Though, Coherence has the necessary built-in configuration, a subset of that configuration is used in this example. Create a directory for the files: bash $ cd .. $ mkdir -p hello-config-example/files/conf $ cd hello-config-example 2. Create the Java program to access the cluster. In the same directory, create a simple java program HelloConfigXml.java : ```java import com.tangosol.net.CacheFactory; import com.tangosol.net.NamedCache; public class HelloConfigXml { public static void main(String[] asArgs) throws Throwable { NamedCache cache = CacheFactory.getCache(\"hello-config-xml\"); Integer IValue = (Integer) cache.get(\"key\"); IValue = (null == IValue) ? Integer.valueOf(1) : Integer.valueOf(IValue + 1); cache.put(\"key\", IValue); System.out.println(\"The value of the key is \" + IValue); } } `` 3. Create the following XML file, next to the java file, called hello-client-config.xml`: ``` hello-config-xml thin-remote thin-remote Proxy 127.0.0.1 20000 #### Create the Server Side Cache Configuration Create the file `files/conf/hello-server-config.xml` with the following content: hello-config-xml ${coherence.profile near}-${coherence.client direct} near-direct {front-limit-entries 10000} thin-direct <!-- near caching scheme for extend clients --> <near-scheme> <scheme-name>near-remote</scheme-name> <scheme-ref>near-direct</scheme-ref> <back-scheme> <remote-cache-scheme> <scheme-ref>thin-remote</scheme-ref> </remote-cache-scheme> </back-scheme> </near-scheme> <!-- remote caching scheme for accessing the proxy from extend clients --> <remote-cache-scheme> <scheme-name>thin-remote</scheme-name> <service-name>RemoteCache</service-name> <proxy-service-name>Proxy</proxy-service-name> </remote-cache-scheme> <!-- partitioned caching scheme for clustered clients --> <distributed-scheme> <scheme-name>thin-direct</scheme-name> <scheme-ref>server</scheme-ref> <local-storage system-property=\"coherence.distributed.localstorage\">false</local-storage> <autostart>false</autostart> </distributed-scheme> <!-- partitioned caching scheme for servers --> <distributed-scheme> <scheme-name>server</scheme-name> <service-name>PartitionedCache</service-name> <local-storage system-property=\"coherence.distributed.localstorage\">true</local-storage> <backing-map-scheme> <local-scheme> <high-units>{back-limit-bytes 0B}</high-units> </local-scheme> </backing-map-scheme> <autostart>true</autostart> </distributed-scheme> <!-- proxy scheme that allows extend clients to connect to the cluster over TCP/IP --> <proxy-scheme> <service-name>Proxy</service-name> <acceptor-config> <tcp-acceptor> <local-address> <address system-property=\"coherence.extend.address\"/> <port system-property=\"coherence.extend.port\"/> </local-address> </tcp-acceptor> </acceptor-config> <load-balancer>client</load-balancer> <autostart>true</autostart> </proxy-scheme> #### Create a Docker Image for the Sidecar Package the XML file within the sidecar Docker image: 1. Create a `Dockerfile` next to the Java file with the following contents. ```bash FROM oraclelinux:7-slim RUN mkdir -p /files/conf COPY files/conf/hello-server-config.xml files/conf/hello-server-config.xml ``` Note that the XML file is placed in the `files/conf` directory relative to the root of the Docker image. This is the default location where Coherence looks for configuration files that apply to Coherence. You can change the location where Coherence looks for configuration files to add to the classpath. 2. Ensure Docker is running on the current host. If not, refer to [Get Started with Docker](https://docs.docker.com/get-started/). 3. Build and tag a Docker image for `hello-server-config-sidecar`: ```bash $ docker build -t \"hello-server-config-sidecar:1.0.0-SNAPSHOT\" . ``` Note that the trailing dot \".\" is very significant which means to run the build relative to the current directory. 4. Push your image to the Docker registry which the Kubernetes cluster can access. The [Quick Start](./quickstart.md) guide describes how to make the Kubernetes cluster pull the images using the Docker credentials. > **Note:** If you are using a local Kubernetes, you can omit this step, since the Kubernetes pulls from the same Docker server as the one to which the local build command built the image. 5. Install the Helm chart with the sidecar image arguments: ```bash $ helm --debug install coherence/coherence --name hello-server-config \\ --set userArtifacts.image=hello-server-config-sidecar:1.0.0-SNAPSHOT \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=hello-server-config.xml Note: If your XML files are in a different location within the sidecar Docker image, use the --set userArtifacts.configDir=<absolute path within docker image> argument to helm install to configure the correct location. In a new terminal window, set up a Kubernetes port forward to expose the Extend port so that your local client can use it. The instructions for doing this are from the output of the helm install command: $ export POD_NAME=$(kubectl get pods --namespace default -l \"app=coherence,release=hello-server-config\" -o jsonpath=\"{.items[0].metadata.name}\") $ kubectl --namespace default port-forward $POD_NAME 20000:20000 This prints the following output and blocks the shell: Forwarding from 127.0.0.1:20000 -> 20000 Forwarding from [::1]:20000 -> 20000 Run the client program In the same directory as the XML and Java source files, run the client: $ javac -cp .:${COHERENCE_HOME}/lib/coherence.jar HelloConfigXml.java $ java -cp .:${COHERENCE_HOME}/lib/coherence.jar \\ -Dcoherence.distributed.localstorage=false \\ -Dcoherence.cacheconfig=$PWD/hello-client-config.xml -Dcoherence.log.level=1 HelloConfigXml The correct coherence.jar must be available at ${COHERENCE_HOME}/lib/coherence.jar . An output similar to the following is displayed: The value of the key is 1 Run the program again and it shows that the value has been incremented: The value of the key is 2. Deploy JAR Containing Application Classes and Configuration files You can deploy a JAR that contains both application classes and configuration files. The sidecar image contains one or more JAR files, each of which can contain application classes, configuration files, or both. JAR files included in the sidecar image are available on the Coherence classpath and all Java classes in those JAR files are available for Classloading by the entire Coherence cluster. The configuration files must be included in the top level of a JAR file so that it can be referenced by the Coherence Helm chart. An example of the sidecar image layout: files/ lib/ coherence-operator-hello-server-config-1.0.0-SNAPSHOT.jar The file layout in the JAR file must be: META-INF/ META-INF/LICENSE META-INF/beans.xml META-INF/maven/org.javassist/javassist/pom.xml META-INF/services/org.glassfish.jersey.server.spi.ContainerProvider com/foo/demo/model/Price.class cache-config.xml pof-config.xml In the following example, Coherence is installed with the configuration files cache-config.xml and pof-config.xml , and the JAR file with all the Java classes in the Coherence classpath. $ helm --debug install coherence/coherence --name hello-server-config \\ --set userArtifacts.image=coherence-operator-hello-server-config:1.0.0-SNAPSHOT \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=cache-config.xml \\ --set store.pof.config=pof-config.xml Extract Heap Dump Files from a Kubernetes Coherence Pod You can use the operator to create log files and JVM heap dump files to debug issues in Coherence applications. Refer to Debugging in Coherence section in Oracle Fusion Middleware Developing Applications with Oracle Coherence . In this example, a .hprof file is collected for a heap dump: Execute the following command to list the pods of the installed operator and Coherence: ```bash $ kubectl get pods NAME READY STATUS RESTARTS AGE coherence-demo-storage-0 1/1 Running 0 45m coherence-demo-storage-1 1/1 Running 0 44m coherence-operator-7bc94cfb4-g4kz2 1/1 Running 0 47m 2. Get a shell to the storage node: ```bash $ kubectl exec -it coherence-demo-storage-0 -- /bin/bash Obtain the process ID (PID) of the Coherence process. Use jps to get the actual PID: ```bash bash-4.2# /usr/java/default/bin/jps 1 DefaultCacheServer 4230 Jps 4. Use the `jcmd` command to extract the heap dump and exit the shell: ```bash bash-4.2# /usr/java/default/bin/jcmd 1 GC.heap_dump /DefaultCache.hprof bash-4.2# exit Use kubectl exec to extract the heap dump: ```bash $ (kubectl exec coherence-demo-storage-0 -it -- cat /DefaultCache.hprof ) > DefaultCache.hprof You can also extract the heap dump using the following single command which can be used repeatedly: ```bash $ (kubectl exec coherence-demo-storage-0 -- /bin/bash -c \"rm -f /tmp/heap.hprof; /usr/java/default/bin/jcmd 1 GC.heap_dump /tmp/heap.hprof; cat /tmp/heap.hprof > /dev/stderr\" ) 2> heap.hprof 1: Heap dump file created In this command, the PID of the Coherence is assumed to be 1 . Also, the heap dump output is redirected to stderr to prevent the unsuppressable output from jcmd from showing up in the heap dump file. You can also refer to Produce and Extract Heap Dump in Samples. Extract Coherence Log Files from Kubernetes When you install the operator and Coherence with the log capture feature enabled, all the log messages from each Coherence cluster are captured in Elasticsearch, stored with Fluentd, and analyzed in Kibana. The common practice is to capture all the log messages from all of the Coherence clusters into a log aggregator than examining individual Coherence cluster node log files for errors. Refer to Enable Log Capture to View Logs in Kiabana sample for installing the operator and Coherence with log capture enabled. With log capture feature enabled, all the log messages from every Coherence cluster member are captured including the cluster members that are not running. The persistence of the stored log messages depends on how Fluentd is configured and is beyond the scope of this documentation. You can reconstruct the log message of each Coherence cluster member by querying Elasticsearch using curl , and manipulating the result using jq to produce output equivalent to a regular Coherence log file. Query Elasticsearch To reach the Elasticsearch endpoint using curl , you can use port forwarding in Kubernetes to access the Elasticsearch pod. Use the kubectl port-forward with the Elasticsearch name and the port number (default 9200). Use the following curl command to capture the log message for each Coherence cluster member: curl -s --output coherence-0.json http://ES_HOST:ES_PORT/coherence-cluster-*/_search?size=9999&q=host%3A%22my-20190514-storage-coherence-1%22&sort=@timestamp In the command, ES_HOST:ES_PORT is the Elasticsearch pod name and port number in which it can reached. This command extracts the log message from the Coherence cluster member named my-storage-coherence-0 and stores in the coherence-0.json file. Reformat the coherence-0.json file using jq : jq -j '.[\"hits\"] | .[\"hits\"] | .[] | .[\"_source\"] | .[\"@timestamp\"],\" \", .[\"product\"],\" <\", .[\"level\"], \"> (thread=\", .[\"thread\"], \", member=\", .[\"member\"], \"):\", .[\"log\"], \"newline\"' coherence-0.json | sed -e $'s/newline/\\\\\\n/g' > coherence-0.log Use Java Management Extensions (JMX) to Inspect and Manage Coherence JMX is the standard way to inspect and manage enterprise Java applications. Applications that expose themselves through JMX does not incur runtime performance penalty unless a tool is actively connected to the JMX connection. The Java Tutorials provide Introduction to JMX . The section JMX with Coherence in Oracle Fusion Middleware Developing Applications with Oracle Coherence describes how to use JMX with Coherence. The Coherence Helm chart must be installed with additional arguments so that you can use the JMX feature in the operator. This section covers how to install Coherence in a Kubernetes cluster with JMX enabled. You can refer to Access JMX in the Coherence Cluster Using JConsole and VisualVM in the Samples. Note that to fully appreciate this use case, deploy an application that uses Coherence and creates some caches. Such an application can be installed using the steps detailed in Deploy JAR Files . See the Quick Start guide to install the operator. Install Coherence using Helm chart with the following additional argument for JMX --set store.jmx.enabled=true* : $ helm --debug install coherence/coherence --name hello-example \\ --set userArtifacts.image=coherence-demo-app:1.0 \\ --set store.jmx.enabled=true \\ --set imagePullSecrets=sample-coherence-secret After the installation completes, you must expose the network port for JMX using the kubectl port-forward command. The instructions also include suggestions on how to use JConsole or VisualVM . In this guide, VisualVM is used to access and manipulate Coherence MBeans when running within Kubernetes. Download the opendmk_jmxremote_optional_jar JAR The JMX endpoint does not use RMI, instead it uses JMXMP. This requires an additional JAR on the classpath of the Java JMX client (VisualVM or JConsole). This can be downloaded as a Maven dependency: <dependency> <groupId>org.glassfish.external</groupId> <artifactId>opendmk_jmxremote_optional_jar</artifactId> <version>1.0-b01-ea</version> </dependency> or directly from: http://central.maven.org/maven2/org/glassfish/external/opendmk_jmxremote_optional_jar/1.0-b01-ea/opendmk_jmxremote_optional_jar-1.0-b01-ea.jar Run VisualVM with the Additional JAR Download and start VisualVM 1.4.2 version to enable connection to Coherence in Kubernetes: visualvm --jdkhome ${JAVA_HOME} --cp:a PATH_TO_DOWNLOADED.jar Manipulate the VisualVM UI to View the Coherence MBeans In the File menu, choose Add JMX Connection . In the Connection field, enter the value that was output by the Helm chart instructions. For example, service:jmx:jmxmp://127.0.0.1:9099 . Click OK . In the left navigation pane, click Applications . Double click the service:jmx:jmxmp... link. Click MBeans to open the MBeans browser. In the left tree view, open Coherence -> Cache -> DistributedCache and search for the cache created by your application. You can view the MBeans here . In particular, HighUnits , which defaults to 0. This can be interactively changed in visualvm . Expand the tree view and select Coherence -> Node and choose one of the nodes. You can view the MBeans here In particular, LoggingLevel , which defaults to 5. This can be also be interactively changed. Note that any changes to MBean attributes done in this way does not persist when the cluster restarts. To make persistent changes, you must modify the Coherence configuration files. Provide Arguments to the JVM that Runs Coherence Any production enterprise Java application must carefully tune the JVM arguments for maximum performance, and Coherence is no exception. This use case explains how to provide JVM arguments to Coherence running inside Kubernetes. You can also see Tune JVM Runtime Settings in the Samples. Also, refer to the Coherence Performance Tuning documentation for more information about performance tuning. There are several values in the values.yaml file of the Coherence Helm chart that provides JVM arguments to the JVM that runs Coherence within Kubernetes. See the source code for the authoritative documentation on these values. Such values include the following. --set left hand side Meaning store.maxHeap Heap size arguments to the JVM. The format should be the same as that used for Java's -Xms and -Xmx JVM options. If not set the JVM defaults are used. store.jmx.maxHeap Heap size arguments passed to the MBean server JVM. Same format and meaning as the preceding row. store.jvmArgs Options passed directly to the JVM running Coherence within Kubernetes store.javaOpts Miscellaneous JVM options to pass to the Coherence store container The following invocation installs and starts Coherence with specific values to be passed to the JVM. $ helm --debug install coherence/coherence --name hello-example \\ --set store.maxHeap=\"8g\" \\ --set store.jvmArgs=\"-Xloggc:/tmp/gc-log -server -Xcomp\" \\ --set store.javaOpts=\"-Dcoherence.log.level=6 -Djava.net.preferIPv4Stack=true\" \\ --set userArtifacts.image=hello-example-sidecar:1.0.0-SNAPSHOT \\ --set imagePullSecrets=sample-coherence-secret The JVM arguments include the store. arguments specified above, in addition to many others required by the operator and Coherence. -Xloggc:/tmp/gc-log -server -Xcomp -Xms8g -Xmx8g -Dcoherence.log.level=6 -Djava.net.preferIPv4Stack=true To inspect the full JVM arguments, you can use kubectl get logs -f <pod-name> and search for one of the arguments you specified. Kubernetes Specific Tasks Using Helm to Scale the Coherence Deployment The Coherence Operator leverages Kubernetes Statefulsets to ensure that the scale up and scale down operations allow the underlying Coherence cluster nodes sufficient time to rebalance the cluster data. You can refer to Scaling a Coherence Deployment in the Samples. Use the following command to scale up the number of Coherence cluster nodes: $ kubectl scale statefulsets <helm_release_name> --replicas=<number_of_nodes> For example, to increase the number of Coherence cluster nodes from 2 to 4 for a Coherence Operator installed using Helm chart and the Helm release named coherence_deploy : $ kubectl scale statefulsets coherence-deploy --replicas=4 You can monitor the progress of the cluster as Kubernetes adjusts to the new configuration. Kubernetes shows the number of pods being adjusted and the status of each pod. The pods state change to Running status. The coherence cluster also completes rebalancing before the increment. To scale down the number of Coherence cluster nodes: $ kubectl scale statefulsets coherence-deploy --replicas=3 The Coherence cluster rebalances the decrease in the number of nodes and the number of pods are adjusted accordingly. Perform a Safe Rolling Upgrade The sidecar docker image created using the JAR file containing application classes is tagged with a version number. The version number enables safe rolling upgrades. See Helm documentation for more information about safe rolling upgrades. The safe rolling upgrade allow you to instruct Kubernetes to replace the currently deployed version of the application classes with a different one. Coherence and Kubernetes ensures that this upgrade is done without any data loss or interruption of service. You can refer to Change Image Version for Coherence or Application Container Using Rolling Upgrade in the Samples. Assuming that you have installed the sidecar docker image using the steps detailed in the Deploy JAR Files section and the new sidecar docker image is available for the upgrade, you can use the following command to upgrade: $ helm --debug upgrade coherence/coherence --name hello-example --reuse-values \\ --set userArtifacts.image=hello-example-sidecar:1.0.1 --wait \\ --set imagePullSecrets=sample-coherence-secret In this example, the hello-example-sidecar:1.0.1 is the upgrade destination tagged image. The operator upgrades the application from hello-example-sidecar:1.0.0-SNAPSHOT to hello-example-sidecar:1.0.1 . Deploy Multiple Coherence Clusters The operator is designed to be installed once on a given Kubernetes cluster. Helm release of the Coherence Operator can monitor and manage all of the Coherence clusters installed in the given Kubernetes cluster. The following commands install the Coherence operator, then install multiple independent Coherence clusters on the same Kubernetes cluster. All the clusters are managed by one operator. You can refer to nstalling Multiple Coherence Clusters with One Operator in the samples. First, install the Coherence Operator with an empty list for the targetNamespaces parameter. This causes the operator to manage all namespaces for Coherence clusters. $ helm --debug install coherence/coherence-operator \\ --name sample-coherence-operator \\ --set \"targetNamespaces={}\" \\ --set imagePullSecrets=sample-coherence-secret Then, install two independent clusters which differ in the values passed to the cluster and userArtifacts.image parameters, and --name option. $ helm --debug install coherence/coherence \\ --set cluster=revenue-management \\ --set imagePullSecrets=sample-coherence-secret \\ --set userArtifacts.image=revenue-app:2.0.1 \\ --name revenue-management $ helm --debug install coherence/coherence \\ --set cluster=charging \\ --set imagePullSecrets=sample-coherence-secret \\ --set userArtifacts.image=charging-app:2.0.1 \\ --name charging The values must be unique to ensure that the two Coherence clusters to not merge and form one cluster. Note : Use the command helm inspect readme <chart name> to print the README.md of the chart. For example helm inspect readme coherence/coherence-operator prints the README.md for the operator chart. This includes documentation on all the possible values that can be configured with --set options to helm . Monitoring Performance and Logging See the following guides for monitoring services and viewing logs: Monitoring Coherence services via Grafana dashboards Accessing the EFK stack for viewing logs Note : Use of Prometheus and Grafana is available only when using the operator with Oracle Coherence 12.2.1.4.0. Configuring SSL Endpoints for Management over REST and Metrics Publishing This section describes how to configure SSL for management over REST and Prometheus metrics: * Configurating SSL Endpoints for Management over REST * Configuring SSL for Metrics Publishing for Prometheus Note: SSL and Management over REST and metrics publishing are available in Oracle Coherence 12.2.1.4. Configuring SSL Endpoints for Management over REST This section describes how to configure a two way SSL for Coherence management over REST with an example: Create Kubernetes secrets for your key store and trust store files. Coherence SSL requires Java key store and trust store files. These files are password protected. Let's define the password protected key store and trust store files: bash keyStore - name of the Java keystore file: myKeystore.jks keyStorePasswordFile - name of the keystore password file: storepassword.txt keyPasswordFile - name of the key password file: keypassword.txt trustStore - name of the Java trust store file: myTruststore.jks trustStorePasswordFile - name of the trust store password file: trustpassword.txt The following command creates a Kubernetes secret named ssl-secret which contains the Java key store and trust store files: ```bash kubectl create secret generic ssl-secret \\ --namespace myNamespace \\ --from-file=./myKeystore.jks \\ --from-file=./myTruststore.jks \\ --from-file=./storepassword.txt \\ --from-file=./keypassword.txt \\ --from-file=./trustpassword.txt ``` Create a YAML file, helm-values-ssl-management.yaml , to enable SSL for Coherence management over REST using the keystore, trust store, and password files in the ssl-secret : ```yaml store: management: ssl: enabled: true secrets: ssl-secret keyStore: myKeystore.jks keyStorePasswordFile: storepassword.txt keyPasswordFile: keypassword.txt keyStoreType: JKS trustStore: myTruststore.jks trustStorePasswordFile: trustpassword.txt trustStoreType: JKS requireClientCert: true readinessProbe: initialDelaySeconds: 10 `` 3. Install the Coherence Helm chart using the YAML file helm-values-ssl-management.yaml`: bash helm install coherence/coherence \\ --name coherence \\ --namespace myNamespace \\ --set imagePullSecrets=my-imagePull-secret \\ -f helm-values-ssl-management.yaml To verify that the Coherence management over REST is running with HTTPS, forward the management listen port to your local machine: and bash $ kubectl port-forward <pod name> 30000:30000 Access the REST endpoint using the URL https://localhost:30000/management/coherence/cluster . SSL certificates are required to access sites using the HTTPS protocol. If you have self-signed certificate, you will get the message that your connection is not secure from the browser. Click Advanced and then Add Exception... to allow the request to access the URL. Also, look for the following message in the log file of the Coherence pod: Started: HttpAcceptor{Name=Proxy:ManagementHttpProxy:HttpAcceptor, State=(SERVICE_STARTED), HttpServer=NettyHttpServer{Protocol=HTTPS, AuthMethod=cert} Configuring SSL for Metrics Publishing for Prometheus To configure a SSL endpoint for Coherence metrics: Create Kubernetes secrets for your key store and trust store files. Coherence SSL requires Java key store and trust store files. These files are password protected. Let's define the password protected key store and trust store files: bash keyStore - name of the Java keystore file: myKeystore.jks keyStorePasswordFile - name of the keystore password file: storepassword.txt keyPasswordFile - name of the key password file: keypassword.txt trustStore - name of the Java trust store file: myTruststore.jks trustStorePasswordFile - name of the trust store password file: trustpassword.txt The following command creates a Kubernetes secret named ssl-secret which contains the Java key store and trust store files: bash kubectl create secret generic ssl-secret \\ --namespace myNamespace \\ --from-file=./myKeystore.jks \\ --from-file=./myTruststore.jks \\ --from-file=./storepassword.txt \\ --from-file=./keypassword.txt \\ --from-file=./trustpassword.txt 2. Create a YAML file, helm-values-ssl-metrics.yaml , using the keystore, trust store, and password file stored in ssl-secret : yaml store: metrics: ssl: enabled: true secrets: ssl-secret keyStore: myKeystore.jks keyStorePasswordFile: storepassword.txt keyPasswordFile: keypassword.txt keyStoreType: JKS trustStore: myTruststore.jks trustStorePasswordFile: trustpassword.txt trustStoreType: JKS requireClientCert: true readinessProbe: initialDelaySeconds: 10 3. Install the Coherence helm chart using the YAML file, helm-values-ssl-metrics.yaml : bash helm install coherence/coherence \\ --name coherence \\ --namespace myNamespace \\ --set imagePullSecrets=my-imagePull-secret \\ -f helm-values-ssl-metrics.yaml To verify that the Coherence metrics for Prometheus is running with HTTPS, forward the Coherence metrics port and access the metrics from your local machine use the following commands: ```bash $ kubectl port-forward 9612:9612 $ curl -X GET https://localhost:9612/metrics --cacert --cert `` You can add --insecure if you use self-signed certificate. Also, look for the following message in the log file of the Coherence pod:</br> Started: HttpAcceptor{Name=Proxy:MetricsHttpProxy:HttpAcceptor, State=(SERVICE_STARTED), HttpServer=NettyHttpServer{Protocol=HTTPS, AuthMethod=cert}` To configure Prometheus SSL (TLS) connections with the Coherence metrics SSL endpoints, see https://github.com/helm/charts/blob/master/stable/prometheus-operator/README.md for more information about how to specify Kubernetes secrets that contain the certificates required for two-way SSL in Prometheus. To configure Prometheus to use SSL (TLS) connections, see https://prometheus.io/docs/prometheus/latest/configuration/configuration/#tls_config. After configuring Prometheus to use SSL, verify that the Prometheus is scraping Coherence metrics over HTTPS by forwarding the Prometheus service port to your local machine and access the following URL: $ kubectl port-forward <Prometheus pod> 9090:9090 http://localhost:9090/graph ``` You should see many vendor:coherence_* metrics. To enable SSL for both management over REST and metrics publishing for Prometheus, install the Coherence chart with both YAML files: ```bash helm --debug install coherence/coherence \\ --name coherence \\ --namespace myNamespace \\ --set imagePullSecrets=my-imagePull-secret \\ -f helm-values-ssl-management.yaml,helm-values-ssl-metrics.yaml","title":"User Guide"},{"location":"user-guide/#user-guide","text":"The User Guide provides detailed information about how to install Coherence and the Coherence Operator in your Kubernetes cluster, and how to use the operator to manage Coherence clusters. Guide to this Document Before You Begin Common Coherence Tasks Provide Configuration Files and Application Classes to the Coherence Cluster within Kubernetes Deploy JAR Files Create a JAR File Create a Docker image for the Sidecar Use Helm to Install Coherence Create the Local Extend Client Configuration Remove Coherence Deploy Configuration Files Create the Server Side Cache Configuration Create a Docker Image for the Sidecar Run the client program Deploy JAR Containing Application Classes and Configuration files Extract Heap Dump Files from a Kubernetes Coherence Pod Extract Coherence Log Files from Kubernetes Query Elasticsearch Use Java Management Extensions (JMX) to Inspect and Manage Coherence Download the opendmk_jmxremote_optional_jar JAR Run VisualVM with the Additional JAR Manipulate the VisualVM UI to View the Coherence MBeans Provide Arguments to the JVM that Runs Coherence Kubernetes Specific Tasks Using Helm to Scale the Coherence Deployment Perform a Safe Rolling Upgrade Deploy Multiple Coherence Clusters Monitoring Performance and Logging Configuring SSL Endpoints for Management over REST and Metrics Publishing Configuring SSL Endpoints for Management over REST Configuring SSL for Metrics Publishing for Prometheus","title":"User Guide"},{"location":"user-guide/#guide-to-this-document","text":"The User Guide provides exclusive steps for managing Coherence within Kubernetes. For most of the administrative tasks for managing Kubernetes, refer to Kubernetes Documentation. The information in this guide is organized into sections that are common and Kubernetes specific tasks. Refer to these sections for managing Coherence: * Common Coherence Tasks * Kubernetes Specific Tasks","title":"Guide to this Document"},{"location":"user-guide/#before-you-begin","text":"Refere to Before You Begin section in the Quick Start guide. All the examples in this guide are installed in a Kubernetes namespace called sample-coherence-ns . To set this namespace as the active namespace, execute the command: $ kubectl config set-context $(kubectl config current-context) --namespace=sample-coherence-ns","title":"Before You Begin"},{"location":"user-guide/#common-coherence-tasks","text":"The most common administrative tasks with Coherence are Overriding the Default Cache Configuration File and deploying JARs for Processing Data in a Cache . Most of the administrative tasks to do with Coherence apply when running within Kubernetes. The Oracle Coherence documentation remains a very useful resource. This section covers a few common scenarios that require special treatment regarding Kubernetes.","title":"Common Coherence Tasks"},{"location":"user-guide/#provide-configuration-files-and-application-classes-to-the-coherence-cluster-within-kubernetes","text":"This section explains how to make custom configuration and JAR files available to your Coherence cluster running in Kubernetes. This approach can be used for any administrative tasks that require to make JAR, XML, or other configuration files available to the Coherence cluster. You can refer to Add Application Jars/Config to a Coherence Deployment in the Samples. The Coherence Operator uses the sidecar pattern , as recommended by Kubernetes , to make resources available to Coherence within the Kubernetes cluster. Docker containers are the most flexible way to allow interaction with the Coherence cluster running in Kubernetes. The steps to use the sidecar pattern include: Determine the JARs or configuration files that you want to make them available to the cluster. Package the files in a Docker image and deploy that image to a Docker registry accessible to the Kubernetes cluster. Install the docker image using the Helm chart.","title":"Provide Configuration Files and Application Classes to the Coherence Cluster within Kubernetes"},{"location":"user-guide/#deploy-jar-files","text":"The concept to create a JAR file is derived from Building Your First Extend Application in Oracle Fusion Middleware Developing Remote Clients for Oracle Coherence for use within Kubernetes.","title":"Deploy JAR Files"},{"location":"user-guide/#create-a-jar-file","text":"To create a JAR file: Create a directory for the files: bash $ mkdir -p hello-example/files/lib $ cd hello-example Create a Java program to access the cluster. Save the java file as HelloExample.java in the hello-example directory. ```java import java.io.Serializable; import java.text.SimpleDateFormat; import java.util.Date; import com.tangosol.net.NamedCache; import com.tangosol.net.CacheFactory; public class HelloExample { public static void main(String[] asArgs) throws Throwable { NamedCache cache = CacheFactory.getCache(\"hello-example\"); Timestamp ts = cache.get(\"ts1\"); cache.put(\"ts1\", ts = new Timestamp((null == ts) ? Long.MIN_VALUE : ts.currentTime)); System.out.println(\"The value of the key is \" + ts.toString()); } public static class Timestamp implements Serializable { public long currentTime; public long previousTime; public Timestamp(long previousTime) { this.currentTime = System.currentTimeMillis(); this.previousTime = previousTime; } public String toString() { SimpleDateFormat f = new SimpleDateFormat(\"HH:mm:ss\"); return \"Timestamp: previousTime: \" + f.format(new Date(previousTime)) + \" currentTime: \" + f.format(new Date(currentTime)); } } } This program uses a static inner class, `Timestamp`, to store the values in Coherence. Any Java object that is stored in Coherence must be accessible by Coherence in compiled form. The Java objects are compiled classes in JAR files on the Coherence classpath. Therefore, compile and archive the file: ``` $ javac -cp .:${COHERENCE_HOME}/lib/coherence.jar HelloExample.java $ jar -cf files/lib/hello-example.jar *.class ``` #### Create a Docker image for the Sidecar Package the created JAR file within the sidecar Docker image: 1. Create a `Dockerfile` with the following contents: ```bash FROM oraclelinux:7-slim RUN mkdir -p /files/lib COPY files/lib/hello-example.jar files/lib ``` Note that the JAR file is placed in the `files/lib` directory relative to the root of the Docker image. This is the default location where Coherence looks for JAR files to add to the classpath. Any JAR files in `files/lib` are added to the classpath. You can change the location where Coherence must look for JARs to add to the classpath. 2. Ensure that the Docker is running on the current host. If not, refer to [ Get Started](https://docs.docker.com/get-started/) in Docker documentation. 3. Build and tag a Docker image for `hello-example-sidecar`: ```bash $ docker build -t \"hello-example-sidecar:1.0.0-SNAPSHOT\" . ``` The trailing dot \".\" in the command refers to run the build relative to the current directory. 4. Push the created Docker image to the Docker registry which the Kubernetes cluster can access. The [Quick Start](quickstart.md#obtain-images-from-oracle-container-registry) guide describes how to make the Kubernetes cluster can pull the images using the Docker credentials. > **Note:** If you are using a local Kubernetes, you can omit this step, since the Kubernetes pulls from the same Docker server as the one to which the local build command built the image. #### Use Helm to Install Coherence Install Coherence using the Helm chart with the details of the sidecar image arguments: ```bash $ helm --debug install coherence/coherence --name hello-example \\ --set userArtifacts.image=hello-example-sidecar:1.0.0-SNAPSHOT \\ --set imagePullSecrets=sample-coherence-secret Note: If the JAR files are in a different location within the sidecar Docker image, use the --set userArtifacts.libDir=<absolute path within docker image> argument to helm install to configure the correct location. In a new terminal window, set up a Kubernetes port forward to expose the Extend port so that your local client can use it: $ export POD_NAME=$(kubectl get pods --namespace default -l \\ \"app=coherence,release=hello-example\" -o jsonpath=\"{.items[0].metadata.name}\") $ kubectl --namespace default port-forward $POD_NAME 20000:20000 This prints the following output and blocks the shell: Forwarding from 127.0.0.1:20000 -> 20000 Forwarding from [::1]:20000 -> 20000","title":"Create a JAR File"},{"location":"user-guide/#create-the-local-extend-client-configuration","text":"A local client configuration is necessary because the local client connects to the service through Coherence*Extend. Create a file named hello-client-config.xml with the following contents: <cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"> <caching-scheme-mapping> <cache-mapping> <cache-name>*</cache-name> <scheme-name>thin-remote</scheme-name> </cache-mapping> </caching-scheme-mapping> <caching-schemes> <remote-cache-scheme> <scheme-name>thin-remote</scheme-name> <service-name>Proxy</service-name> <initiator-config> <tcp-initiator> <remote-addresses> <socket-address> <address>127.0.0.1</address> <port>20000</port> </socket-address> </remote-addresses> </tcp-initiator> </initiator-config> </remote-cache-scheme> </caching-schemes> </cache-config> Run the client using the following command: $ java -cp files/lib/hello-example.jar:${COHERENCE_HOME}/lib/coherence.jar \\ -Dcoherence.log.level=1 -Dcoherence.distributed.localstorage=false \\ -Dcoherence.cacheconfig=$PWD/hello-client-config.xml HelloExample An output similar to the following is displayed: The value of the key is Timestamp: previousTime: 11:47:04 currentTime: 16:09:30 Run the command again and it shows the updated Timestamp : The value of the key is Timestamp: previousTime: 16:09:30 currentTime: 16:10:20","title":"Create the Local Extend Client Configuration"},{"location":"user-guide/#remove-coherence","text":"$ helm delete --purge hello-example","title":"Remove Coherence"},{"location":"user-guide/#deploy-configuration-files","text":"The similar sidecar approach is used to deploy configuration files to Coherence inside Kubernetes. Though, Coherence has the necessary built-in configuration, a subset of that configuration is used in this example. Create a directory for the files: bash $ cd .. $ mkdir -p hello-config-example/files/conf $ cd hello-config-example 2. Create the Java program to access the cluster. In the same directory, create a simple java program HelloConfigXml.java : ```java import com.tangosol.net.CacheFactory; import com.tangosol.net.NamedCache; public class HelloConfigXml { public static void main(String[] asArgs) throws Throwable { NamedCache cache = CacheFactory.getCache(\"hello-config-xml\"); Integer IValue = (Integer) cache.get(\"key\"); IValue = (null == IValue) ? Integer.valueOf(1) : Integer.valueOf(IValue + 1); cache.put(\"key\", IValue); System.out.println(\"The value of the key is \" + IValue); } } `` 3. Create the following XML file, next to the java file, called hello-client-config.xml`: ``` hello-config-xml thin-remote thin-remote Proxy 127.0.0.1 20000 #### Create the Server Side Cache Configuration Create the file `files/conf/hello-server-config.xml` with the following content: hello-config-xml ${coherence.profile near}-${coherence.client direct} near-direct {front-limit-entries 10000} thin-direct <!-- near caching scheme for extend clients --> <near-scheme> <scheme-name>near-remote</scheme-name> <scheme-ref>near-direct</scheme-ref> <back-scheme> <remote-cache-scheme> <scheme-ref>thin-remote</scheme-ref> </remote-cache-scheme> </back-scheme> </near-scheme> <!-- remote caching scheme for accessing the proxy from extend clients --> <remote-cache-scheme> <scheme-name>thin-remote</scheme-name> <service-name>RemoteCache</service-name> <proxy-service-name>Proxy</proxy-service-name> </remote-cache-scheme> <!-- partitioned caching scheme for clustered clients --> <distributed-scheme> <scheme-name>thin-direct</scheme-name> <scheme-ref>server</scheme-ref> <local-storage system-property=\"coherence.distributed.localstorage\">false</local-storage> <autostart>false</autostart> </distributed-scheme> <!-- partitioned caching scheme for servers --> <distributed-scheme> <scheme-name>server</scheme-name> <service-name>PartitionedCache</service-name> <local-storage system-property=\"coherence.distributed.localstorage\">true</local-storage> <backing-map-scheme> <local-scheme> <high-units>{back-limit-bytes 0B}</high-units> </local-scheme> </backing-map-scheme> <autostart>true</autostart> </distributed-scheme> <!-- proxy scheme that allows extend clients to connect to the cluster over TCP/IP --> <proxy-scheme> <service-name>Proxy</service-name> <acceptor-config> <tcp-acceptor> <local-address> <address system-property=\"coherence.extend.address\"/> <port system-property=\"coherence.extend.port\"/> </local-address> </tcp-acceptor> </acceptor-config> <load-balancer>client</load-balancer> <autostart>true</autostart> </proxy-scheme> #### Create a Docker Image for the Sidecar Package the XML file within the sidecar Docker image: 1. Create a `Dockerfile` next to the Java file with the following contents. ```bash FROM oraclelinux:7-slim RUN mkdir -p /files/conf COPY files/conf/hello-server-config.xml files/conf/hello-server-config.xml ``` Note that the XML file is placed in the `files/conf` directory relative to the root of the Docker image. This is the default location where Coherence looks for configuration files that apply to Coherence. You can change the location where Coherence looks for configuration files to add to the classpath. 2. Ensure Docker is running on the current host. If not, refer to [Get Started with Docker](https://docs.docker.com/get-started/). 3. Build and tag a Docker image for `hello-server-config-sidecar`: ```bash $ docker build -t \"hello-server-config-sidecar:1.0.0-SNAPSHOT\" . ``` Note that the trailing dot \".\" is very significant which means to run the build relative to the current directory. 4. Push your image to the Docker registry which the Kubernetes cluster can access. The [Quick Start](./quickstart.md) guide describes how to make the Kubernetes cluster pull the images using the Docker credentials. > **Note:** If you are using a local Kubernetes, you can omit this step, since the Kubernetes pulls from the same Docker server as the one to which the local build command built the image. 5. Install the Helm chart with the sidecar image arguments: ```bash $ helm --debug install coherence/coherence --name hello-server-config \\ --set userArtifacts.image=hello-server-config-sidecar:1.0.0-SNAPSHOT \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=hello-server-config.xml Note: If your XML files are in a different location within the sidecar Docker image, use the --set userArtifacts.configDir=<absolute path within docker image> argument to helm install to configure the correct location. In a new terminal window, set up a Kubernetes port forward to expose the Extend port so that your local client can use it. The instructions for doing this are from the output of the helm install command: $ export POD_NAME=$(kubectl get pods --namespace default -l \"app=coherence,release=hello-server-config\" -o jsonpath=\"{.items[0].metadata.name}\") $ kubectl --namespace default port-forward $POD_NAME 20000:20000 This prints the following output and blocks the shell: Forwarding from 127.0.0.1:20000 -> 20000 Forwarding from [::1]:20000 -> 20000","title":"Deploy Configuration Files"},{"location":"user-guide/#run-the-client-program","text":"In the same directory as the XML and Java source files, run the client: $ javac -cp .:${COHERENCE_HOME}/lib/coherence.jar HelloConfigXml.java $ java -cp .:${COHERENCE_HOME}/lib/coherence.jar \\ -Dcoherence.distributed.localstorage=false \\ -Dcoherence.cacheconfig=$PWD/hello-client-config.xml -Dcoherence.log.level=1 HelloConfigXml The correct coherence.jar must be available at ${COHERENCE_HOME}/lib/coherence.jar . An output similar to the following is displayed: The value of the key is 1 Run the program again and it shows that the value has been incremented: The value of the key is 2.","title":"Run the client program"},{"location":"user-guide/#deploy-jar-containing-application-classes-and-configuration-files","text":"You can deploy a JAR that contains both application classes and configuration files. The sidecar image contains one or more JAR files, each of which can contain application classes, configuration files, or both. JAR files included in the sidecar image are available on the Coherence classpath and all Java classes in those JAR files are available for Classloading by the entire Coherence cluster. The configuration files must be included in the top level of a JAR file so that it can be referenced by the Coherence Helm chart. An example of the sidecar image layout: files/ lib/ coherence-operator-hello-server-config-1.0.0-SNAPSHOT.jar The file layout in the JAR file must be: META-INF/ META-INF/LICENSE META-INF/beans.xml META-INF/maven/org.javassist/javassist/pom.xml META-INF/services/org.glassfish.jersey.server.spi.ContainerProvider com/foo/demo/model/Price.class cache-config.xml pof-config.xml In the following example, Coherence is installed with the configuration files cache-config.xml and pof-config.xml , and the JAR file with all the Java classes in the Coherence classpath. $ helm --debug install coherence/coherence --name hello-server-config \\ --set userArtifacts.image=coherence-operator-hello-server-config:1.0.0-SNAPSHOT \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=cache-config.xml \\ --set store.pof.config=pof-config.xml","title":"Deploy JAR Containing Application Classes and Configuration files"},{"location":"user-guide/#extract-heap-dump-files-from-a-kubernetes-coherence-pod","text":"You can use the operator to create log files and JVM heap dump files to debug issues in Coherence applications. Refer to Debugging in Coherence section in Oracle Fusion Middleware Developing Applications with Oracle Coherence . In this example, a .hprof file is collected for a heap dump: Execute the following command to list the pods of the installed operator and Coherence: ```bash $ kubectl get pods NAME READY STATUS RESTARTS AGE coherence-demo-storage-0 1/1 Running 0 45m coherence-demo-storage-1 1/1 Running 0 44m coherence-operator-7bc94cfb4-g4kz2 1/1 Running 0 47m 2. Get a shell to the storage node: ```bash $ kubectl exec -it coherence-demo-storage-0 -- /bin/bash Obtain the process ID (PID) of the Coherence process. Use jps to get the actual PID: ```bash bash-4.2# /usr/java/default/bin/jps 1 DefaultCacheServer 4230 Jps 4. Use the `jcmd` command to extract the heap dump and exit the shell: ```bash bash-4.2# /usr/java/default/bin/jcmd 1 GC.heap_dump /DefaultCache.hprof bash-4.2# exit Use kubectl exec to extract the heap dump: ```bash $ (kubectl exec coherence-demo-storage-0 -it -- cat /DefaultCache.hprof ) > DefaultCache.hprof You can also extract the heap dump using the following single command which can be used repeatedly: ```bash $ (kubectl exec coherence-demo-storage-0 -- /bin/bash -c \"rm -f /tmp/heap.hprof; /usr/java/default/bin/jcmd 1 GC.heap_dump /tmp/heap.hprof; cat /tmp/heap.hprof > /dev/stderr\" ) 2> heap.hprof 1: Heap dump file created In this command, the PID of the Coherence is assumed to be 1 . Also, the heap dump output is redirected to stderr to prevent the unsuppressable output from jcmd from showing up in the heap dump file. You can also refer to Produce and Extract Heap Dump in Samples.","title":"Extract Heap Dump Files from a Kubernetes Coherence Pod"},{"location":"user-guide/#extract-coherence-log-files-from-kubernetes","text":"When you install the operator and Coherence with the log capture feature enabled, all the log messages from each Coherence cluster are captured in Elasticsearch, stored with Fluentd, and analyzed in Kibana. The common practice is to capture all the log messages from all of the Coherence clusters into a log aggregator than examining individual Coherence cluster node log files for errors. Refer to Enable Log Capture to View Logs in Kiabana sample for installing the operator and Coherence with log capture enabled. With log capture feature enabled, all the log messages from every Coherence cluster member are captured including the cluster members that are not running. The persistence of the stored log messages depends on how Fluentd is configured and is beyond the scope of this documentation. You can reconstruct the log message of each Coherence cluster member by querying Elasticsearch using curl , and manipulating the result using jq to produce output equivalent to a regular Coherence log file.","title":"Extract Coherence Log Files from Kubernetes"},{"location":"user-guide/#query-elasticsearch","text":"To reach the Elasticsearch endpoint using curl , you can use port forwarding in Kubernetes to access the Elasticsearch pod. Use the kubectl port-forward with the Elasticsearch name and the port number (default 9200). Use the following curl command to capture the log message for each Coherence cluster member: curl -s --output coherence-0.json http://ES_HOST:ES_PORT/coherence-cluster-*/_search?size=9999&q=host%3A%22my-20190514-storage-coherence-1%22&sort=@timestamp In the command, ES_HOST:ES_PORT is the Elasticsearch pod name and port number in which it can reached. This command extracts the log message from the Coherence cluster member named my-storage-coherence-0 and stores in the coherence-0.json file. Reformat the coherence-0.json file using jq : jq -j '.[\"hits\"] | .[\"hits\"] | .[] | .[\"_source\"] | .[\"@timestamp\"],\" \", .[\"product\"],\" <\", .[\"level\"], \"> (thread=\", .[\"thread\"], \", member=\", .[\"member\"], \"):\", .[\"log\"], \"newline\"' coherence-0.json | sed -e $'s/newline/\\\\\\n/g' > coherence-0.log","title":"Query Elasticsearch"},{"location":"user-guide/#use-java-management-extensions-jmx-to-inspect-and-manage-coherence","text":"JMX is the standard way to inspect and manage enterprise Java applications. Applications that expose themselves through JMX does not incur runtime performance penalty unless a tool is actively connected to the JMX connection. The Java Tutorials provide Introduction to JMX . The section JMX with Coherence in Oracle Fusion Middleware Developing Applications with Oracle Coherence describes how to use JMX with Coherence. The Coherence Helm chart must be installed with additional arguments so that you can use the JMX feature in the operator. This section covers how to install Coherence in a Kubernetes cluster with JMX enabled. You can refer to Access JMX in the Coherence Cluster Using JConsole and VisualVM in the Samples. Note that to fully appreciate this use case, deploy an application that uses Coherence and creates some caches. Such an application can be installed using the steps detailed in Deploy JAR Files . See the Quick Start guide to install the operator. Install Coherence using Helm chart with the following additional argument for JMX --set store.jmx.enabled=true* : $ helm --debug install coherence/coherence --name hello-example \\ --set userArtifacts.image=coherence-demo-app:1.0 \\ --set store.jmx.enabled=true \\ --set imagePullSecrets=sample-coherence-secret After the installation completes, you must expose the network port for JMX using the kubectl port-forward command. The instructions also include suggestions on how to use JConsole or VisualVM . In this guide, VisualVM is used to access and manipulate Coherence MBeans when running within Kubernetes.","title":"Use Java Management Extensions (JMX) to Inspect and Manage Coherence"},{"location":"user-guide/#download-the-opendmk_jmxremote_optional_jar-jar","text":"The JMX endpoint does not use RMI, instead it uses JMXMP. This requires an additional JAR on the classpath of the Java JMX client (VisualVM or JConsole). This can be downloaded as a Maven dependency: <dependency> <groupId>org.glassfish.external</groupId> <artifactId>opendmk_jmxremote_optional_jar</artifactId> <version>1.0-b01-ea</version> </dependency> or directly from: http://central.maven.org/maven2/org/glassfish/external/opendmk_jmxremote_optional_jar/1.0-b01-ea/opendmk_jmxremote_optional_jar-1.0-b01-ea.jar","title":"Download the opendmk_jmxremote_optional_jar JAR"},{"location":"user-guide/#run-visualvm-with-the-additional-jar","text":"Download and start VisualVM 1.4.2 version to enable connection to Coherence in Kubernetes: visualvm --jdkhome ${JAVA_HOME} --cp:a PATH_TO_DOWNLOADED.jar","title":"Run VisualVM with the Additional JAR"},{"location":"user-guide/#manipulate-the-visualvm-ui-to-view-the-coherence-mbeans","text":"In the File menu, choose Add JMX Connection . In the Connection field, enter the value that was output by the Helm chart instructions. For example, service:jmx:jmxmp://127.0.0.1:9099 . Click OK . In the left navigation pane, click Applications . Double click the service:jmx:jmxmp... link. Click MBeans to open the MBeans browser. In the left tree view, open Coherence -> Cache -> DistributedCache and search for the cache created by your application. You can view the MBeans here . In particular, HighUnits , which defaults to 0. This can be interactively changed in visualvm . Expand the tree view and select Coherence -> Node and choose one of the nodes. You can view the MBeans here In particular, LoggingLevel , which defaults to 5. This can be also be interactively changed. Note that any changes to MBean attributes done in this way does not persist when the cluster restarts. To make persistent changes, you must modify the Coherence configuration files.","title":"Manipulate the VisualVM UI to View the Coherence MBeans"},{"location":"user-guide/#provide-arguments-to-the-jvm-that-runs-coherence","text":"Any production enterprise Java application must carefully tune the JVM arguments for maximum performance, and Coherence is no exception. This use case explains how to provide JVM arguments to Coherence running inside Kubernetes. You can also see Tune JVM Runtime Settings in the Samples. Also, refer to the Coherence Performance Tuning documentation for more information about performance tuning. There are several values in the values.yaml file of the Coherence Helm chart that provides JVM arguments to the JVM that runs Coherence within Kubernetes. See the source code for the authoritative documentation on these values. Such values include the following. --set left hand side Meaning store.maxHeap Heap size arguments to the JVM. The format should be the same as that used for Java's -Xms and -Xmx JVM options. If not set the JVM defaults are used. store.jmx.maxHeap Heap size arguments passed to the MBean server JVM. Same format and meaning as the preceding row. store.jvmArgs Options passed directly to the JVM running Coherence within Kubernetes store.javaOpts Miscellaneous JVM options to pass to the Coherence store container The following invocation installs and starts Coherence with specific values to be passed to the JVM. $ helm --debug install coherence/coherence --name hello-example \\ --set store.maxHeap=\"8g\" \\ --set store.jvmArgs=\"-Xloggc:/tmp/gc-log -server -Xcomp\" \\ --set store.javaOpts=\"-Dcoherence.log.level=6 -Djava.net.preferIPv4Stack=true\" \\ --set userArtifacts.image=hello-example-sidecar:1.0.0-SNAPSHOT \\ --set imagePullSecrets=sample-coherence-secret The JVM arguments include the store. arguments specified above, in addition to many others required by the operator and Coherence. -Xloggc:/tmp/gc-log -server -Xcomp -Xms8g -Xmx8g -Dcoherence.log.level=6 -Djava.net.preferIPv4Stack=true To inspect the full JVM arguments, you can use kubectl get logs -f <pod-name> and search for one of the arguments you specified.","title":"Provide Arguments to the JVM that Runs Coherence"},{"location":"user-guide/#kubernetes-specific-tasks","text":"","title":"Kubernetes Specific Tasks"},{"location":"user-guide/#using-helm-to-scale-the-coherence-deployment","text":"The Coherence Operator leverages Kubernetes Statefulsets to ensure that the scale up and scale down operations allow the underlying Coherence cluster nodes sufficient time to rebalance the cluster data. You can refer to Scaling a Coherence Deployment in the Samples. Use the following command to scale up the number of Coherence cluster nodes: $ kubectl scale statefulsets <helm_release_name> --replicas=<number_of_nodes> For example, to increase the number of Coherence cluster nodes from 2 to 4 for a Coherence Operator installed using Helm chart and the Helm release named coherence_deploy : $ kubectl scale statefulsets coherence-deploy --replicas=4 You can monitor the progress of the cluster as Kubernetes adjusts to the new configuration. Kubernetes shows the number of pods being adjusted and the status of each pod. The pods state change to Running status. The coherence cluster also completes rebalancing before the increment. To scale down the number of Coherence cluster nodes: $ kubectl scale statefulsets coherence-deploy --replicas=3 The Coherence cluster rebalances the decrease in the number of nodes and the number of pods are adjusted accordingly.","title":"Using Helm to Scale the Coherence Deployment"},{"location":"user-guide/#perform-a-safe-rolling-upgrade","text":"The sidecar docker image created using the JAR file containing application classes is tagged with a version number. The version number enables safe rolling upgrades. See Helm documentation for more information about safe rolling upgrades. The safe rolling upgrade allow you to instruct Kubernetes to replace the currently deployed version of the application classes with a different one. Coherence and Kubernetes ensures that this upgrade is done without any data loss or interruption of service. You can refer to Change Image Version for Coherence or Application Container Using Rolling Upgrade in the Samples. Assuming that you have installed the sidecar docker image using the steps detailed in the Deploy JAR Files section and the new sidecar docker image is available for the upgrade, you can use the following command to upgrade: $ helm --debug upgrade coherence/coherence --name hello-example --reuse-values \\ --set userArtifacts.image=hello-example-sidecar:1.0.1 --wait \\ --set imagePullSecrets=sample-coherence-secret In this example, the hello-example-sidecar:1.0.1 is the upgrade destination tagged image. The operator upgrades the application from hello-example-sidecar:1.0.0-SNAPSHOT to hello-example-sidecar:1.0.1 .","title":"Perform a Safe Rolling Upgrade"},{"location":"user-guide/#deploy-multiple-coherence-clusters","text":"The operator is designed to be installed once on a given Kubernetes cluster. Helm release of the Coherence Operator can monitor and manage all of the Coherence clusters installed in the given Kubernetes cluster. The following commands install the Coherence operator, then install multiple independent Coherence clusters on the same Kubernetes cluster. All the clusters are managed by one operator. You can refer to nstalling Multiple Coherence Clusters with One Operator in the samples. First, install the Coherence Operator with an empty list for the targetNamespaces parameter. This causes the operator to manage all namespaces for Coherence clusters. $ helm --debug install coherence/coherence-operator \\ --name sample-coherence-operator \\ --set \"targetNamespaces={}\" \\ --set imagePullSecrets=sample-coherence-secret Then, install two independent clusters which differ in the values passed to the cluster and userArtifacts.image parameters, and --name option. $ helm --debug install coherence/coherence \\ --set cluster=revenue-management \\ --set imagePullSecrets=sample-coherence-secret \\ --set userArtifacts.image=revenue-app:2.0.1 \\ --name revenue-management $ helm --debug install coherence/coherence \\ --set cluster=charging \\ --set imagePullSecrets=sample-coherence-secret \\ --set userArtifacts.image=charging-app:2.0.1 \\ --name charging The values must be unique to ensure that the two Coherence clusters to not merge and form one cluster. Note : Use the command helm inspect readme <chart name> to print the README.md of the chart. For example helm inspect readme coherence/coherence-operator prints the README.md for the operator chart. This includes documentation on all the possible values that can be configured with --set options to helm .","title":"Deploy Multiple Coherence Clusters"},{"location":"user-guide/#monitoring-performance-and-logging","text":"See the following guides for monitoring services and viewing logs: Monitoring Coherence services via Grafana dashboards Accessing the EFK stack for viewing logs Note : Use of Prometheus and Grafana is available only when using the operator with Oracle Coherence 12.2.1.4.0.","title":"Monitoring Performance and Logging"},{"location":"user-guide/#configuring-ssl-endpoints-for-management-over-rest-and-metrics-publishing","text":"This section describes how to configure SSL for management over REST and Prometheus metrics: * Configurating SSL Endpoints for Management over REST * Configuring SSL for Metrics Publishing for Prometheus Note: SSL and Management over REST and metrics publishing are available in Oracle Coherence 12.2.1.4.","title":"Configuring SSL Endpoints for Management over REST and Metrics Publishing"},{"location":"user-guide/#configuring-ssl-endpoints-for-management-over-rest","text":"This section describes how to configure a two way SSL for Coherence management over REST with an example: Create Kubernetes secrets for your key store and trust store files. Coherence SSL requires Java key store and trust store files. These files are password protected. Let's define the password protected key store and trust store files: bash keyStore - name of the Java keystore file: myKeystore.jks keyStorePasswordFile - name of the keystore password file: storepassword.txt keyPasswordFile - name of the key password file: keypassword.txt trustStore - name of the Java trust store file: myTruststore.jks trustStorePasswordFile - name of the trust store password file: trustpassword.txt The following command creates a Kubernetes secret named ssl-secret which contains the Java key store and trust store files: ```bash kubectl create secret generic ssl-secret \\ --namespace myNamespace \\ --from-file=./myKeystore.jks \\ --from-file=./myTruststore.jks \\ --from-file=./storepassword.txt \\ --from-file=./keypassword.txt \\ --from-file=./trustpassword.txt ``` Create a YAML file, helm-values-ssl-management.yaml , to enable SSL for Coherence management over REST using the keystore, trust store, and password files in the ssl-secret : ```yaml store: management: ssl: enabled: true secrets: ssl-secret keyStore: myKeystore.jks keyStorePasswordFile: storepassword.txt keyPasswordFile: keypassword.txt keyStoreType: JKS trustStore: myTruststore.jks trustStorePasswordFile: trustpassword.txt trustStoreType: JKS requireClientCert: true readinessProbe: initialDelaySeconds: 10 `` 3. Install the Coherence Helm chart using the YAML file helm-values-ssl-management.yaml`: bash helm install coherence/coherence \\ --name coherence \\ --namespace myNamespace \\ --set imagePullSecrets=my-imagePull-secret \\ -f helm-values-ssl-management.yaml To verify that the Coherence management over REST is running with HTTPS, forward the management listen port to your local machine: and bash $ kubectl port-forward <pod name> 30000:30000 Access the REST endpoint using the URL https://localhost:30000/management/coherence/cluster . SSL certificates are required to access sites using the HTTPS protocol. If you have self-signed certificate, you will get the message that your connection is not secure from the browser. Click Advanced and then Add Exception... to allow the request to access the URL. Also, look for the following message in the log file of the Coherence pod: Started: HttpAcceptor{Name=Proxy:ManagementHttpProxy:HttpAcceptor, State=(SERVICE_STARTED), HttpServer=NettyHttpServer{Protocol=HTTPS, AuthMethod=cert}","title":"Configuring SSL Endpoints for Management over REST"},{"location":"user-guide/#configuring-ssl-for-metrics-publishing-for-prometheus","text":"To configure a SSL endpoint for Coherence metrics: Create Kubernetes secrets for your key store and trust store files. Coherence SSL requires Java key store and trust store files. These files are password protected. Let's define the password protected key store and trust store files: bash keyStore - name of the Java keystore file: myKeystore.jks keyStorePasswordFile - name of the keystore password file: storepassword.txt keyPasswordFile - name of the key password file: keypassword.txt trustStore - name of the Java trust store file: myTruststore.jks trustStorePasswordFile - name of the trust store password file: trustpassword.txt The following command creates a Kubernetes secret named ssl-secret which contains the Java key store and trust store files: bash kubectl create secret generic ssl-secret \\ --namespace myNamespace \\ --from-file=./myKeystore.jks \\ --from-file=./myTruststore.jks \\ --from-file=./storepassword.txt \\ --from-file=./keypassword.txt \\ --from-file=./trustpassword.txt 2. Create a YAML file, helm-values-ssl-metrics.yaml , using the keystore, trust store, and password file stored in ssl-secret : yaml store: metrics: ssl: enabled: true secrets: ssl-secret keyStore: myKeystore.jks keyStorePasswordFile: storepassword.txt keyPasswordFile: keypassword.txt keyStoreType: JKS trustStore: myTruststore.jks trustStorePasswordFile: trustpassword.txt trustStoreType: JKS requireClientCert: true readinessProbe: initialDelaySeconds: 10 3. Install the Coherence helm chart using the YAML file, helm-values-ssl-metrics.yaml : bash helm install coherence/coherence \\ --name coherence \\ --namespace myNamespace \\ --set imagePullSecrets=my-imagePull-secret \\ -f helm-values-ssl-metrics.yaml To verify that the Coherence metrics for Prometheus is running with HTTPS, forward the Coherence metrics port and access the metrics from your local machine use the following commands: ```bash $ kubectl port-forward 9612:9612 $ curl -X GET https://localhost:9612/metrics --cacert --cert `` You can add --insecure if you use self-signed certificate. Also, look for the following message in the log file of the Coherence pod:</br> Started: HttpAcceptor{Name=Proxy:MetricsHttpProxy:HttpAcceptor, State=(SERVICE_STARTED), HttpServer=NettyHttpServer{Protocol=HTTPS, AuthMethod=cert}` To configure Prometheus SSL (TLS) connections with the Coherence metrics SSL endpoints, see https://github.com/helm/charts/blob/master/stable/prometheus-operator/README.md for more information about how to specify Kubernetes secrets that contain the certificates required for two-way SSL in Prometheus. To configure Prometheus to use SSL (TLS) connections, see https://prometheus.io/docs/prometheus/latest/configuration/configuration/#tls_config. After configuring Prometheus to use SSL, verify that the Prometheus is scraping Coherence metrics over HTTPS by forwarding the Prometheus service port to your local machine and access the following URL: $ kubectl port-forward <Prometheus pod> 9090:9090 http://localhost:9090/graph ``` You should see many vendor:coherence_* metrics. To enable SSL for both management over REST and metrics publishing for Prometheus, install the Coherence chart with both YAML files: ```bash helm --debug install coherence/coherence \\ --name coherence \\ --namespace myNamespace \\ --set imagePullSecrets=my-imagePull-secret \\ -f helm-values-ssl-management.yaml,helm-values-ssl-metrics.yaml","title":"Configuring SSL for Metrics Publishing for Prometheus"},{"location":"samples/","text":"Samples The samples provide demonstrations of how to accomplish common tasks. These samples are provided for educational and demonstration purposes only; they are not intended to be used in production deployments or to be depended upon to create production environments. Read through the samples, understand how they work, and then customize them to your requirements. See the Oracle Coherence Demonstration for an example of an application using Coherence and running via the Coherence Operator. Table of Contents Get Started Check Runtime Prerequisites Check JDK 8 and Maven Installation Install Coherence into Local Maven Repository Create a Secret Use Docker Hub for Images Provide a Secret to Kubernetes for a Repository on Docker Hub Create the Sample Namespace Clone the GitHub Repository Install the Coherence Operator Install the Coherence Operator Without Prometheus and Log Capture Enable Prometheus Enable Log Capture Enable Prometheus and Log Capture Check Operator Status List Of Samples Troubleshooting Tips Coherence Cluster pods never reach ready \"1/1\" Error: ImagePullBackOff after installing Operator or coherence Error: configmaps \"coherence-internal-config\" not found Unable to delete pods when using log capture Receive Error 'no matches for kind \"Prometheus\"' Accessing UI endpoints Access Grafana Access Kibana Access Prometheus Run Samples Integration Tests Get Started To setup Coherence Operator, follow these steps: Check Runtime Prerequisites Check JDK 8 and Maven Installation Install Coherence into Local Maven Repository Create the Sample Namespace Create a Secret Clone the GitHub Repository Install the Coherence Operator If you have already run samples before, you can go to the List of Samples . Check Runtime Prerequisites Refer to the following sections in the Quick Start Guide for software versions and runtime environment configuration: Runtime Environment Prerequisites Software Requirements - Helm and Kubernetes versions Runtime Environment Requirements - Helm and Kubernetes configuration Environment Configuration Add the Helm repository for Coherence Obtain Images from Oracle Container Registry Note: For all the helm install commands, you can leave the --version option off and the latest version of the chart is retrieved. If you wanted to use a specific version, such as 0.9.8, add --version 0.9.8 to all installs for the coherence-operator and coherence charts. Check JDK 8 and Maven Installation Ensure that you have the following installed: JDK 8+ Maven 3.5.4+ Note: You can use a later version of Java, for example, JDK11, as the maven.compiler.source and target are set to JDK 8 in the sample pom.xml files. Install Coherence into Local Maven Repository If you are running samples that have a Maven project, then follow these steps: Download and install Oracle Coherence 12.2.1.3 from Oracle Technology Network . Ensure that the COHERENCE_HOME environment variable is set to point to the coherence directory under your install location containing the bin, lib, and doc directories. This is required only for the Maven install-file commands. Install Coherence into your local Maven repository: bash $ mvn install:install-file -Dfile=$COHERENCE_HOME/lib/coherence.jar \\ -DpomFile=$COHERENCE_HOME/plugins/maven/com/oracle/coherence/coherence/12.2.1/coherence.12.2.1.pom If you are running Coherence 12.2.1.4, you need to install coherence-metrics . bash $ mvn install:install-file -Dfile=$COHERENCE_HOME/lib/coherence-metrics.jar -DpomFile=$COHERENCE_HOME/plugins/maven/com/oracle/coherence/coherence-metrics/12.2.1/coherence-metrics.12.2.1.pom Create the Sample Namespace You need to create the namespace for the first time to run any of the samples. Create your target namespace: ```bash $ kubectl create namespace sample-coherence-ns namespace/sample-coherence-ns created `` In the samples, a Kubernetes namespace called sample-coherence-ns` is used. If you want to change this namespace, ensure that you change any references to this namespace to match your selected namespace. Create a Secret If all of your images can be pulled from public repositories, this step is not required. Otherwise, you need to enable your Kubernetes cluster to pull images from private repositories. You must create a secret to convey the docker credentials to Kubernetes. In the samples, the secret named sample-coherence-secret is used in the namespace sample-coherence-ns . See https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ for more information. bash $ kubectl get secret sample-coherence-secret -n sample-coherence-ns console NAME TYPE DATA AGE sample-coherence-secret kubernetes.io/dockerconfigjson 1 18s Use Docker Hub for Images You can pull Docker images from one repository, modify them, re-tag, and push the images to the repositories that you own. For example, repositories that you can create on Docker Hub . The repositories are public by default, but can also be private. Provide a Secret to Kubernetes for a Repository on Docker Hub In this example, the Coherence 12.2.1.3 Docker image is pulled from the Docker Store, re-tagged, and pushed in to a freshly created docker repository within your Docker Hub account. Then, create a secret to allow Kubernetes to pull that image. This approach can also be used when pushing sidecar images in other samples. Sign in to Docker Hub . Click Create a Repository on the Docker Welcome page. Enter Coherence as the Name of the repository and in the Description type Re-tags of Official Coherence Image . Select Public or Private for the repository and click Create . Note the docker tag displayed. bash docker push <dockerid>/coherence:tagname <dockerid>/coherence:tagname is the docker tag for the repository. Follow the steps in the section Obtain Images from Oracle Container Registry to get the Coherence 12.2.1.3.x Docker image. Re-tag the Coherence 12.2.1.3.x Docker image with your repository and docker tag. bash docker tag store/oracle/coherence:12.2.1.3 <dockerid>/coherence:12.2.1.3 Log in to your Docker Hub account: bash docker login Enter your Docker ID and password. Push the re-tagged image to your Docker repository: bash docker push <dockerid>/coherence:12.2.1.3 Note the value of the The push refers to repository statement. The first part of that is necessary to create the secret for Kubernetes. In this case, it should be something like docker.io/mydockerid/coherence . Create the secret within your namespace: bash kubectl create secret docker-registry sample-coherence-secret --namespace sample-coherence-ns --docker-server=hub.docker.com --docker-username=docker.io/<dockerid> --docker-password=\"your docker password\" --docker-email=\"the email address of your docker account\" When invoking helm , you can specify one or more secrets using the --set imagePullSecrets option. bash --set \"imagePullSecrets{sample-coherence-secret}\" Clone the GitHub Repository The samples exist in the gh-pages branch of the Coherence Operator GitHub repository - https://github.com/oracle/coherence-operator. Clone the repository and switch to the gh-pages branch: $ git clone https://github.com/oracle/coherence-operator $ cd coherence-operator $ git checkout gh-pages $ cd docs/samples In the samples root directory, check the pom.xml and verify that the value of the coherence.version property matches the version of Coherence that you are actually using. For example, if you have Coherence 12.2.1.3.0, then the value of coherence.version must be 12.2.1-3-0 . If this value needs ajustment, use the -Dcoherence.version= argument for all invocations of mvn . Use the following command to ensure that all the projects with source code build correcly: $ mvn clean install Note : Any compilation errors indicates that the Coherence JARs are not properly installed or you have not set the JDK correctly in your system. Install the Coherence Operator Install the operator first to try out the samples. When you install the operator, you can optionally enable the following: Prometheus integration: Captures metrics and displays in Grafana. (Available only for Coherence 12.2.1.4.0 or later) Log capture: Uses Fluentd to send logs to Elasticsearch which can be then viewed in Kibana. When you enable both Prometheus and log capture, you require extra system resources. Note: When you are running the operator locally, for example, Docker on Mac, you should allocate sufficient memory to your Docker for Mac process. The minimum recommended memory to run is 8G. Install the Coherence Operator Without Prometheus and Log Capture The following command installs the operator without Prometheus or log capture enabled: $ helm install \\ --namespace sample-coherence-ns \\ --set imagePullSecrets=sample-coherence-secret \\ --name coherence-operator \\ --set \"targetNamespaces={sample-coherence-ns}\" \\ coherence/coherence-operator Enable Prometheus Note: Use of Prometheus and Grafana is available only when using the operator with Coherence 12.2.1.4 or later version. To enable Prometheus, add the following options to the operator installation command: --set prometheusoperator.enabled=true \\ --set prometheusoperator.prometheusOperator.createCustomResource=false The complete helm install example to enable Prometheus is as follows: $ helm install \\ --namespace sample-coherence-ns \\ --set imagePullSecrets=sample-coherence-secret \\ --name coherence-operator \\ --set prometheusoperator.enabled=true \\ --set prometheusoperator.prometheusOperator.createCustomResource=false \\ --set \"targetNamespaces={sample-coherence-ns}\" \\ coherence/coherence-operator Note: When you install prometheusOperator for the first time, you must set createCustomResource=true . For subsequent installation of the operator, it must be set to false . Enable Log Capture To enable log capture, which includes Fluentd, Elasticsearch and Kibana, add the following options to the helm install command: bash --set logCaptureEnabled=true The complete helm install example to enable log capture is as follows: bash $ helm install \\ --namespace sample-coherence-ns \\ --set imagePullSecrets=sample-coherence-secret \\ --name coherence-operator \\ --set logCaptureEnabled=true \\ --set \"targetNamespaces={sample-coherence-ns}\" \\ coherence/coherence-operator Enable Prometheus and Log Capture You can enable both Prometheus and log capture by setting both of the options to true . Check Operator Status Use kubectl get pods -n sample-coherence-ns to ensure that all the pods are in running status. When you enable Prometheus, the following output is displayed: NAME READY STATUS RESTARTS AGE coherence-operator-66f9bb7b75-nxwdc 1/1 Running 0 13m coherence-operator-grafana-898fc8bbd-nls45 3/3 Running 0 13m coherence-operator-kube-state-metrics-5d5f6855bd-klzj5 1/1 Running 0 13m coherence-operator-prometh-operator-58bd58ddfd-dhd9q 1/1 Running 0 13m coherence-operator-prometheus-node-exporter-5hxwh 1/1 Running 0 13m prometheus-coherence-operator-prometh-prometheus-0 3/3 Running 1 12m Depending upon the number of CPU cores, you can see multiple node-exporter processes. When you enable log capture, the following output is displayed: NAME READY STATUS RESTARTS AGE coherence-operator-64b4f8f95d-fmz2x 2/2 Running 0 2m elasticsearch-5b5474865c-tlr44 1/1 Running 0 2m kibana-f6955c4b9-n8krf 1/1 Running 0 2m List Of Samples Samples legend: * \u2714 - Available for Coherence 12.2.1.3.x and above \u2726 - Available for Coherence 12.2.1.4.x and above Coherence Operator Logging Enable Log capture to View Logs in Kiabana \u2714 Configure Custom Logger and View in Kibana \u2714 Push Logs to Your Elasticsearch Instance \u2714 Metrics (12.2.1.4.X only) Deploy the Operator with Prometheus Enabled and View in Grafana \u2726 Enable SSL for Metrics \u2726 Scrape Metrics from Your Prometheus Instance \u2726 Scaling a Coherence Deployment \u2714 Change Image Version for Coherence or aApplication Container Using Rolling Upgrade \u2714 Coherence Deployments Add Application JARs/Config to a Coherence Deployment \u2714 Accessing Coherence via Coherence*Extend Access Coherence via the Default Proxy Port \u2714 Access Coherence via the Separate Proxy Tier \u2714 Enabling SSL for Proxy Servers \u2714 Using multiple Coherence*Extend Proxies \u2714 Accessing Coherence via storage-disabled Clients Storage-disabled Client in Cluster via Interceptor \u2714 Storage-disabled Client in Cluster as Separate User image \u2714 Federation (12.2.1.4.X only) Within a Single Kubernetes Cluster \u2726 Across Separate Kubernets Clusters \u2726 Persistence Use Default Persistent Volume Claim \u2714 Use a Specific Persistent Volume \u2714 Elastic Data Deploy Using Default FlashJournal Locations \u2714 Deploy Using External Volume Mapped to the Host \u2714 Installing Multiple Coherence Clusters with One Operator Management Using Management over REST (12.2.1.4.X only) Access Management over REST \u2726 Access Management over REST Using JVisualVM plugin \u2726 Enable SSL with Management over REST \u2726 Modify Writable MBeans \u2726 Access JMX in the Coherence Cluster via JConsole and JVisualVM \u2714 Access Coherence Console and CohQL on a Cluster Node \u2714 Diagnostic Tools Produce and Extract a Heap Dump \u2714 Produce and Extract a Java Flight Recorder (JFR) file \u2726 Manage and Use the Reporter \u2726 Provide Arguments to the JVM that Runs Coherence \u2714 Troubleshooting Tips Coherence Cluster pods never reach ready \"1/1\" Use the following kubectl command to see the message from the pod: $ kubectl describe pod pod-name -n sample-coherence-ns Error: ImagePullBackOff after installing Operator or coherence When you see Error: ImagePullBackOff for one of the pod status, examine the pod using kubectl describe pod -n sample-coherence-ns pod-name to determine the image causing the issue. Ensure that you have set the following to a valid secret: --set imagePullSecrets=sample-coherence-secret Error: configmaps \"coherence-internal-config\" not found If your pods don't start and the kubectl describe command shows the error, ensure that you have included the --targetNamespaces option when installing the coherence-operator . Error: configmaps \"coherence-internal-config\" not found Unable to delete pods when using log capture If you are using Kubernetes version older than 1.13.0, you cannot delete pods when you have enabled log capture feature. This is a known issue (fluentd) and you need to add the options --force --grace-period=0 to force delete the pods. Refer to https://github.com/kubernetes/kubernetes/issues/45688 . Receive Error 'no matches for kind \"Prometheus\"' If you have enabled metrics, you can get the following error when you try to install the operator: Error: validation failed: [unable to recognize \"\": no matches for kind \"Prometheus\" in version \"monitoring.coreos.com/v1\", unable to recognize \"\": no matches for kind \"PrometheusRule\" in version \"monitoring.coreos.com/v1\", unable to recognize \"\": no ... Ensure that you have set the following option in the operator installation: --set prometheusoperator.prometheusOperator.createCustomResource=true This is required only when you install Prometheus for the first time in the namespace. Accessing UI endpoints Access Grafana Note: Use of Prometheus and Grafana is available only when using the operator with Oracle Coherence 12.2.1.4 version. If you have enabled Prometheus, then you can use the port-forward-grafana.sh script in the common directory to view metrics. Start the port-forward bash $ ./port-forward-grafana.sh sample-coherence-ns console Forwarding from 127.0.0.1:3000 -> 3000 Forwarding from [::1]:3000 -> 3000 Access Grafana using the following URL: http://127.0.0.1:3000/d/coh-main/coherence-dashboard-main Username: admin Password: prom-operator Access Kibana If you have enabled log capture, then you can use the port-forward-kibana.sh script in the common directory to view metrics. Start the port-forward bash $ ./port-forward-kibana.sh sample-coherence-ns console Forwarding from 127.0.0.1:5601 -> 5601 Forwarding from [::1]:5601 -> 5601 2. Access Kibana using the following URL: http://127.0.0.1:5601/ Access Prometheus Note: Use of Prometheus and Grafana is available only when using the operator with Oracle Coherence 12.2.1.4 version. If you have enabled Prometheus, then you can use the port-forward-prometheus.sh script in the common directory to view metrics directly. Start the port-forward bash $ ./port-forward-prometheus.sh sample-coherence-ns console Forwarding from 127.0.0.1:9090 -> 9090 Forwarding from [::1]:9090 -> 9090 2. Access Prometheus using the following URL: http://127.0.0.1:9090/ Run Samples Integration Tests Refer to Developer Guide for more information about how to run the samples integration tests.","title":"Samples"},{"location":"samples/#samples","text":"The samples provide demonstrations of how to accomplish common tasks. These samples are provided for educational and demonstration purposes only; they are not intended to be used in production deployments or to be depended upon to create production environments. Read through the samples, understand how they work, and then customize them to your requirements. See the Oracle Coherence Demonstration for an example of an application using Coherence and running via the Coherence Operator.","title":"Samples"},{"location":"samples/#table-of-contents","text":"Get Started Check Runtime Prerequisites Check JDK 8 and Maven Installation Install Coherence into Local Maven Repository Create a Secret Use Docker Hub for Images Provide a Secret to Kubernetes for a Repository on Docker Hub Create the Sample Namespace Clone the GitHub Repository Install the Coherence Operator Install the Coherence Operator Without Prometheus and Log Capture Enable Prometheus Enable Log Capture Enable Prometheus and Log Capture Check Operator Status List Of Samples Troubleshooting Tips Coherence Cluster pods never reach ready \"1/1\" Error: ImagePullBackOff after installing Operator or coherence Error: configmaps \"coherence-internal-config\" not found Unable to delete pods when using log capture Receive Error 'no matches for kind \"Prometheus\"' Accessing UI endpoints Access Grafana Access Kibana Access Prometheus Run Samples Integration Tests","title":"Table of Contents"},{"location":"samples/#get-started","text":"To setup Coherence Operator, follow these steps: Check Runtime Prerequisites Check JDK 8 and Maven Installation Install Coherence into Local Maven Repository Create the Sample Namespace Create a Secret Clone the GitHub Repository Install the Coherence Operator If you have already run samples before, you can go to the List of Samples .","title":"Get Started"},{"location":"samples/#check-runtime-prerequisites","text":"Refer to the following sections in the Quick Start Guide for software versions and runtime environment configuration: Runtime Environment Prerequisites Software Requirements - Helm and Kubernetes versions Runtime Environment Requirements - Helm and Kubernetes configuration Environment Configuration Add the Helm repository for Coherence Obtain Images from Oracle Container Registry Note: For all the helm install commands, you can leave the --version option off and the latest version of the chart is retrieved. If you wanted to use a specific version, such as 0.9.8, add --version 0.9.8 to all installs for the coherence-operator and coherence charts.","title":"Check Runtime Prerequisites"},{"location":"samples/#check-jdk-8-and-maven-installation","text":"Ensure that you have the following installed: JDK 8+ Maven 3.5.4+ Note: You can use a later version of Java, for example, JDK11, as the maven.compiler.source and target are set to JDK 8 in the sample pom.xml files.","title":"Check JDK 8 and Maven Installation"},{"location":"samples/#install-coherence-into-local-maven-repository","text":"If you are running samples that have a Maven project, then follow these steps: Download and install Oracle Coherence 12.2.1.3 from Oracle Technology Network . Ensure that the COHERENCE_HOME environment variable is set to point to the coherence directory under your install location containing the bin, lib, and doc directories. This is required only for the Maven install-file commands. Install Coherence into your local Maven repository: bash $ mvn install:install-file -Dfile=$COHERENCE_HOME/lib/coherence.jar \\ -DpomFile=$COHERENCE_HOME/plugins/maven/com/oracle/coherence/coherence/12.2.1/coherence.12.2.1.pom If you are running Coherence 12.2.1.4, you need to install coherence-metrics . bash $ mvn install:install-file -Dfile=$COHERENCE_HOME/lib/coherence-metrics.jar -DpomFile=$COHERENCE_HOME/plugins/maven/com/oracle/coherence/coherence-metrics/12.2.1/coherence-metrics.12.2.1.pom","title":"Install Coherence into Local Maven Repository"},{"location":"samples/#create-the-sample-namespace","text":"You need to create the namespace for the first time to run any of the samples. Create your target namespace: ```bash $ kubectl create namespace sample-coherence-ns namespace/sample-coherence-ns created `` In the samples, a Kubernetes namespace called sample-coherence-ns` is used. If you want to change this namespace, ensure that you change any references to this namespace to match your selected namespace.","title":"Create the Sample Namespace"},{"location":"samples/#create-a-secret","text":"If all of your images can be pulled from public repositories, this step is not required. Otherwise, you need to enable your Kubernetes cluster to pull images from private repositories. You must create a secret to convey the docker credentials to Kubernetes. In the samples, the secret named sample-coherence-secret is used in the namespace sample-coherence-ns . See https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ for more information. bash $ kubectl get secret sample-coherence-secret -n sample-coherence-ns console NAME TYPE DATA AGE sample-coherence-secret kubernetes.io/dockerconfigjson 1 18s","title":"Create a Secret"},{"location":"samples/#use-docker-hub-for-images","text":"You can pull Docker images from one repository, modify them, re-tag, and push the images to the repositories that you own. For example, repositories that you can create on Docker Hub . The repositories are public by default, but can also be private.","title":"Use Docker Hub for Images"},{"location":"samples/#provide-a-secret-to-kubernetes-for-a-repository-on-docker-hub","text":"In this example, the Coherence 12.2.1.3 Docker image is pulled from the Docker Store, re-tagged, and pushed in to a freshly created docker repository within your Docker Hub account. Then, create a secret to allow Kubernetes to pull that image. This approach can also be used when pushing sidecar images in other samples. Sign in to Docker Hub . Click Create a Repository on the Docker Welcome page. Enter Coherence as the Name of the repository and in the Description type Re-tags of Official Coherence Image . Select Public or Private for the repository and click Create . Note the docker tag displayed. bash docker push <dockerid>/coherence:tagname <dockerid>/coherence:tagname is the docker tag for the repository. Follow the steps in the section Obtain Images from Oracle Container Registry to get the Coherence 12.2.1.3.x Docker image. Re-tag the Coherence 12.2.1.3.x Docker image with your repository and docker tag. bash docker tag store/oracle/coherence:12.2.1.3 <dockerid>/coherence:12.2.1.3 Log in to your Docker Hub account: bash docker login Enter your Docker ID and password. Push the re-tagged image to your Docker repository: bash docker push <dockerid>/coherence:12.2.1.3 Note the value of the The push refers to repository statement. The first part of that is necessary to create the secret for Kubernetes. In this case, it should be something like docker.io/mydockerid/coherence . Create the secret within your namespace: bash kubectl create secret docker-registry sample-coherence-secret --namespace sample-coherence-ns --docker-server=hub.docker.com --docker-username=docker.io/<dockerid> --docker-password=\"your docker password\" --docker-email=\"the email address of your docker account\" When invoking helm , you can specify one or more secrets using the --set imagePullSecrets option. bash --set \"imagePullSecrets{sample-coherence-secret}\"","title":"Provide a Secret to Kubernetes for a Repository on Docker Hub"},{"location":"samples/#clone-the-github-repository","text":"The samples exist in the gh-pages branch of the Coherence Operator GitHub repository - https://github.com/oracle/coherence-operator. Clone the repository and switch to the gh-pages branch: $ git clone https://github.com/oracle/coherence-operator $ cd coherence-operator $ git checkout gh-pages $ cd docs/samples In the samples root directory, check the pom.xml and verify that the value of the coherence.version property matches the version of Coherence that you are actually using. For example, if you have Coherence 12.2.1.3.0, then the value of coherence.version must be 12.2.1-3-0 . If this value needs ajustment, use the -Dcoherence.version= argument for all invocations of mvn . Use the following command to ensure that all the projects with source code build correcly: $ mvn clean install Note : Any compilation errors indicates that the Coherence JARs are not properly installed or you have not set the JDK correctly in your system.","title":"Clone the GitHub Repository"},{"location":"samples/#install-the-coherence-operator","text":"Install the operator first to try out the samples. When you install the operator, you can optionally enable the following: Prometheus integration: Captures metrics and displays in Grafana. (Available only for Coherence 12.2.1.4.0 or later) Log capture: Uses Fluentd to send logs to Elasticsearch which can be then viewed in Kibana. When you enable both Prometheus and log capture, you require extra system resources. Note: When you are running the operator locally, for example, Docker on Mac, you should allocate sufficient memory to your Docker for Mac process. The minimum recommended memory to run is 8G.","title":"Install the Coherence Operator"},{"location":"samples/#install-the-coherence-operator-without-prometheus-and-log-capture","text":"The following command installs the operator without Prometheus or log capture enabled: $ helm install \\ --namespace sample-coherence-ns \\ --set imagePullSecrets=sample-coherence-secret \\ --name coherence-operator \\ --set \"targetNamespaces={sample-coherence-ns}\" \\ coherence/coherence-operator","title":"Install the Coherence Operator Without Prometheus and Log Capture"},{"location":"samples/#enable-prometheus","text":"Note: Use of Prometheus and Grafana is available only when using the operator with Coherence 12.2.1.4 or later version. To enable Prometheus, add the following options to the operator installation command: --set prometheusoperator.enabled=true \\ --set prometheusoperator.prometheusOperator.createCustomResource=false The complete helm install example to enable Prometheus is as follows: $ helm install \\ --namespace sample-coherence-ns \\ --set imagePullSecrets=sample-coherence-secret \\ --name coherence-operator \\ --set prometheusoperator.enabled=true \\ --set prometheusoperator.prometheusOperator.createCustomResource=false \\ --set \"targetNamespaces={sample-coherence-ns}\" \\ coherence/coherence-operator Note: When you install prometheusOperator for the first time, you must set createCustomResource=true . For subsequent installation of the operator, it must be set to false .","title":"Enable Prometheus"},{"location":"samples/#enable-log-capture","text":"To enable log capture, which includes Fluentd, Elasticsearch and Kibana, add the following options to the helm install command: bash --set logCaptureEnabled=true The complete helm install example to enable log capture is as follows: bash $ helm install \\ --namespace sample-coherence-ns \\ --set imagePullSecrets=sample-coherence-secret \\ --name coherence-operator \\ --set logCaptureEnabled=true \\ --set \"targetNamespaces={sample-coherence-ns}\" \\ coherence/coherence-operator","title":"Enable Log Capture"},{"location":"samples/#enable-prometheus-and-log-capture","text":"You can enable both Prometheus and log capture by setting both of the options to true .","title":"Enable Prometheus and Log Capture"},{"location":"samples/#check-operator-status","text":"Use kubectl get pods -n sample-coherence-ns to ensure that all the pods are in running status. When you enable Prometheus, the following output is displayed: NAME READY STATUS RESTARTS AGE coherence-operator-66f9bb7b75-nxwdc 1/1 Running 0 13m coherence-operator-grafana-898fc8bbd-nls45 3/3 Running 0 13m coherence-operator-kube-state-metrics-5d5f6855bd-klzj5 1/1 Running 0 13m coherence-operator-prometh-operator-58bd58ddfd-dhd9q 1/1 Running 0 13m coherence-operator-prometheus-node-exporter-5hxwh 1/1 Running 0 13m prometheus-coherence-operator-prometh-prometheus-0 3/3 Running 1 12m Depending upon the number of CPU cores, you can see multiple node-exporter processes. When you enable log capture, the following output is displayed: NAME READY STATUS RESTARTS AGE coherence-operator-64b4f8f95d-fmz2x 2/2 Running 0 2m elasticsearch-5b5474865c-tlr44 1/1 Running 0 2m kibana-f6955c4b9-n8krf 1/1 Running 0 2m","title":"Check Operator Status"},{"location":"samples/#list-of-samples","text":"Samples legend: * \u2714 - Available for Coherence 12.2.1.3.x and above \u2726 - Available for Coherence 12.2.1.4.x and above Coherence Operator Logging Enable Log capture to View Logs in Kiabana \u2714 Configure Custom Logger and View in Kibana \u2714 Push Logs to Your Elasticsearch Instance \u2714 Metrics (12.2.1.4.X only) Deploy the Operator with Prometheus Enabled and View in Grafana \u2726 Enable SSL for Metrics \u2726 Scrape Metrics from Your Prometheus Instance \u2726 Scaling a Coherence Deployment \u2714 Change Image Version for Coherence or aApplication Container Using Rolling Upgrade \u2714 Coherence Deployments Add Application JARs/Config to a Coherence Deployment \u2714 Accessing Coherence via Coherence*Extend Access Coherence via the Default Proxy Port \u2714 Access Coherence via the Separate Proxy Tier \u2714 Enabling SSL for Proxy Servers \u2714 Using multiple Coherence*Extend Proxies \u2714 Accessing Coherence via storage-disabled Clients Storage-disabled Client in Cluster via Interceptor \u2714 Storage-disabled Client in Cluster as Separate User image \u2714 Federation (12.2.1.4.X only) Within a Single Kubernetes Cluster \u2726 Across Separate Kubernets Clusters \u2726 Persistence Use Default Persistent Volume Claim \u2714 Use a Specific Persistent Volume \u2714 Elastic Data Deploy Using Default FlashJournal Locations \u2714 Deploy Using External Volume Mapped to the Host \u2714 Installing Multiple Coherence Clusters with One Operator Management Using Management over REST (12.2.1.4.X only) Access Management over REST \u2726 Access Management over REST Using JVisualVM plugin \u2726 Enable SSL with Management over REST \u2726 Modify Writable MBeans \u2726 Access JMX in the Coherence Cluster via JConsole and JVisualVM \u2714 Access Coherence Console and CohQL on a Cluster Node \u2714 Diagnostic Tools Produce and Extract a Heap Dump \u2714 Produce and Extract a Java Flight Recorder (JFR) file \u2726 Manage and Use the Reporter \u2726 Provide Arguments to the JVM that Runs Coherence \u2714","title":"List Of Samples"},{"location":"samples/#troubleshooting-tips","text":"","title":"Troubleshooting Tips"},{"location":"samples/#coherence-cluster-pods-never-reach-ready-11","text":"Use the following kubectl command to see the message from the pod: $ kubectl describe pod pod-name -n sample-coherence-ns","title":"Coherence Cluster pods never reach ready \"1/1\""},{"location":"samples/#error-imagepullbackoff-after-installing-operator-or-coherence","text":"When you see Error: ImagePullBackOff for one of the pod status, examine the pod using kubectl describe pod -n sample-coherence-ns pod-name to determine the image causing the issue. Ensure that you have set the following to a valid secret: --set imagePullSecrets=sample-coherence-secret","title":"Error: ImagePullBackOff after installing Operator or coherence"},{"location":"samples/#error-configmaps-coherence-internal-config-not-found","text":"If your pods don't start and the kubectl describe command shows the error, ensure that you have included the --targetNamespaces option when installing the coherence-operator . Error: configmaps \"coherence-internal-config\" not found","title":"Error: configmaps \"coherence-internal-config\" not found"},{"location":"samples/#unable-to-delete-pods-when-using-log-capture","text":"If you are using Kubernetes version older than 1.13.0, you cannot delete pods when you have enabled log capture feature. This is a known issue (fluentd) and you need to add the options --force --grace-period=0 to force delete the pods. Refer to https://github.com/kubernetes/kubernetes/issues/45688 .","title":"Unable to delete pods when using log capture"},{"location":"samples/#receive-error-no-matches-for-kind-prometheus","text":"If you have enabled metrics, you can get the following error when you try to install the operator: Error: validation failed: [unable to recognize \"\": no matches for kind \"Prometheus\" in version \"monitoring.coreos.com/v1\", unable to recognize \"\": no matches for kind \"PrometheusRule\" in version \"monitoring.coreos.com/v1\", unable to recognize \"\": no ... Ensure that you have set the following option in the operator installation: --set prometheusoperator.prometheusOperator.createCustomResource=true This is required only when you install Prometheus for the first time in the namespace.","title":"Receive Error 'no matches for kind \"Prometheus\"'"},{"location":"samples/#accessing-ui-endpoints","text":"","title":"Accessing UI endpoints"},{"location":"samples/#access-grafana","text":"Note: Use of Prometheus and Grafana is available only when using the operator with Oracle Coherence 12.2.1.4 version. If you have enabled Prometheus, then you can use the port-forward-grafana.sh script in the common directory to view metrics. Start the port-forward bash $ ./port-forward-grafana.sh sample-coherence-ns console Forwarding from 127.0.0.1:3000 -> 3000 Forwarding from [::1]:3000 -> 3000 Access Grafana using the following URL: http://127.0.0.1:3000/d/coh-main/coherence-dashboard-main Username: admin Password: prom-operator","title":"Access Grafana"},{"location":"samples/#access-kibana","text":"If you have enabled log capture, then you can use the port-forward-kibana.sh script in the common directory to view metrics. Start the port-forward bash $ ./port-forward-kibana.sh sample-coherence-ns console Forwarding from 127.0.0.1:5601 -> 5601 Forwarding from [::1]:5601 -> 5601 2. Access Kibana using the following URL: http://127.0.0.1:5601/","title":"Access Kibana"},{"location":"samples/#access-prometheus","text":"Note: Use of Prometheus and Grafana is available only when using the operator with Oracle Coherence 12.2.1.4 version. If you have enabled Prometheus, then you can use the port-forward-prometheus.sh script in the common directory to view metrics directly. Start the port-forward bash $ ./port-forward-prometheus.sh sample-coherence-ns console Forwarding from 127.0.0.1:9090 -> 9090 Forwarding from [::1]:9090 -> 9090 2. Access Prometheus using the following URL: http://127.0.0.1:9090/","title":"Access Prometheus"},{"location":"samples/#run-samples-integration-tests","text":"Refer to Developer Guide for more information about how to run the samples integration tests.","title":"Run Samples Integration Tests"},{"location":"samples/developer/","text":"Running the samples integration tests Prerequisites Please ensure that you have met all the prerequisites as described in the Samples Readme . Run the tests Create the test namespace ```bash export CI_BUILD_ID=test export NS=test-sample-${CI_BUILD_ID} kubectl create namespace $NS ``` Create the secrets ```bash kubectl create secret docker-registry coherence-k8s-operator-development-secret \\ --namespace $NS \\ --docker-server=your-docker-server \\ --docker-username=your-docker-username \\ --docker-password='your-docker-password' kubectl create secret docker-registry sample-coherence-secret \\ --namespace $NS \\ --docker-server=your-docker-server \\ --docker-username=your-docker-username \\ --docker-password='your-docker-password' ``` Note: You must have push permissions for this repository if you not running Kubernetes locally. Run the tests bash cd docs/samples mvn -Dcoherence.image.prefix=store/oracle/ -Dk8s.chart.test.versions=0.9.4 \\ -Dk8s.namespace=$NS -Dk8s.create.namespace=false -P docker,helm-test clean verify Note: If you are running against a remote Kubernetes cluster, you must also specify the profile dockerPush Note: You can also specify multiple versions of the chart to test: e.g. -Dk8s.chart.test.versions=0.9.4,1.0.0 . Note: You can set the operator.helm.chart.package and coherence.helm.chart.package properties to run against operator charts that have been built on disk.","title":"Running the samples integration tests"},{"location":"samples/developer/#running-the-samples-integration-tests","text":"","title":"Running the samples integration tests"},{"location":"samples/developer/#prerequisites","text":"Please ensure that you have met all the prerequisites as described in the Samples Readme .","title":"Prerequisites"},{"location":"samples/developer/#run-the-tests","text":"Create the test namespace ```bash export CI_BUILD_ID=test export NS=test-sample-${CI_BUILD_ID} kubectl create namespace $NS ``` Create the secrets ```bash kubectl create secret docker-registry coherence-k8s-operator-development-secret \\ --namespace $NS \\ --docker-server=your-docker-server \\ --docker-username=your-docker-username \\ --docker-password='your-docker-password' kubectl create secret docker-registry sample-coherence-secret \\ --namespace $NS \\ --docker-server=your-docker-server \\ --docker-username=your-docker-username \\ --docker-password='your-docker-password' ``` Note: You must have push permissions for this repository if you not running Kubernetes locally. Run the tests bash cd docs/samples mvn -Dcoherence.image.prefix=store/oracle/ -Dk8s.chart.test.versions=0.9.4 \\ -Dk8s.namespace=$NS -Dk8s.create.namespace=false -P docker,helm-test clean verify Note: If you are running against a remote Kubernetes cluster, you must also specify the profile dockerPush Note: You can also specify multiple versions of the chart to test: e.g. -Dk8s.chart.test.versions=0.9.4,1.0.0 . Note: You can set the operator.helm.chart.package and coherence.helm.chart.package properties to run against operator charts that have been built on disk.","title":"Run the tests"},{"location":"samples/coherence-deployments/","text":"Coherence Deployments Table of Contents Add Application Jars/Config to a Coherence Deployment Accessing Coherence via Coherence*Extend Access Coherence via the Default Proxy Port Access Coherence via a Separate Proxy Tier Enable SSL in Coherence 12.2.1.3.X Using Multiple Coherence*Extend Proxies Accessing Coherence via Storage-Disabled Clients Storage-Disabled Client in Cluster via Interceptor Storage-Disabled Client in Cluster as Separate User Image Federation Within a Single Kubernetes Cluster Across Separate Kubernetes Clusters Persistence Use the Default Persistent Volume Claim Use Specific Persistent Volumes Elastic Data Deploy Elastic Data Using Default FlashJournal Locations Deploy Elastic Data Using External Volume Mapped to the Host Installing Multiple Coherence Clusters with One Operator Return to samples","title":"Coherence Deployments"},{"location":"samples/coherence-deployments/#coherence-deployments","text":"","title":"Coherence Deployments"},{"location":"samples/coherence-deployments/#table-of-contents","text":"Add Application Jars/Config to a Coherence Deployment Accessing Coherence via Coherence*Extend Access Coherence via the Default Proxy Port Access Coherence via a Separate Proxy Tier Enable SSL in Coherence 12.2.1.3.X Using Multiple Coherence*Extend Proxies Accessing Coherence via Storage-Disabled Clients Storage-Disabled Client in Cluster via Interceptor Storage-Disabled Client in Cluster as Separate User Image Federation Within a Single Kubernetes Cluster Across Separate Kubernetes Clusters Persistence Use the Default Persistent Volume Claim Use Specific Persistent Volumes Elastic Data Deploy Elastic Data Using Default FlashJournal Locations Deploy Elastic Data Using External Volume Mapped to the Host Installing Multiple Coherence Clusters with One Operator Return to samples","title":"Table of Contents"},{"location":"samples/coherence-deployments/elastic-data/","text":"Elastic Data The Elastic Data feature is used to seamlessly store data across memory and disk-based devices. This feature is especially tuned to take advantage of the fast disk-based devices, such as, Solid State Disks (SSD) and enables near memory speed while storing and reading data from SSDs. The Elastic Data feature uses a technique called journaling to optimize the storage across memory and disk. Table of Contents Deploy Elastic Data using default FlashJournal locations Deploy Elastic Data using external volume mapped to the host Return to Coherence Deployments samples / Return to samples Overview","title":"Elastic Data"},{"location":"samples/coherence-deployments/elastic-data/#elastic-data","text":"The Elastic Data feature is used to seamlessly store data across memory and disk-based devices. This feature is especially tuned to take advantage of the fast disk-based devices, such as, Solid State Disks (SSD) and enables near memory speed while storing and reading data from SSDs. The Elastic Data feature uses a technique called journaling to optimize the storage across memory and disk.","title":"Elastic Data"},{"location":"samples/coherence-deployments/elastic-data/#table-of-contents","text":"Deploy Elastic Data using default FlashJournal locations Deploy Elastic Data using external volume mapped to the host Return to Coherence Deployments samples / Return to samples","title":"Table of Contents"},{"location":"samples/coherence-deployments/elastic-data/#overview","text":"","title":"Overview"},{"location":"samples/coherence-deployments/elastic-data/default/","text":"Deploy Elastic Data Using Default FlashJournal Locations This sample shows how to enable Elastic Data using the default FlashJournal directory, /tmp/ . Refer to the Oracle Elastic Data documentation for more information about Elastic Data. Return to Elastic Data samples / Return to Coherence Deployments samples / Return to samples Sample files src/main/docker/Dockerfile - Dockerfile for creating sidecar image from which the configuration will be read at pod startup src/main/resources/conf/elastic-data-cache-config.xml - Cache configuration for storage-tier Prerequisites Ensure that you have installed Coherence Operator by following the instructions here . Installation Steps Change to the samples/coherence-deployments/elastic-data/default directory. Ensure that you have your maven build environment set for JDK8, and build the project. bash $ mvn clean install -P docker This builds the Docker image with the cache configuration files, with the name in the format elastic-data-sample-default:${version} . For example, bash elastic-data-sample-default:1.0.0-SNAPSHOT Note: If you are running against a remote Kubernetes cluster, then you must push the above image to your repository accessible to that cluster. You must also prefix the image name in the helm command, as shown below. Install the Coherence cluster. bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=elastic-data-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=elastic-data-cache-config.xml \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set userArtifacts.image=elastic-data-sample-default:1.0.0-SNAPSHOT \\ coherence/coherence After the installation is complete, list the pods by running the command: bash $ kubectl get pods -n sample-coherence-ns All the three storage-coherence pods should be running and ready, as shown in the output: console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 4m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 2m Add data to Elastic Data. a. Connect to the Coherence console to create a cache using FlashJournal : bash $ kubectl exec -it --namespace sample-coherence-ns storage-coherence-0 -- bash /scripts/startCoherence.sh console At the Map (?): prompt, type cache flash-01 . This creates a cache in the service, DistributedSchemeFlash which is a FlashJournal scheme. b. Use the following to add 100,000 objects of size 1024 bytes, starting at index 0, and using batches of 100. ```bash bulkput 100000 1024 0 100 Mon Apr 15 07:37:03 GMT 2019: adding 100000 items (starting with #0) each 1024 bytes ... Mon Apr 15 07:37:15 GMT 2019: done putting (11578ms, 8878KB/sec, 8637 items/sec) ``` At the prompt, type size and it should return 100000. Then, type bye to exit the console . Ensure that the Elastic Data FlashJournal files exist. Run the following command against one of the Coherence pods to list the files used by Elastic Data: bash $ kubectl exec -it -n sample-coherence-ns storage-coherence-0 -- bash -c 'ls -l /tmp/' console total 84744 -rw-r--r-- 1 root root 86769664 Apr 15 07:37 coh1781907747204398478.tmp drwxr-xr-x 2 root root 4096 Apr 15 07:49 hsperfdata_root Type exit to leave the exec session. Uninstall the Charts Run the following commands to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Deploy Elastic Data Using Default FlashJournal Locations"},{"location":"samples/coherence-deployments/elastic-data/default/#deploy-elastic-data-using-default-flashjournal-locations","text":"This sample shows how to enable Elastic Data using the default FlashJournal directory, /tmp/ . Refer to the Oracle Elastic Data documentation for more information about Elastic Data. Return to Elastic Data samples / Return to Coherence Deployments samples / Return to samples","title":"Deploy Elastic Data Using Default FlashJournal Locations"},{"location":"samples/coherence-deployments/elastic-data/default/#sample-files","text":"src/main/docker/Dockerfile - Dockerfile for creating sidecar image from which the configuration will be read at pod startup src/main/resources/conf/elastic-data-cache-config.xml - Cache configuration for storage-tier","title":"Sample files"},{"location":"samples/coherence-deployments/elastic-data/default/#prerequisites","text":"Ensure that you have installed Coherence Operator by following the instructions here .","title":"Prerequisites"},{"location":"samples/coherence-deployments/elastic-data/default/#installation-steps","text":"Change to the samples/coherence-deployments/elastic-data/default directory. Ensure that you have your maven build environment set for JDK8, and build the project. bash $ mvn clean install -P docker This builds the Docker image with the cache configuration files, with the name in the format elastic-data-sample-default:${version} . For example, bash elastic-data-sample-default:1.0.0-SNAPSHOT Note: If you are running against a remote Kubernetes cluster, then you must push the above image to your repository accessible to that cluster. You must also prefix the image name in the helm command, as shown below. Install the Coherence cluster. bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=elastic-data-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=elastic-data-cache-config.xml \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set userArtifacts.image=elastic-data-sample-default:1.0.0-SNAPSHOT \\ coherence/coherence After the installation is complete, list the pods by running the command: bash $ kubectl get pods -n sample-coherence-ns All the three storage-coherence pods should be running and ready, as shown in the output: console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 4m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 2m Add data to Elastic Data. a. Connect to the Coherence console to create a cache using FlashJournal : bash $ kubectl exec -it --namespace sample-coherence-ns storage-coherence-0 -- bash /scripts/startCoherence.sh console At the Map (?): prompt, type cache flash-01 . This creates a cache in the service, DistributedSchemeFlash which is a FlashJournal scheme. b. Use the following to add 100,000 objects of size 1024 bytes, starting at index 0, and using batches of 100. ```bash bulkput 100000 1024 0 100 Mon Apr 15 07:37:03 GMT 2019: adding 100000 items (starting with #0) each 1024 bytes ... Mon Apr 15 07:37:15 GMT 2019: done putting (11578ms, 8878KB/sec, 8637 items/sec) ``` At the prompt, type size and it should return 100000. Then, type bye to exit the console . Ensure that the Elastic Data FlashJournal files exist. Run the following command against one of the Coherence pods to list the files used by Elastic Data: bash $ kubectl exec -it -n sample-coherence-ns storage-coherence-0 -- bash -c 'ls -l /tmp/' console total 84744 -rw-r--r-- 1 root root 86769664 Apr 15 07:37 coh1781907747204398478.tmp drwxr-xr-x 2 root root 4096 Apr 15 07:49 hsperfdata_root Type exit to leave the exec session.","title":"Installation Steps"},{"location":"samples/coherence-deployments/elastic-data/default/#uninstall-the-charts","text":"Run the following commands to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/coherence-deployments/elastic-data/external/","text":"Deploy Elastic Data Using External Volume Mapped to the Host This sample shows how to create persistent volumes (PV), and then map the Elastic Data to be stored on these PV. This would allow for a specific size to be used for storing Elastic Data, rather than only relying on the size of the underlying default /tmp/ directory. Refer to the Oracle Elastic Data Documentation for more information about Elastic Data. Return to Elastic Data samples / Return to Coherence Deployments samples / Return to samples Sample files src/main/docker/Dockerfile - Dockerfile for creating sidecar image from which the configuration will be read at pod startup src/main/resources/conf/elastic-data-cache-config.xml - Cache configuration for storage-tier Prerequisites Ensure that you have installed Coherence Operator by following the instructions here . Installation Steps Change to the samples/coherence-deployments/elastic-data/external directory. Ensure that you have your maven build environment set for JDK8, and build the project. bash $ mvn clean install -P docker This builds the docker image with the cache configuration files, with the name in the format, elastic-data-sample-external:${version} . For example, bash elastic-data-sample-external:1.0.0-SNAPSHOT Note: If you are running against a remote Kubernetes cluster, then you must push the above image to your repository accessible to that cluster. You must also prefix the image name in the helm command, as shown below. Install the Coherence cluster. You can use the following three options to specify volumes , volumeMounts & volumeClaimTemplates : --set store.volumes - Defines extra volume mappings that will be added to the Coherence Pod --set store.volumeClaimTemplates - Defines extra PVC mappings that will be added to the Coherence Pod --set store.volumeMounts - Defines extra volume mounts to map to the additional volumes or PVC declared above in store.volumes and store.volumeClaimTemplates For this sample, we are going to use the YAML file, volumes.yaml to specify hostPath volumes. Note: You should set the values appropriately for your Kubernetes environment and needs. Also, set --set store.javaOpts=\"-Dcoherence.flashjournal.dir=/elastic-data\" - to point Elastic data to the mount path. Note: The coherence.flashjournal.dir option was only added in Coherence 12.2.1.4. Therefore, we must include an override file to define this so that it works in 12.2.1.3.x as well. bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set store.javaOpts=\"-Dcoherence.flashjournal.dir=/elastic-data\" \\ --set clusterSize=1 \\ --set cluster=elastic-data-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=elastic-data-cache-config.xml \\ --set store.overrideConfig=elastic-data-override.xml \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set userArtifacts.image=elastic-data-sample-external:1.0.0-SNAPSHOT \\ -f src/main/yaml/volumes.yaml \\ coherence/coherence After installing, list the pods by running the following command: bash $ kubectl get pods -n sample-coherence-ns All the three storage-coherence pods must be running and ready, as shown in the output: console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 4m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 1m 3. Confirm the mounted volume. bash $ kubectl exec -it storage-coherence-0 -n sample-coherence-ns -- bash -c df You should see your /elastic-data volume mounted. console Filesystem 1K-blocks Used Available Use% Mounted on overlay 61252420 16809112 41302140 29% / tmpfs 65536 0 65536 0% /dev tmpfs 4334408 0 4334408 0% /sys/fs/cgroup /dev/sda1 61252420 16809112 41302140 29% /logs overlay 4334408 356 4334052 1% /elastic-data shm 65536 0 65536 0% /dev/shm tmpfs 4334408 12 4334396 1% /run/secrets/kubernetes.io/serviceaccount tmpfs 4334408 0 4334408 0% /proc/acpi tmpfs 4334408 0 4334408 0% /sys/firmware 4. Add data to Elastic Data. a. Connect to the Coherence console to create a cache using FlashJournal : bash $ kubectl exec -it --namespace sample-coherence-ns storage-coherence-0 bash /scripts/startCoherence.sh console At the Map (?): prompt, type cache flash-01 . This creates a cache in the service, DistributedSchemeFlash which is a FlashJournal scheme. b. Use the following to add 100,000 objects of size 1024 bytes, starting at index 0, and using batches of 100. ```bash bulkput 100000 1024 0 100 Mon Apr 15 07:37:03 GMT 2019: adding 100000 items (starting with #0) each 1024 bytes ... Mon Apr 15 07:37:15 GMT 2019: done putting (11578ms, 8878KB/sec, 8637 items/sec) ``` At the prompt, type size and it should show 100000. Then, type bye to exit the console . Ensure that the Elastic Data FlashJournal files exist. Run the following command against one of the Coherence pods to list the files used by Elastic Data: bash $ kubectl exec -it -n sample-coherence-ns storage-coherence-0 -- bash -c 'ls -l /elastic-data' console total 202496 -rw-r--r-- 1 root root 69468160 May 1 08:44 coh1207347469383108692.tmp -rw-r--r-- 1 root root 68943872 May 1 08:44 coh5447980795344195354.tmp -rw-r--r-- 1 root root 68943872 May 1 08:44 coh6857569664465911116.tmp Type exit to leave the exec session. Uninstall the Charts Run the following commands to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Deploy Elastic Data Using External Volume Mapped to the Host"},{"location":"samples/coherence-deployments/elastic-data/external/#deploy-elastic-data-using-external-volume-mapped-to-the-host","text":"This sample shows how to create persistent volumes (PV), and then map the Elastic Data to be stored on these PV. This would allow for a specific size to be used for storing Elastic Data, rather than only relying on the size of the underlying default /tmp/ directory. Refer to the Oracle Elastic Data Documentation for more information about Elastic Data. Return to Elastic Data samples / Return to Coherence Deployments samples / Return to samples","title":"Deploy Elastic Data Using External Volume Mapped to the Host"},{"location":"samples/coherence-deployments/elastic-data/external/#sample-files","text":"src/main/docker/Dockerfile - Dockerfile for creating sidecar image from which the configuration will be read at pod startup src/main/resources/conf/elastic-data-cache-config.xml - Cache configuration for storage-tier","title":"Sample files"},{"location":"samples/coherence-deployments/elastic-data/external/#prerequisites","text":"Ensure that you have installed Coherence Operator by following the instructions here .","title":"Prerequisites"},{"location":"samples/coherence-deployments/elastic-data/external/#installation-steps","text":"Change to the samples/coherence-deployments/elastic-data/external directory. Ensure that you have your maven build environment set for JDK8, and build the project. bash $ mvn clean install -P docker This builds the docker image with the cache configuration files, with the name in the format, elastic-data-sample-external:${version} . For example, bash elastic-data-sample-external:1.0.0-SNAPSHOT Note: If you are running against a remote Kubernetes cluster, then you must push the above image to your repository accessible to that cluster. You must also prefix the image name in the helm command, as shown below. Install the Coherence cluster. You can use the following three options to specify volumes , volumeMounts & volumeClaimTemplates : --set store.volumes - Defines extra volume mappings that will be added to the Coherence Pod --set store.volumeClaimTemplates - Defines extra PVC mappings that will be added to the Coherence Pod --set store.volumeMounts - Defines extra volume mounts to map to the additional volumes or PVC declared above in store.volumes and store.volumeClaimTemplates For this sample, we are going to use the YAML file, volumes.yaml to specify hostPath volumes. Note: You should set the values appropriately for your Kubernetes environment and needs. Also, set --set store.javaOpts=\"-Dcoherence.flashjournal.dir=/elastic-data\" - to point Elastic data to the mount path. Note: The coherence.flashjournal.dir option was only added in Coherence 12.2.1.4. Therefore, we must include an override file to define this so that it works in 12.2.1.3.x as well. bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set store.javaOpts=\"-Dcoherence.flashjournal.dir=/elastic-data\" \\ --set clusterSize=1 \\ --set cluster=elastic-data-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=elastic-data-cache-config.xml \\ --set store.overrideConfig=elastic-data-override.xml \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set userArtifacts.image=elastic-data-sample-external:1.0.0-SNAPSHOT \\ -f src/main/yaml/volumes.yaml \\ coherence/coherence After installing, list the pods by running the following command: bash $ kubectl get pods -n sample-coherence-ns All the three storage-coherence pods must be running and ready, as shown in the output: console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 4m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 1m 3. Confirm the mounted volume. bash $ kubectl exec -it storage-coherence-0 -n sample-coherence-ns -- bash -c df You should see your /elastic-data volume mounted. console Filesystem 1K-blocks Used Available Use% Mounted on overlay 61252420 16809112 41302140 29% / tmpfs 65536 0 65536 0% /dev tmpfs 4334408 0 4334408 0% /sys/fs/cgroup /dev/sda1 61252420 16809112 41302140 29% /logs overlay 4334408 356 4334052 1% /elastic-data shm 65536 0 65536 0% /dev/shm tmpfs 4334408 12 4334396 1% /run/secrets/kubernetes.io/serviceaccount tmpfs 4334408 0 4334408 0% /proc/acpi tmpfs 4334408 0 4334408 0% /sys/firmware 4. Add data to Elastic Data. a. Connect to the Coherence console to create a cache using FlashJournal : bash $ kubectl exec -it --namespace sample-coherence-ns storage-coherence-0 bash /scripts/startCoherence.sh console At the Map (?): prompt, type cache flash-01 . This creates a cache in the service, DistributedSchemeFlash which is a FlashJournal scheme. b. Use the following to add 100,000 objects of size 1024 bytes, starting at index 0, and using batches of 100. ```bash bulkput 100000 1024 0 100 Mon Apr 15 07:37:03 GMT 2019: adding 100000 items (starting with #0) each 1024 bytes ... Mon Apr 15 07:37:15 GMT 2019: done putting (11578ms, 8878KB/sec, 8637 items/sec) ``` At the prompt, type size and it should show 100000. Then, type bye to exit the console . Ensure that the Elastic Data FlashJournal files exist. Run the following command against one of the Coherence pods to list the files used by Elastic Data: bash $ kubectl exec -it -n sample-coherence-ns storage-coherence-0 -- bash -c 'ls -l /elastic-data' console total 202496 -rw-r--r-- 1 root root 69468160 May 1 08:44 coh1207347469383108692.tmp -rw-r--r-- 1 root root 68943872 May 1 08:44 coh5447980795344195354.tmp -rw-r--r-- 1 root root 68943872 May 1 08:44 coh6857569664465911116.tmp Type exit to leave the exec session.","title":"Installation Steps"},{"location":"samples/coherence-deployments/elastic-data/external/#uninstall-the-charts","text":"Run the following commands to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/coherence-deployments/extend/","text":"Accessing Coherence via Coherence*Extend Table of Contents Access Coherence via default proxy port Access Coherence via separate proxy tier Enabling SLL for proxy servers Using multiple Coherence*Extend proxies Return to Coherence Deployments samples / Return to samples","title":"Accessing Coherence via Coherence*Extend"},{"location":"samples/coherence-deployments/extend/#accessing-coherence-via-coherenceextend","text":"","title":"Accessing Coherence via Coherence*Extend"},{"location":"samples/coherence-deployments/extend/#table-of-contents","text":"Access Coherence via default proxy port Access Coherence via separate proxy tier Enabling SLL for proxy servers Using multiple Coherence*Extend proxies Return to Coherence Deployments samples / Return to samples","title":"Table of Contents"},{"location":"samples/coherence-deployments/extend/default/","text":"Access Coherence via the Default Proxy Port This sample shows how to access the Coherence cluster via the default proxy service exposed on port 20000. Return to Coherence*Extend samples / Return to Coherence Deployments samples / Return to samples Sample files src/main/resources/client-cache-config.xml - Client configuration for the extend client Prerequisites Ensure that you have installed the Coherence Operator by following the instructions here . Installation Steps Change to the samples/coherence-deployments/extend/default directory. Ensure that you have your maven build environment set for JDK8, and build the project. bash $ mvn clean install Install the Coherence cluster. bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=storage-tier-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ coherence/coherence Once the installation is complete, get the list of pods by using the kubectl command: bash $ kubectl get pods -n sample-coherence-ns All the three storage-coherence pods should be running and ready, as shown in the output: console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 4m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 1m Port forward the proxy port on the storage-coherence-0 pod using the kubectl command: bash $ kubectl port-forward -n sample-coherence-ns storage-coherence-0 20000:20000 Connect via CohQL and run the following commands: bash $ mvn exec:java Run the following CohQL commands to insert data into the cluster. ```sql insert into 'test' key('key-1') value('value-1'); select key(), value() from 'test'; Results [\"key-1\", \"value-1\"] select count() from 'test'; Results 1 ``` Uninstall the Chart Run the following command to delete the chart installed in this sample. $ helm delete storage --purge Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Access Coherence via the Default Proxy Port"},{"location":"samples/coherence-deployments/extend/default/#access-coherence-via-the-default-proxy-port","text":"This sample shows how to access the Coherence cluster via the default proxy service exposed on port 20000. Return to Coherence*Extend samples / Return to Coherence Deployments samples / Return to samples","title":"Access Coherence via the Default Proxy Port"},{"location":"samples/coherence-deployments/extend/default/#sample-files","text":"src/main/resources/client-cache-config.xml - Client configuration for the extend client","title":"Sample files"},{"location":"samples/coherence-deployments/extend/default/#prerequisites","text":"Ensure that you have installed the Coherence Operator by following the instructions here .","title":"Prerequisites"},{"location":"samples/coherence-deployments/extend/default/#installation-steps","text":"Change to the samples/coherence-deployments/extend/default directory. Ensure that you have your maven build environment set for JDK8, and build the project. bash $ mvn clean install Install the Coherence cluster. bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=storage-tier-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ coherence/coherence Once the installation is complete, get the list of pods by using the kubectl command: bash $ kubectl get pods -n sample-coherence-ns All the three storage-coherence pods should be running and ready, as shown in the output: console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 4m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 1m Port forward the proxy port on the storage-coherence-0 pod using the kubectl command: bash $ kubectl port-forward -n sample-coherence-ns storage-coherence-0 20000:20000 Connect via CohQL and run the following commands: bash $ mvn exec:java Run the following CohQL commands to insert data into the cluster. ```sql insert into 'test' key('key-1') value('value-1'); select key(), value() from 'test'; Results [\"key-1\", \"value-1\"] select count() from 'test'; Results 1 ```","title":"Installation Steps"},{"location":"samples/coherence-deployments/extend/default/#uninstall-the-chart","text":"Run the following command to delete the chart installed in this sample. $ helm delete storage --purge Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Uninstall the Chart"},{"location":"samples/coherence-deployments/extend/multiple/","text":"Using Multiple Coherence*Extend Proxies This sample shows how to run multiple Proxy services on a single cluster node. To support this within the Coherence Operator, a custom cache configuration must be added as well as an additional pod must be exposed on the pod. Return to Coherence*Extend samples / Return to Coherence Deployments samples / Return to samples Sample files src/main/docker/Dockerfile - Dockerfile for creating sidecar image from which the configuration will be read at pod startup src/main/resources/client-cache-config.xml - Cache configuration for the extend client src/main/resources/conf/storage-cache-config.xml - Cache configuration for storage-tier Prerequisites Ensure that you have installed Coherence Operator by following the instructions here . Installation Steps Change to the samples/coherence-deployments/extend/multiple directory. Ensure that you have your maven build environment set for JDK8, and build the project. bash $ mvn clean install -P docker As a result, the docker image will be built with the cache configuration files and compiled Java classes with the name in the format, multiple-proxy-sample:${version} . For example, bash multiple-proxy-sample:1.0.0-SNAPSHOT Note If you are running against a remote Kubernetes cluster, then you must push the above image to your repository accessible to that cluster. You must also prefix the image name in your helm command below. Install the Coherence cluster. Set the following additional properties: --set store.ports.custom-port=20001 - This property sets the port for the second proxy server. --set store.javaOpts=\"-Dcoherence.extend.port2=20001\" - Set this property for cache configuration. bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=multiple-proxy-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=storage-cache-config.xml \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set userArtifacts.image=multiple-proxy-sample:1.0.0-SNAPSHOT \\ --set store.javaOpts=\"-Dcoherence.extend.port2=20001\" \\ --set store.ports.custom-port=20001 \\ coherence/coherence Once the installation is complete, run the following command to retrieve the list of pods: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 4m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 2m Ensure that both Proxy services are running, by using the following kubectl command: bash $ kubectl logs storage-coherence-0 -n sample-coherence-ns | grep 'TcpAcceptor now listening' | grep ProxyService 2019-05-01 01:55:38.856/8.215 Oracle Coherence GE 12.2.1.4.0 <Info> (thread=Proxy:ProxyService1:TcpAcceptor, member=1): TcpAcceptor now listening for connections on storage-coherence-0.coherence.sample-coherence-ns.svc.cluster.local:20000 2019-05-01 01:55:38.955/8.313 Oracle Coherence GE 12.2.1.4.0 <Info> (thread=Proxy:ProxyService2:TcpAcceptor, member=1): TcpAcceptor now listening for connections on storage-coherence-0.coherence.sample-coherence-ns.svc.cluster.local:20001 Port forward the proxy ports on the proxy-tier. bash $ kubectl port-forward -n sample-coherence-ns storage-coherence-0 20000:20000 bash $ kubectl port-forward -n sample-coherence-ns storage-coherence-0 20001:20001 Connect via CohQL using port 20000 and run the following command: bash $ mvn exec:java -Dproxy.port=20000 Run the following CohQL commands to insert data into the cluster via proxy port 20000. ``` insert into 'test' key('key-1') value('value-1'); select key(), value() from 'test'; Results [\"key-1\", \"value-1\"] select count() from 'test'; Results 1 ``` You should see a message indicating the connection to 127.0.0.1:20000, as shown in the output: console 2019-05-01 10:01:08.007/5.684 Oracle Coherence GE 12.2.1.4.0 <D6> (thread=com.tangosol.coherence.dslquery.QueryPlus.main(), member=n/a): Connecting Socket to 127.0.0.1:20000 Connect via CohQL using port 20001 using the following command: bash $ mvn exec:java -Dproxy.port=20001 Run the following CohQL commands to insert data into the cluster via proxy port 20001: ``` select key(), value() from 'test'; Results [\"key-1\", \"value-1\"] select count() from 'test'; Results 1 ``` You should see a message indicating the connection to 127.0.0.1:20001, as shown in the output: bash 2019-05-01 10:05:18.764/20.659 Oracle Coherence GE 12.2.1.4.0 <D6> (thread=com.tangosol.coherence.dslquery.QueryPlus.main(), member=n/a): Connecting Socket to 127.0.0.1:20001 Uninstall the Charts Run the following command to delete both the charts installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Using Multiple Coherence*Extend Proxies"},{"location":"samples/coherence-deployments/extend/multiple/#using-multiple-coherenceextend-proxies","text":"This sample shows how to run multiple Proxy services on a single cluster node. To support this within the Coherence Operator, a custom cache configuration must be added as well as an additional pod must be exposed on the pod. Return to Coherence*Extend samples / Return to Coherence Deployments samples / Return to samples","title":"Using Multiple Coherence*Extend Proxies"},{"location":"samples/coherence-deployments/extend/multiple/#sample-files","text":"src/main/docker/Dockerfile - Dockerfile for creating sidecar image from which the configuration will be read at pod startup src/main/resources/client-cache-config.xml - Cache configuration for the extend client src/main/resources/conf/storage-cache-config.xml - Cache configuration for storage-tier","title":"Sample files"},{"location":"samples/coherence-deployments/extend/multiple/#prerequisites","text":"Ensure that you have installed Coherence Operator by following the instructions here .","title":"Prerequisites"},{"location":"samples/coherence-deployments/extend/multiple/#installation-steps","text":"Change to the samples/coherence-deployments/extend/multiple directory. Ensure that you have your maven build environment set for JDK8, and build the project. bash $ mvn clean install -P docker As a result, the docker image will be built with the cache configuration files and compiled Java classes with the name in the format, multiple-proxy-sample:${version} . For example, bash multiple-proxy-sample:1.0.0-SNAPSHOT Note If you are running against a remote Kubernetes cluster, then you must push the above image to your repository accessible to that cluster. You must also prefix the image name in your helm command below. Install the Coherence cluster. Set the following additional properties: --set store.ports.custom-port=20001 - This property sets the port for the second proxy server. --set store.javaOpts=\"-Dcoherence.extend.port2=20001\" - Set this property for cache configuration. bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=multiple-proxy-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=storage-cache-config.xml \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set userArtifacts.image=multiple-proxy-sample:1.0.0-SNAPSHOT \\ --set store.javaOpts=\"-Dcoherence.extend.port2=20001\" \\ --set store.ports.custom-port=20001 \\ coherence/coherence Once the installation is complete, run the following command to retrieve the list of pods: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 4m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 2m Ensure that both Proxy services are running, by using the following kubectl command: bash $ kubectl logs storage-coherence-0 -n sample-coherence-ns | grep 'TcpAcceptor now listening' | grep ProxyService 2019-05-01 01:55:38.856/8.215 Oracle Coherence GE 12.2.1.4.0 <Info> (thread=Proxy:ProxyService1:TcpAcceptor, member=1): TcpAcceptor now listening for connections on storage-coherence-0.coherence.sample-coherence-ns.svc.cluster.local:20000 2019-05-01 01:55:38.955/8.313 Oracle Coherence GE 12.2.1.4.0 <Info> (thread=Proxy:ProxyService2:TcpAcceptor, member=1): TcpAcceptor now listening for connections on storage-coherence-0.coherence.sample-coherence-ns.svc.cluster.local:20001 Port forward the proxy ports on the proxy-tier. bash $ kubectl port-forward -n sample-coherence-ns storage-coherence-0 20000:20000 bash $ kubectl port-forward -n sample-coherence-ns storage-coherence-0 20001:20001 Connect via CohQL using port 20000 and run the following command: bash $ mvn exec:java -Dproxy.port=20000 Run the following CohQL commands to insert data into the cluster via proxy port 20000. ``` insert into 'test' key('key-1') value('value-1'); select key(), value() from 'test'; Results [\"key-1\", \"value-1\"] select count() from 'test'; Results 1 ``` You should see a message indicating the connection to 127.0.0.1:20000, as shown in the output: console 2019-05-01 10:01:08.007/5.684 Oracle Coherence GE 12.2.1.4.0 <D6> (thread=com.tangosol.coherence.dslquery.QueryPlus.main(), member=n/a): Connecting Socket to 127.0.0.1:20000 Connect via CohQL using port 20001 using the following command: bash $ mvn exec:java -Dproxy.port=20001 Run the following CohQL commands to insert data into the cluster via proxy port 20001: ``` select key(), value() from 'test'; Results [\"key-1\", \"value-1\"] select count() from 'test'; Results 1 ``` You should see a message indicating the connection to 127.0.0.1:20001, as shown in the output: bash 2019-05-01 10:05:18.764/20.659 Oracle Coherence GE 12.2.1.4.0 <D6> (thread=com.tangosol.coherence.dslquery.QueryPlus.main(), member=n/a): Connecting Socket to 127.0.0.1:20001","title":"Installation Steps"},{"location":"samples/coherence-deployments/extend/multiple/#uninstall-the-charts","text":"Run the following command to delete both the charts installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/coherence-deployments/extend/proxy-tier/","text":"Access Coherence via a Separate Proxy Tier This sample shows how to deploy two tiers, a storage-enabled data tier and a storage-disabled proxy tier. This is a common scenario when you are using Coherence*Extend to connect to a cluster and when you want to separate the proxy tier from the data tier. This is achieved by using two helm install commands, both of which include a sidecar container for the data tier and proxy tier cache configuration. Return to Coherence*Extend samples / Return to Coherence Deployments samples / Return to samples Sample files src/main/docker/Dockerfile - Dockerfile for creating sidecar image from which the configuration will be read at pod startup src/main/resources/client-cache-config.xml - Cache configuration for the extend client src/main/resources/conf/proxy-cache-config.xml - Cache configuration for proxy-tier src/main/resources/conf/storage-cache-config.xml - Cache configuration for storage-tier Prerequisites Ensure that you have installed the Coherence Operator by following the instructions here . Installation Steps Change to the samples/coherence-deployments/extend/proxy-tier directory. Ensure that you have your maven build environment set for JDK8, and build the project. bash $ mvn clean install -P docker This builds the Docker image with the cache configuration files and compiled Java classes, with the name in the format proxy-tier-sample:${version} . For example, ```bash proxy-tier-sample:1.0.0-SNAPSHOT ``` Note: If you are running against a remote Kubernetes cluster, you must push the above image to your repository accessible to that cluster. You must also prefix the image name in your helm command as shown below. Install the Coherence cluster. bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=proxy-tier-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=storage-cache-config.xml \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set userArtifacts.image=proxy-tier-sample:1.0.0-SNAPSHOT \\ coherence/coherence Once the installation is complete, run the following command to retrieve the list of pods: bash $ kubectl get pods -n sample-coherence-ns All the three storage-coherence pods should be running and ready, as shown in the output: console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 4m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 2m Install the storage-disabled proxy-tier. Set the following properties to ensure that this release connects to the Coherence clusterSize created by the storage release: --set cluster=proxy-tier-cluster - Uses the same cluster name --set store.wka=storage-coherence-headless - Ensures that it can contact the cluster --set cluster=proxy-tier-cluster - Ensures that the cluster name is the same --set prometheusoperator.enabled=false - Sets storage to false --set store.cacheConfig=proxy-cache-config.xml - Uses proxy cache configuration from sidecar Note: For the proxy-tier, we are using a clusterSize of one, to save resources. You can scale out the proxy-tier for high availability purposes. bash $ helm install \\ --namespace sample-coherence-ns \\ --set cluster=proxy-tier-cluster \\ --set clusterSize=1 \\ --set store.storageEnabled=false \\ --set store.wka=storage-coherence-headless \\ --set prometheusoperator.enabled=false \\ --name proxy-tier \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=proxy-cache-config.xml \\ --set prometheusoperator.enabled=false \\ --set userArtifacts.image=proxy-tier-sample:1.0.0-SNAPSHOT \\ coherence/coherence To confirm that the proxy-tier has joined the cluster, you can look at the logs using: bash $ kubectl logs proxy-tier-coherence-0 -n sample-coherence-ns | grep ActualMemberSet This should return the following, which indicates that there are now four members: console ActualMemberSet=MemberSet(Size=4 You should now see three charts installed: bash $ helm ls NAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE coherence-operator 1 Wed Mar 20 14:12:31 2019 DEPLOYED coherence-operator-1.0.0-SNAPSHOT 1.0.0-SNAPSHOT sample-coherence-ns proxy-tier 1 Wed Mar 20 14:54:57 2019 DEPLOYED coherence-1.0.0-SNAPSHOT 1.0.0-SNAPSHOT sample-coherence-ns storage 1 Wed Mar 20 14:53:58 2019 DEPLOYED coherence-1.0.0-SNAPSHOT 1.0.0-SNAPSHOT sample-coherence-ns Port forward the proxy port on the proxy-tier. bash $ kubectl port-forward -n sample-coherence-ns proxy-tier-coherence-0 20000:20000 Connect via CohQL and run the commands: bash $ mvn exec:java Run the following CohQL commands to insert data into the cluster. ```sql insert into 'test' key('key-1') value('value-1'); select key(), value() from 'test'; Results [\"key-1\", \"value-1\"] select count() from 'test'; Results 1 ``` Uninstall the Charts Run the following command to delete both the charts installed in this sample: $ helm delete storage proxy-tier --purge Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Access Coherence via a Separate Proxy Tier"},{"location":"samples/coherence-deployments/extend/proxy-tier/#access-coherence-via-a-separate-proxy-tier","text":"This sample shows how to deploy two tiers, a storage-enabled data tier and a storage-disabled proxy tier. This is a common scenario when you are using Coherence*Extend to connect to a cluster and when you want to separate the proxy tier from the data tier. This is achieved by using two helm install commands, both of which include a sidecar container for the data tier and proxy tier cache configuration. Return to Coherence*Extend samples / Return to Coherence Deployments samples / Return to samples","title":"Access Coherence via a Separate Proxy Tier"},{"location":"samples/coherence-deployments/extend/proxy-tier/#sample-files","text":"src/main/docker/Dockerfile - Dockerfile for creating sidecar image from which the configuration will be read at pod startup src/main/resources/client-cache-config.xml - Cache configuration for the extend client src/main/resources/conf/proxy-cache-config.xml - Cache configuration for proxy-tier src/main/resources/conf/storage-cache-config.xml - Cache configuration for storage-tier","title":"Sample files"},{"location":"samples/coherence-deployments/extend/proxy-tier/#prerequisites","text":"Ensure that you have installed the Coherence Operator by following the instructions here .","title":"Prerequisites"},{"location":"samples/coherence-deployments/extend/proxy-tier/#installation-steps","text":"Change to the samples/coherence-deployments/extend/proxy-tier directory. Ensure that you have your maven build environment set for JDK8, and build the project. bash $ mvn clean install -P docker This builds the Docker image with the cache configuration files and compiled Java classes, with the name in the format proxy-tier-sample:${version} . For example, ```bash proxy-tier-sample:1.0.0-SNAPSHOT ``` Note: If you are running against a remote Kubernetes cluster, you must push the above image to your repository accessible to that cluster. You must also prefix the image name in your helm command as shown below. Install the Coherence cluster. bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=proxy-tier-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=storage-cache-config.xml \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set userArtifacts.image=proxy-tier-sample:1.0.0-SNAPSHOT \\ coherence/coherence Once the installation is complete, run the following command to retrieve the list of pods: bash $ kubectl get pods -n sample-coherence-ns All the three storage-coherence pods should be running and ready, as shown in the output: console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 4m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 2m Install the storage-disabled proxy-tier. Set the following properties to ensure that this release connects to the Coherence clusterSize created by the storage release: --set cluster=proxy-tier-cluster - Uses the same cluster name --set store.wka=storage-coherence-headless - Ensures that it can contact the cluster --set cluster=proxy-tier-cluster - Ensures that the cluster name is the same --set prometheusoperator.enabled=false - Sets storage to false --set store.cacheConfig=proxy-cache-config.xml - Uses proxy cache configuration from sidecar Note: For the proxy-tier, we are using a clusterSize of one, to save resources. You can scale out the proxy-tier for high availability purposes. bash $ helm install \\ --namespace sample-coherence-ns \\ --set cluster=proxy-tier-cluster \\ --set clusterSize=1 \\ --set store.storageEnabled=false \\ --set store.wka=storage-coherence-headless \\ --set prometheusoperator.enabled=false \\ --name proxy-tier \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=proxy-cache-config.xml \\ --set prometheusoperator.enabled=false \\ --set userArtifacts.image=proxy-tier-sample:1.0.0-SNAPSHOT \\ coherence/coherence To confirm that the proxy-tier has joined the cluster, you can look at the logs using: bash $ kubectl logs proxy-tier-coherence-0 -n sample-coherence-ns | grep ActualMemberSet This should return the following, which indicates that there are now four members: console ActualMemberSet=MemberSet(Size=4 You should now see three charts installed: bash $ helm ls NAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE coherence-operator 1 Wed Mar 20 14:12:31 2019 DEPLOYED coherence-operator-1.0.0-SNAPSHOT 1.0.0-SNAPSHOT sample-coherence-ns proxy-tier 1 Wed Mar 20 14:54:57 2019 DEPLOYED coherence-1.0.0-SNAPSHOT 1.0.0-SNAPSHOT sample-coherence-ns storage 1 Wed Mar 20 14:53:58 2019 DEPLOYED coherence-1.0.0-SNAPSHOT 1.0.0-SNAPSHOT sample-coherence-ns Port forward the proxy port on the proxy-tier. bash $ kubectl port-forward -n sample-coherence-ns proxy-tier-coherence-0 20000:20000 Connect via CohQL and run the commands: bash $ mvn exec:java Run the following CohQL commands to insert data into the cluster. ```sql insert into 'test' key('key-1') value('value-1'); select key(), value() from 'test'; Results [\"key-1\", \"value-1\"] select count() from 'test'; Results 1 ```","title":"Installation Steps"},{"location":"samples/coherence-deployments/extend/proxy-tier/#uninstall-the-charts","text":"Run the following command to delete both the charts installed in this sample: $ helm delete storage proxy-tier --purge Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/coherence-deployments/extend/ssl/","text":"Enable SSL in Coherence 12.2.1.3.X This sample shows how to secure Coherence*Extend traffic via 2-way SSL when using the Oracle Coherence Operator with Coherence 12.2.1.3.x. Refer to the Coherence Documentation for more information about using SSL with Coherence. Note: If you are using Coherence 12.2.1.4.0, the instructions are slightly different, and you should refer to them here . Return to Coherence*Extend SSL samples / Return to Coherence*Extend samples / Return to Coherence Deployments samples / Return to samples Sample files src/main/docker/Dockerfile - Dockerfile for creating sidecar image from which the configuration will be read at pod startup src/main/resources/certs/keys.sh - File for generating keys for this example src/main/resources/client-cache-config.xml - Cache configuration for the extend client src/main/resources/conf/storage-cache-config.xml - Cache configuration for storage-tier Prerequisites Ensure that you have installed Coherence Operator by following the instructions here . Installation Steps Change to the samples/coherence-deployments/extend/ssl/12213 directory. Ensure that you have your maven build environment set for JDK8, and build the project. Note : This sample uses self-signed certificates and simple passwords. They are for demonstration purposes only and should NOT be used in a production environment. You should use and generate standard certificates with appropriate passwords. bash $ mvn clean install -P docker As a result, the docker image will be built with the cache configuration files and compiled Java classes with the name in the format, proxy-ssl-sample:${version} . For example, bash proxy-ssl-sample:1.0.0-SNAPSHOT Note: If you are running against a remote Kubernetes cluster, then you must push the above image to your repository accessible to that cluster. You must also prefix the image name in the helm command, as shown below. Install the Coherence cluster. Set the following property and ensure that correct cache configuration is used: --set store.cacheConfig=storage-cache-config.xml bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=proxy-ssl-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=storage-cache-config.xml \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set userArtifacts.image=proxy-ssl-sample:1.0.0-SNAPSHOT \\ coherence/coherence Once the installation is complete, run the following command to retrieve the list of pods: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 4m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 2m Port forward the proxy port on the proxy-tier. bash $ kubectl port-forward -n sample-coherence-ns storage-coherence-0 20000:20000 Connect via CohQL and run the commands: bash $ mvn exec:java Run the following CohQL commands to insert data into the cluster. sql insert into 'test' key('key-1') value('value-1'); ```sql select key(), value() from 'test'; Results [\"key-1\", \"value-1\"] select count() from 'test'; Results 1 ``` You should see a message indicating that the Coherence*Extend client is using SSLSocketProvider with 2-way auth, as shown in the output: console 2019-05-06 10:58:49.752/5.105 Oracle Coherence GE 12.2.1.3.2 <D5> (thread=com.tangosol.coherence.dslquery.QueryPlus.main(), member=n/a): instantiated SSLSocketProviderDependencies: SSLSocketProvider(auth=two-way, \\ identity=SunX509/file:conf/certs/groot.jks, trust=SunX509/file:conf/certs/truststore-all.jks) Type bye or CTRL-C to exit CohQL. Uninstall the Charts Run the following command to delete both the charts installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Enable SSL in Coherence 12.2.1.3.X"},{"location":"samples/coherence-deployments/extend/ssl/#enable-ssl-in-coherence-12213x","text":"This sample shows how to secure Coherence*Extend traffic via 2-way SSL when using the Oracle Coherence Operator with Coherence 12.2.1.3.x. Refer to the Coherence Documentation for more information about using SSL with Coherence. Note: If you are using Coherence 12.2.1.4.0, the instructions are slightly different, and you should refer to them here . Return to Coherence*Extend SSL samples / Return to Coherence*Extend samples / Return to Coherence Deployments samples / Return to samples","title":"Enable SSL in Coherence 12.2.1.3.X"},{"location":"samples/coherence-deployments/extend/ssl/#sample-files","text":"src/main/docker/Dockerfile - Dockerfile for creating sidecar image from which the configuration will be read at pod startup src/main/resources/certs/keys.sh - File for generating keys for this example src/main/resources/client-cache-config.xml - Cache configuration for the extend client src/main/resources/conf/storage-cache-config.xml - Cache configuration for storage-tier","title":"Sample files"},{"location":"samples/coherence-deployments/extend/ssl/#prerequisites","text":"Ensure that you have installed Coherence Operator by following the instructions here .","title":"Prerequisites"},{"location":"samples/coherence-deployments/extend/ssl/#installation-steps","text":"Change to the samples/coherence-deployments/extend/ssl/12213 directory. Ensure that you have your maven build environment set for JDK8, and build the project. Note : This sample uses self-signed certificates and simple passwords. They are for demonstration purposes only and should NOT be used in a production environment. You should use and generate standard certificates with appropriate passwords. bash $ mvn clean install -P docker As a result, the docker image will be built with the cache configuration files and compiled Java classes with the name in the format, proxy-ssl-sample:${version} . For example, bash proxy-ssl-sample:1.0.0-SNAPSHOT Note: If you are running against a remote Kubernetes cluster, then you must push the above image to your repository accessible to that cluster. You must also prefix the image name in the helm command, as shown below. Install the Coherence cluster. Set the following property and ensure that correct cache configuration is used: --set store.cacheConfig=storage-cache-config.xml bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=proxy-ssl-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=storage-cache-config.xml \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set userArtifacts.image=proxy-ssl-sample:1.0.0-SNAPSHOT \\ coherence/coherence Once the installation is complete, run the following command to retrieve the list of pods: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 4m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 2m Port forward the proxy port on the proxy-tier. bash $ kubectl port-forward -n sample-coherence-ns storage-coherence-0 20000:20000 Connect via CohQL and run the commands: bash $ mvn exec:java Run the following CohQL commands to insert data into the cluster. sql insert into 'test' key('key-1') value('value-1'); ```sql select key(), value() from 'test'; Results [\"key-1\", \"value-1\"] select count() from 'test'; Results 1 ``` You should see a message indicating that the Coherence*Extend client is using SSLSocketProvider with 2-way auth, as shown in the output: console 2019-05-06 10:58:49.752/5.105 Oracle Coherence GE 12.2.1.3.2 <D5> (thread=com.tangosol.coherence.dslquery.QueryPlus.main(), member=n/a): instantiated SSLSocketProviderDependencies: SSLSocketProvider(auth=two-way, \\ identity=SunX509/file:conf/certs/groot.jks, trust=SunX509/file:conf/certs/truststore-all.jks) Type bye or CTRL-C to exit CohQL.","title":"Installation Steps"},{"location":"samples/coherence-deployments/extend/ssl/#uninstall-the-charts","text":"Run the following command to delete both the charts installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/coherence-deployments/federation/","text":"Federation Note: Use of Coherence Federation is only available when using the operator with Coherence 12.2.1.4. Table of Contents Within a single Kubernetes cluster Across across separate Kubernetes clusters Return to Coherence Deployments samples / Return to samples","title":"Federation"},{"location":"samples/coherence-deployments/federation/#federation","text":"Note: Use of Coherence Federation is only available when using the operator with Coherence 12.2.1.4.","title":"Federation"},{"location":"samples/coherence-deployments/federation/#table-of-contents","text":"Within a single Kubernetes cluster Across across separate Kubernetes clusters Return to Coherence Deployments samples / Return to samples","title":"Table of Contents"},{"location":"samples/coherence-deployments/federation/across-clusters/","text":"Across Separate Kubernetes Clusters The Federated Caching feature federates cached data asynchronously across multiple geographically dispersed clusters. Cached data is federated across clusters to provide redundancy, off-site backup, and multiple points of access for application users in different geographical locations. This sample shows how to set up two Federated Coherence clusters across separate Kubernetes clusters. Note : To set up two Federated Coherence clusters within a single Kubernetes cluster, refer to the additional information here . Return to Federation samples / Return to Coherence Deployments samples / Return to samples Installation Steps Follow the steps described in Federation Within a Single Kubernetes Cluster , with the following changes: Expose port 40000 on each cluster to a visible IP or load balancer. You must expose the cluster port 40000 (or an alternative) via an IP directly or via load balancer on each cluster. This allows the clusters to communicate. Refer to the Kubernetes documentation for more information. Ignore any port-forward commands. Install the Coherence clusters. Once you have an IP/PORT for each cluster, you must change the following in the --set store.javaOpts option in the installation steps 2 and 4 : priamry.cluster.host - The external IP of the primary cluster primary.cluster.port - The external port (default to 40000) of the primary cluster secondary.cluster.host - The external IP of the secondary cluster secondary.cluster.port - The external port (default to 40000) of the secondary cluster Eg. bash --set store.javaOpts=\"-Dprimary.cluster=PrimaryCluster -Dprimary.cluster.port=40000 -Dprimary.cluster.host=PRIMARY-CLUSTER-IP -Dsecondary.cluster=SecondaryCluster -Dsecondary.cluster.port=40000 -Dsecondary.cluster.host=SECONDARY-CLUSTER-IP\" \\","title":"Across Separate Kubernetes Clusters"},{"location":"samples/coherence-deployments/federation/across-clusters/#across-separate-kubernetes-clusters","text":"The Federated Caching feature federates cached data asynchronously across multiple geographically dispersed clusters. Cached data is federated across clusters to provide redundancy, off-site backup, and multiple points of access for application users in different geographical locations. This sample shows how to set up two Federated Coherence clusters across separate Kubernetes clusters. Note : To set up two Federated Coherence clusters within a single Kubernetes cluster, refer to the additional information here . Return to Federation samples / Return to Coherence Deployments samples / Return to samples","title":"Across Separate Kubernetes Clusters"},{"location":"samples/coherence-deployments/federation/across-clusters/#installation-steps","text":"Follow the steps described in Federation Within a Single Kubernetes Cluster , with the following changes: Expose port 40000 on each cluster to a visible IP or load balancer. You must expose the cluster port 40000 (or an alternative) via an IP directly or via load balancer on each cluster. This allows the clusters to communicate. Refer to the Kubernetes documentation for more information. Ignore any port-forward commands. Install the Coherence clusters. Once you have an IP/PORT for each cluster, you must change the following in the --set store.javaOpts option in the installation steps 2 and 4 : priamry.cluster.host - The external IP of the primary cluster primary.cluster.port - The external port (default to 40000) of the primary cluster secondary.cluster.host - The external IP of the secondary cluster secondary.cluster.port - The external port (default to 40000) of the secondary cluster Eg. bash --set store.javaOpts=\"-Dprimary.cluster=PrimaryCluster -Dprimary.cluster.port=40000 -Dprimary.cluster.host=PRIMARY-CLUSTER-IP -Dsecondary.cluster=SecondaryCluster -Dsecondary.cluster.port=40000 -Dsecondary.cluster.host=SECONDARY-CLUSTER-IP\" \\","title":"Installation Steps"},{"location":"samples/coherence-deployments/federation/within-cluster/","text":"Within a Single Kubernetes Cluster The Federated Caching feature federates cached data asynchronously across multiple geographically dispersed clusters. Cached data is federated across clusters to provide redundancy, off-site backup, and multiple points of access for application users in different geographical locations. This sample shows how to set up two Federated Coherence clusters within a single Kubernetes cluster. Although this is not a recommended topology, due to the co-location of the Coherence clusters, this is included as a sample for how to use Federation. Note : To set up two Federated Coherence clusters across different Kubernetes clusters, refer to the additional information here . The setup for this example uses two Coherence clusters in the same Kubernetes cluster, with the following details: Primary Cluster Release name: cluster-1 Cluster name: PrimaryCluster Secondary Cluster Release name: cluster-2 Cluster name: SecondaryCluster Return to Federation samples / Return to Coherence Deployments samples / Return to samples Sample files src/main/docker/Dockerfile - Dockerfile for creating sidecar image from which the configuration will be read at pod startup src/main/resources/client-cache-config.xml - Cache configuration for the extend client src/main/resources/conf/cache-config-federation.xml - Cache configuration for Federation src/main/resources/conf/tangosol-coherence-override-federation.xml - Override for Federation Prerequisites Ensure that you have installed Coherence Operator by following the instructions here . Installation Steps Change to the samples/coherence-deployments/federation/within-cluster directory. Ensure you have your maven build environment set for JDK8, and build the project. bash $ mvn clean install -P docker This builds the Docker image with the cache configuration files, with the name in the format, proxy-tier-sample:${version} . For example, bash federation-within-cluster-sample:1.0.0-SNAPSHOT Note: If you are running against a remote Kubernetes cluster, you must push the above image to your repository accessible to that cluster. You must also prefix the image name in the helm command, as shown below. Install the primary Coherence cluster, PrimaryCluster . bash $ helm install \\ --namespace sample-coherence-ns \\ --name cluster-1 \\ --set clusterSize=2 \\ --set cluster=PrimaryCluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=cache-config-federation.xml \\ --set store.overrideConfig=tangosol-coherence-override-federation.xml \\ --set store.javaOpts=\"-Dprimary.cluster=PrimaryCluster -Dprimary.cluster.port=40000 -Dprimary.cluster.host=cluster-1-coherence-headless -Dsecondary.cluster=SecondaryCluster -Dsecondary.cluster.port=40000 -Dsecondary.cluster.host=cluster-2-coherence-headless\" \\ --set store.ports.federation=40000 \\ --set userArtifacts.image=federation-within-cluster-sample:1.0.0-SNAPSHOT \\ --set coherence.image=your-12.2.1.4.0-Coherence-image \\ coherence/coherence Note: Ensure that you replace your-12.2.1.4.0-Coherence-image with the appropriate Coherence 12.2.1.4.0 Docker image. After the installation is complete, get the list of pods by running the following command: bash $ kubectl get pods -n sample-coherence-ns Both the cluster-1-coherence pods should be running and ready, as shown in the output: console NAME READY STATUS RESTARTS AGE cluster-1-coherence-0 1/1 Running 0 24s cluster-1-coherence-1 1/1 Running 0 24s coherence-operator-695b9456d5-bzbhl 1/1 Running 0 30m Port Forward the PrimaryCluster Coherence*Extend - Port 20000. bash $ kubectl port-forward --namespace sample-coherence-ns cluster-1-coherence-0 20000:20000 Install the secondary Coherence cluster, SecondaryCluster . bash $ helm install \\ --namespace sample-coherence-ns \\ --name cluster-2 \\ --set clusterSize=2 \\ --set cluster=SecondaryCluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=cache-config-federation.xml \\ --set store.overrideConfig=tangosol-coherence-override-federation.xml \\ --set store.javaOpts=\"-Dprimary.cluster=PrimaryCluster -Dprimary.cluster.port=40000 -Dprimary.cluster.host=cluster-1-coherence-headless -Dsecondary.cluster=SecondaryCluster -Dsecondary.cluster.port=40000 -Dsecondary.cluster.host=cluster-2-coherence-headless\" \\ --set store.ports.federation=40000 \\ --set userArtifacts.image=federation-within-cluster-sample:1.0.0-SNAPSHOT \\ --set coherence.image=your-12.2.1.4.0-Coherence-image \\ coherence/coherence After the installation has completed, get the list of pods by running the following command: bash $ kubectl get pods -n sample-coherence-ns All the four cluster-1 and cluster-2 pods should be running and ready, as shown in the output: console NAME READY STATUS RESTARTS AGE cluster-1-coherence-0 1/1 Running 0 4m cluster-1-coherence-1 1/1 Running 0 4m cluster-2-coherence-0 1/1 Running 0 36s cluster-2-coherence-1 1/1 Running 0 36s coherence-operator-695b9456d5-bzbhl 1/1 Running 0 34m Port Forward the SecondaryCluster Coherence*Extend - Port 20001. bash $ kubectl port-forward --namespace sample-coherence-ns cluster-2-coherence-0 20001:20000 Use CohQL to connect to each cluster. Run CohQL against the PrimaryCluster by using the following command: bash $ mvn exec:java -Dproxy.port=20000 Open another terminal and run CohQL against the SecondaryCluster : bash $ mvn exec:java -Dproxy.port=20001 Insert data into the PrimaryCluster . Run the following CohQL command to insert data into the PrimaryCluster : ```sql insert into 'test' key('key-1') value('value-1'); select key(), value() from 'test'; Results [\"key-1\", \"value-1\"] select count() from 'test'; Results 1 ``` Confirm that the data has been federated to the SecondaryCluster . Run the following CohQL command to insert data into the SecondaryCluster : sql select key(), value() from 'test'; Results [\"key-1\", \"value-1\"] Insert data into the SecondaryCluster Run the following CohQL command to insert data into the SecondaryCluster : ```sql insert into 'test' key('key-2') value('value-2'); select key(), value() from 'test'; Results [\"key-1\", \"value-1\"] [\"key-2\", \"value-2\"] ``` Confirm that the data has been federated to the PrimaryCluster . Run the following CohQL command to insert data into the PrimaryCluster : sql select key(), value() from 'test'; Results [\"key-1\", \"value-1\"] [\"key-2\", \"value-2\"] Uninstall the Charts Run the following command to delete both the charts installed in this sample: $ helm delete cluster-1 cluster-2 --purge Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Within a Single Kubernetes Cluster"},{"location":"samples/coherence-deployments/federation/within-cluster/#within-a-single-kubernetes-cluster","text":"The Federated Caching feature federates cached data asynchronously across multiple geographically dispersed clusters. Cached data is federated across clusters to provide redundancy, off-site backup, and multiple points of access for application users in different geographical locations. This sample shows how to set up two Federated Coherence clusters within a single Kubernetes cluster. Although this is not a recommended topology, due to the co-location of the Coherence clusters, this is included as a sample for how to use Federation. Note : To set up two Federated Coherence clusters across different Kubernetes clusters, refer to the additional information here . The setup for this example uses two Coherence clusters in the same Kubernetes cluster, with the following details: Primary Cluster Release name: cluster-1 Cluster name: PrimaryCluster Secondary Cluster Release name: cluster-2 Cluster name: SecondaryCluster Return to Federation samples / Return to Coherence Deployments samples / Return to samples","title":"Within a Single Kubernetes Cluster"},{"location":"samples/coherence-deployments/federation/within-cluster/#sample-files","text":"src/main/docker/Dockerfile - Dockerfile for creating sidecar image from which the configuration will be read at pod startup src/main/resources/client-cache-config.xml - Cache configuration for the extend client src/main/resources/conf/cache-config-federation.xml - Cache configuration for Federation src/main/resources/conf/tangosol-coherence-override-federation.xml - Override for Federation","title":"Sample files"},{"location":"samples/coherence-deployments/federation/within-cluster/#prerequisites","text":"Ensure that you have installed Coherence Operator by following the instructions here .","title":"Prerequisites"},{"location":"samples/coherence-deployments/federation/within-cluster/#installation-steps","text":"Change to the samples/coherence-deployments/federation/within-cluster directory. Ensure you have your maven build environment set for JDK8, and build the project. bash $ mvn clean install -P docker This builds the Docker image with the cache configuration files, with the name in the format, proxy-tier-sample:${version} . For example, bash federation-within-cluster-sample:1.0.0-SNAPSHOT Note: If you are running against a remote Kubernetes cluster, you must push the above image to your repository accessible to that cluster. You must also prefix the image name in the helm command, as shown below. Install the primary Coherence cluster, PrimaryCluster . bash $ helm install \\ --namespace sample-coherence-ns \\ --name cluster-1 \\ --set clusterSize=2 \\ --set cluster=PrimaryCluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=cache-config-federation.xml \\ --set store.overrideConfig=tangosol-coherence-override-federation.xml \\ --set store.javaOpts=\"-Dprimary.cluster=PrimaryCluster -Dprimary.cluster.port=40000 -Dprimary.cluster.host=cluster-1-coherence-headless -Dsecondary.cluster=SecondaryCluster -Dsecondary.cluster.port=40000 -Dsecondary.cluster.host=cluster-2-coherence-headless\" \\ --set store.ports.federation=40000 \\ --set userArtifacts.image=federation-within-cluster-sample:1.0.0-SNAPSHOT \\ --set coherence.image=your-12.2.1.4.0-Coherence-image \\ coherence/coherence Note: Ensure that you replace your-12.2.1.4.0-Coherence-image with the appropriate Coherence 12.2.1.4.0 Docker image. After the installation is complete, get the list of pods by running the following command: bash $ kubectl get pods -n sample-coherence-ns Both the cluster-1-coherence pods should be running and ready, as shown in the output: console NAME READY STATUS RESTARTS AGE cluster-1-coherence-0 1/1 Running 0 24s cluster-1-coherence-1 1/1 Running 0 24s coherence-operator-695b9456d5-bzbhl 1/1 Running 0 30m Port Forward the PrimaryCluster Coherence*Extend - Port 20000. bash $ kubectl port-forward --namespace sample-coherence-ns cluster-1-coherence-0 20000:20000 Install the secondary Coherence cluster, SecondaryCluster . bash $ helm install \\ --namespace sample-coherence-ns \\ --name cluster-2 \\ --set clusterSize=2 \\ --set cluster=SecondaryCluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=cache-config-federation.xml \\ --set store.overrideConfig=tangosol-coherence-override-federation.xml \\ --set store.javaOpts=\"-Dprimary.cluster=PrimaryCluster -Dprimary.cluster.port=40000 -Dprimary.cluster.host=cluster-1-coherence-headless -Dsecondary.cluster=SecondaryCluster -Dsecondary.cluster.port=40000 -Dsecondary.cluster.host=cluster-2-coherence-headless\" \\ --set store.ports.federation=40000 \\ --set userArtifacts.image=federation-within-cluster-sample:1.0.0-SNAPSHOT \\ --set coherence.image=your-12.2.1.4.0-Coherence-image \\ coherence/coherence After the installation has completed, get the list of pods by running the following command: bash $ kubectl get pods -n sample-coherence-ns All the four cluster-1 and cluster-2 pods should be running and ready, as shown in the output: console NAME READY STATUS RESTARTS AGE cluster-1-coherence-0 1/1 Running 0 4m cluster-1-coherence-1 1/1 Running 0 4m cluster-2-coherence-0 1/1 Running 0 36s cluster-2-coherence-1 1/1 Running 0 36s coherence-operator-695b9456d5-bzbhl 1/1 Running 0 34m Port Forward the SecondaryCluster Coherence*Extend - Port 20001. bash $ kubectl port-forward --namespace sample-coherence-ns cluster-2-coherence-0 20001:20000 Use CohQL to connect to each cluster. Run CohQL against the PrimaryCluster by using the following command: bash $ mvn exec:java -Dproxy.port=20000 Open another terminal and run CohQL against the SecondaryCluster : bash $ mvn exec:java -Dproxy.port=20001 Insert data into the PrimaryCluster . Run the following CohQL command to insert data into the PrimaryCluster : ```sql insert into 'test' key('key-1') value('value-1'); select key(), value() from 'test'; Results [\"key-1\", \"value-1\"] select count() from 'test'; Results 1 ``` Confirm that the data has been federated to the SecondaryCluster . Run the following CohQL command to insert data into the SecondaryCluster : sql select key(), value() from 'test'; Results [\"key-1\", \"value-1\"] Insert data into the SecondaryCluster Run the following CohQL command to insert data into the SecondaryCluster : ```sql insert into 'test' key('key-2') value('value-2'); select key(), value() from 'test'; Results [\"key-1\", \"value-1\"] [\"key-2\", \"value-2\"] ``` Confirm that the data has been federated to the PrimaryCluster . Run the following CohQL command to insert data into the PrimaryCluster : sql select key(), value() from 'test'; Results [\"key-1\", \"value-1\"] [\"key-2\", \"value-2\"]","title":"Installation Steps"},{"location":"samples/coherence-deployments/federation/within-cluster/#uninstall-the-charts","text":"Run the following command to delete both the charts installed in this sample: $ helm delete cluster-1 cluster-2 --purge Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/coherence-deployments/multiple-clusters/","text":"Installing Multiple Coherence Clusters with One Operator This sample shows how Coherence Operator can manage two or more Coherence clusters, and how you can see the logs from both clusters using Kibana. Return to Coherence Deployments samples / Return to samples Installation Steps Install Coherence Operator. Run the following command to install coherence-operator with log capture enabled: Note: If you have already installede coherence-operator without log capture enabled, you must delete it using helm delete coherence-operator --purge before continuing. bash $ helm install \\ --namespace sample-coherence-ns \\ --set imagePullSecrets=sample-coherence-secret \\ --name coherence-operator \\ --set logCaptureEnabled=true \\ --set \"targetNamespaces={sample-coherence-ns}\" \\ coherence/coherence-operator After installation, run the following command to list the pods: bash $ kubectl get pods -n sample-coherence-ns Along with coherence-operator , the command also returns the elasticsearch and kibana pods. console NAME READY STATUS RESTARTS AGE coherence-operator-7f596c6796-nns9n 2/2 Running 0 41s elasticsearch-5b5474865c-86888 1/1 Running 0 41s kibana-f6955c4b9-4ndsh 1/1 Running 0 41s 2. Install the first Coherence cluster, cluster-a . For each cluster, create only two pods to save resources. bash $ helm install \\ --namespace sample-coherence-ns \\ --name cluster-a \\ --set clusterSize=2 \\ --set cluster=cluster-a \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=true \\ coherence/coherence Install the second Coherence cluster, cluster-b For each cluster, create only two pods to save resources. bash $ helm install \\ --namespace sample-coherence-ns \\ --name cluster-b \\ --set clusterSize=2 \\ --set cluster=cluster-b \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=true \\ coherence/coherence Access Kibana Use the port-forward-kibana.sh script in the ../../common directory to view log messages. Start port forwarding. bash $ ./port-forward-kibana.sh sample-coherence-ns console Forwarding from 127.0.0.1:5601 -> 5601 Forwarding from [::1]:5601 -> 5601 2. Access Kibana using the following URL: http://127.0.0.1:5601/ Note: It may take up to five minutes for the data to reach the elasticsearch instance. Once logged in, click Dashboard on the left and choose Coherence Cluster - All Messages . You should see messages from both clusters, cluster-a and cluster-b . Uninstall the Charts Run the following command to delete the charts installed in this sample. $ helm delete cluster-a cluster-b --purge Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Installing Multiple Coherence Clusters with One Operator"},{"location":"samples/coherence-deployments/multiple-clusters/#installing-multiple-coherence-clusters-with-one-operator","text":"This sample shows how Coherence Operator can manage two or more Coherence clusters, and how you can see the logs from both clusters using Kibana. Return to Coherence Deployments samples / Return to samples","title":"Installing Multiple Coherence Clusters with One Operator"},{"location":"samples/coherence-deployments/multiple-clusters/#installation-steps","text":"Install Coherence Operator. Run the following command to install coherence-operator with log capture enabled: Note: If you have already installede coherence-operator without log capture enabled, you must delete it using helm delete coherence-operator --purge before continuing. bash $ helm install \\ --namespace sample-coherence-ns \\ --set imagePullSecrets=sample-coherence-secret \\ --name coherence-operator \\ --set logCaptureEnabled=true \\ --set \"targetNamespaces={sample-coherence-ns}\" \\ coherence/coherence-operator After installation, run the following command to list the pods: bash $ kubectl get pods -n sample-coherence-ns Along with coherence-operator , the command also returns the elasticsearch and kibana pods. console NAME READY STATUS RESTARTS AGE coherence-operator-7f596c6796-nns9n 2/2 Running 0 41s elasticsearch-5b5474865c-86888 1/1 Running 0 41s kibana-f6955c4b9-4ndsh 1/1 Running 0 41s 2. Install the first Coherence cluster, cluster-a . For each cluster, create only two pods to save resources. bash $ helm install \\ --namespace sample-coherence-ns \\ --name cluster-a \\ --set clusterSize=2 \\ --set cluster=cluster-a \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=true \\ coherence/coherence Install the second Coherence cluster, cluster-b For each cluster, create only two pods to save resources. bash $ helm install \\ --namespace sample-coherence-ns \\ --name cluster-b \\ --set clusterSize=2 \\ --set cluster=cluster-b \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=true \\ coherence/coherence","title":"Installation Steps"},{"location":"samples/coherence-deployments/multiple-clusters/#access-kibana","text":"Use the port-forward-kibana.sh script in the ../../common directory to view log messages. Start port forwarding. bash $ ./port-forward-kibana.sh sample-coherence-ns console Forwarding from 127.0.0.1:5601 -> 5601 Forwarding from [::1]:5601 -> 5601 2. Access Kibana using the following URL: http://127.0.0.1:5601/ Note: It may take up to five minutes for the data to reach the elasticsearch instance. Once logged in, click Dashboard on the left and choose Coherence Cluster - All Messages . You should see messages from both clusters, cluster-a and cluster-b .","title":"Access Kibana"},{"location":"samples/coherence-deployments/multiple-clusters/#uninstall-the-charts","text":"Run the following command to delete the charts installed in this sample. $ helm delete cluster-a cluster-b --purge Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/coherence-deployments/persistence/","text":"Persistence The Coherence chart allows you to configure various Persistence options. Refer to the Coherence documentation for more information about Persistence. Table of Contents Use default persistent volume claim Use specific persistent volumes Return to Coherence Deployments samples / Return to samples","title":"Persistence"},{"location":"samples/coherence-deployments/persistence/#persistence","text":"The Coherence chart allows you to configure various Persistence options. Refer to the Coherence documentation for more information about Persistence.","title":"Persistence"},{"location":"samples/coherence-deployments/persistence/#table-of-contents","text":"Use default persistent volume claim Use specific persistent volumes Return to Coherence Deployments samples / Return to samples","title":"Table of Contents"},{"location":"samples/coherence-deployments/persistence/default/","text":"Use the Default Persistent Volume Claim By default, when you enable Coherence Persistence, the required infrastructure in terms of persistent volumes (PV) and persistent volume claims (PVC) is set up automatically. Also, the persistence-mode is set to active . This allows the Coherence cluster to be restarted and the data to be retained. This sample shows how to enable Persistence with all the defaults under store.persistence in the coherence chart, values.yaml . Refer to this sample for more details about setting other values such as storageClasses . Return to Persistence samples / Return to Coherence Deployments samples / Return to samples Prerequisites Ensure that you have installed Coherence Operator by following the instructions here . Installation Steps Install the Coherence cluster. bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=persistence-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set store.persistence.enabled=true \\ --set store.snapshot.enabled=true \\ coherence/coherence You may also change the size of the default directories: /persistence default 2Gi - --set store.persistence.size=10Gi /snapshot default 2Gi - --set store.snapshot.size-10Gi Ensure that the pods are running, by using the following command: bash $ kubectl get pods -n sample-coherence-ns When all the three pods are in the Running state, proceed to check the PVC. console NAME READY STATUS RESTARTS AGE coherence-operator-66f9bb7b75-hqk4l 1/1 Running 0 13m storage-coherence-0 1/1 Running 0 3m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 0/1 Running 0 44s Run the following command to check the PVC created: bash $ kubectl get pvc -n sample-coherence-ns console NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistence-volume-storage-coherence-0 Bound pvc-a3fa6ce3-6588-11e9-bad6-025000000001 2Gi RWO hostpath 44m persistence-volume-storage-coherence-1 Bound pvc-d2f732e7-6588-11e9-bad6-025000000001 2Gi RWO hostpath 43m persistence-volume-storage-coherence-2 Bound pvc-fc175ea1-6588-11e9-bad6-025000000001 2Gi RWO hostpath 41m snapshot-volume-storage-coherence-0 Bound pvc-a3fb2172-6588-11e9-bad6-025000000001 2Gi RWO hostpath 44m snapshot-volume-storage-coherence-1 Bound pvc-d2f89ce9-6588-11e9-bad6-025000000001 2Gi RWO hostpath 43m snapshot-volume-storage-coherence-2 Bound pvc-fc13ae4b-6588-11e9-bad6-025000000001 2Gi RWO hostpath 41m Add data to the cluster. a. Connect to the Coherence console using the following command to create a cache: bash $ kubectl exec -it --namespace sample-coherence-ns storage-coherence-0 bash /scripts/startCoherence.sh console b. At the Map (?): prompt, type cache test . This creates a cache in the service, PartitionedCache . c. Use the following to add 50,000 objects of size 1024 bytes, starting at index 0 and using batches of 100: ```bash bulkput 50000 1024 0 100 Mon Apr 15 07:37:03 GMT 2019: adding 500000 items (starting with #0) each 1024 bytes ... Mon Apr 15 07:37:15 GMT 2019: done putting (11578ms, 8878KB/sec, 8637 items/sec) ``` At the prompt, type size and it should show 50000. d. Create a snapshot of the PartitionedCache service which contains the cache test . This is for later use. bash snapshot create test-snapshot Issuing createSnapshot for service PartitionedCache and snapshot test-snapshot Success bash snapshot list Snapshots for service PartitionedCache test-snapshot To exit the console, type bye or CTRL-C. Delete the Coherence cluster by using helm delete : bash $ helm delete storage --purge Before continuing, ensure that the pods are deleted using the following command: bash $ kubectl get pods -n sample-coherence-ns Ensure that there are no pods running with the name coherence-storage-* . Run the following command to ensure that the PVC still exists: bash $ kubectl get pvc -n sample-coherence-ns console NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistence-volume-storage-coherence-0 Bound pvc-a3fa6ce3-6588-11e9-bad6-025000000001 2Gi RWO hostpath 44m persistence-volume-storage-coherence-1 Bound pvc-d2f732e7-6588-11e9-bad6-025000000001 2Gi RWO hostpath 43m persistence-volume-storage-coherence-2 Bound pvc-fc175ea1-6588-11e9-bad6-025000000001 2Gi RWO hostpath 41m snapshot-volume-storage-coherence-0 Bound pvc-a3fb2172-6588-11e9-bad6-025000000001 2Gi RWO hostpath 44m snapshot-volume-storage-coherence-1 Bound pvc-d2f89ce9-6588-11e9-bad6-025000000001 2Gi RWO hostpath 43m snapshot-volume-storage-coherence-2 Bound pvc-fc13ae4b-6588-11e9-bad6-025000000001 2Gi RWO hostpath 41m Reinstall the Coherence cluster with Persistence enabled, using the following command: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=storage-tier-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set store.persistence.enabled=true \\ --set store.snapshot.enabled=true \\ coherence/coherence Wait until all the three pods are running before you continue to the next step. Ensure that the data previously added still exists. a. Connect to the Coherence console using the following: bash $ kubectl exec -it --namespace sample-coherence-ns storage-coherence-0 bash /scripts/startCoherence.sh console b. At the Map (?): prompt, type cache test . This creates/uses a cache in the service, PartitionedCache . c. At the prompt, type size and it should show 50000. This shows that the previous data entered has automatically been recovered as the PVC was honoured. Note: There is currently a bug with default /persistence mount not being created on Docker for Mac environments, and therefore the size may show zero. d. Clear the cache using the clear command and confirm that the cache size is zero. Recover the `test-snapshot` using: bash snapshot recover test-snapshot The size of the cache should now be 50000. To exit the console, type bye . Uninstall the Charts Run the following commands to delete the chart installed in this sample. $ helm delete storage --purge After deleting the pods, run the following command to delete the PVC: $ kubectl get pvc -n sample-coherence-ns | sed 1d | awk '{print $1}' | xargs kubectl delete pvc -n sample-coherence-ns persistentvolumeclaim \"persistence-volume-storage-coherence-0\" deleted persistentvolumeclaim \"persistence-volume-storage-coherence-1\" deleted persistentvolumeclaim \"persistence-volume-storage-coherence-2\" deleted persistentvolumeclaim \"snapshot-volume-storage-coherence-0\" deleted persistentvolumeclaim \"snapshot-volume-storage-coherence-1\" deleted persistentvolumeclaim \"snapshot-volume-storage-coherence-2\" deleted $ kubectl get pvc -n sample-coherence-ns No resources found. Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Use the Default Persistent Volume Claim"},{"location":"samples/coherence-deployments/persistence/default/#use-the-default-persistent-volume-claim","text":"By default, when you enable Coherence Persistence, the required infrastructure in terms of persistent volumes (PV) and persistent volume claims (PVC) is set up automatically. Also, the persistence-mode is set to active . This allows the Coherence cluster to be restarted and the data to be retained. This sample shows how to enable Persistence with all the defaults under store.persistence in the coherence chart, values.yaml . Refer to this sample for more details about setting other values such as storageClasses . Return to Persistence samples / Return to Coherence Deployments samples / Return to samples","title":"Use the Default Persistent Volume Claim"},{"location":"samples/coherence-deployments/persistence/default/#prerequisites","text":"Ensure that you have installed Coherence Operator by following the instructions here .","title":"Prerequisites"},{"location":"samples/coherence-deployments/persistence/default/#installation-steps","text":"Install the Coherence cluster. bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=persistence-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set store.persistence.enabled=true \\ --set store.snapshot.enabled=true \\ coherence/coherence You may also change the size of the default directories: /persistence default 2Gi - --set store.persistence.size=10Gi /snapshot default 2Gi - --set store.snapshot.size-10Gi Ensure that the pods are running, by using the following command: bash $ kubectl get pods -n sample-coherence-ns When all the three pods are in the Running state, proceed to check the PVC. console NAME READY STATUS RESTARTS AGE coherence-operator-66f9bb7b75-hqk4l 1/1 Running 0 13m storage-coherence-0 1/1 Running 0 3m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 0/1 Running 0 44s Run the following command to check the PVC created: bash $ kubectl get pvc -n sample-coherence-ns console NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistence-volume-storage-coherence-0 Bound pvc-a3fa6ce3-6588-11e9-bad6-025000000001 2Gi RWO hostpath 44m persistence-volume-storage-coherence-1 Bound pvc-d2f732e7-6588-11e9-bad6-025000000001 2Gi RWO hostpath 43m persistence-volume-storage-coherence-2 Bound pvc-fc175ea1-6588-11e9-bad6-025000000001 2Gi RWO hostpath 41m snapshot-volume-storage-coherence-0 Bound pvc-a3fb2172-6588-11e9-bad6-025000000001 2Gi RWO hostpath 44m snapshot-volume-storage-coherence-1 Bound pvc-d2f89ce9-6588-11e9-bad6-025000000001 2Gi RWO hostpath 43m snapshot-volume-storage-coherence-2 Bound pvc-fc13ae4b-6588-11e9-bad6-025000000001 2Gi RWO hostpath 41m Add data to the cluster. a. Connect to the Coherence console using the following command to create a cache: bash $ kubectl exec -it --namespace sample-coherence-ns storage-coherence-0 bash /scripts/startCoherence.sh console b. At the Map (?): prompt, type cache test . This creates a cache in the service, PartitionedCache . c. Use the following to add 50,000 objects of size 1024 bytes, starting at index 0 and using batches of 100: ```bash bulkput 50000 1024 0 100 Mon Apr 15 07:37:03 GMT 2019: adding 500000 items (starting with #0) each 1024 bytes ... Mon Apr 15 07:37:15 GMT 2019: done putting (11578ms, 8878KB/sec, 8637 items/sec) ``` At the prompt, type size and it should show 50000. d. Create a snapshot of the PartitionedCache service which contains the cache test . This is for later use. bash snapshot create test-snapshot Issuing createSnapshot for service PartitionedCache and snapshot test-snapshot Success bash snapshot list Snapshots for service PartitionedCache test-snapshot To exit the console, type bye or CTRL-C. Delete the Coherence cluster by using helm delete : bash $ helm delete storage --purge Before continuing, ensure that the pods are deleted using the following command: bash $ kubectl get pods -n sample-coherence-ns Ensure that there are no pods running with the name coherence-storage-* . Run the following command to ensure that the PVC still exists: bash $ kubectl get pvc -n sample-coherence-ns console NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistence-volume-storage-coherence-0 Bound pvc-a3fa6ce3-6588-11e9-bad6-025000000001 2Gi RWO hostpath 44m persistence-volume-storage-coherence-1 Bound pvc-d2f732e7-6588-11e9-bad6-025000000001 2Gi RWO hostpath 43m persistence-volume-storage-coherence-2 Bound pvc-fc175ea1-6588-11e9-bad6-025000000001 2Gi RWO hostpath 41m snapshot-volume-storage-coherence-0 Bound pvc-a3fb2172-6588-11e9-bad6-025000000001 2Gi RWO hostpath 44m snapshot-volume-storage-coherence-1 Bound pvc-d2f89ce9-6588-11e9-bad6-025000000001 2Gi RWO hostpath 43m snapshot-volume-storage-coherence-2 Bound pvc-fc13ae4b-6588-11e9-bad6-025000000001 2Gi RWO hostpath 41m Reinstall the Coherence cluster with Persistence enabled, using the following command: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=storage-tier-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set store.persistence.enabled=true \\ --set store.snapshot.enabled=true \\ coherence/coherence Wait until all the three pods are running before you continue to the next step. Ensure that the data previously added still exists. a. Connect to the Coherence console using the following: bash $ kubectl exec -it --namespace sample-coherence-ns storage-coherence-0 bash /scripts/startCoherence.sh console b. At the Map (?): prompt, type cache test . This creates/uses a cache in the service, PartitionedCache . c. At the prompt, type size and it should show 50000. This shows that the previous data entered has automatically been recovered as the PVC was honoured. Note: There is currently a bug with default /persistence mount not being created on Docker for Mac environments, and therefore the size may show zero. d. Clear the cache using the clear command and confirm that the cache size is zero. Recover the `test-snapshot` using: bash snapshot recover test-snapshot The size of the cache should now be 50000. To exit the console, type bye .","title":"Installation Steps"},{"location":"samples/coherence-deployments/persistence/default/#uninstall-the-charts","text":"Run the following commands to delete the chart installed in this sample. $ helm delete storage --purge After deleting the pods, run the following command to delete the PVC: $ kubectl get pvc -n sample-coherence-ns | sed 1d | awk '{print $1}' | xargs kubectl delete pvc -n sample-coherence-ns persistentvolumeclaim \"persistence-volume-storage-coherence-0\" deleted persistentvolumeclaim \"persistence-volume-storage-coherence-1\" deleted persistentvolumeclaim \"persistence-volume-storage-coherence-2\" deleted persistentvolumeclaim \"snapshot-volume-storage-coherence-0\" deleted persistentvolumeclaim \"snapshot-volume-storage-coherence-1\" deleted persistentvolumeclaim \"snapshot-volume-storage-coherence-2\" deleted $ kubectl get pvc -n sample-coherence-ns No resources found. Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/coherence-deployments/persistence/pvc/","text":"Use Specific Persistent Volumes This sample shows how to use specific persistent volumes (PV) for Coherence when using active persistence mode. Local storage is the recommended storage type for achieving the best performance for active persistence, but this sample can be modified to use any storage class. Note: We only show how to set store.persistence.* chart values which apply for active persistence (/persistence mount point) only. It is equally applicable to the store.snapshot.* chart values that apply to the /snapshot volume. Return to Persistence samples / Return to Coherence Deployments samples / Return to samples Sample files local-sc.yaml - YAML for creating local storage class mylocal-pv0.yaml - YAML for creating persistent volume mylocal-pv0 mylocal-pv1.yaml - YAML for creating persistent volume mylocal-pv0 mylocal-pv2.yaml - YAML for creating persistent volume mylocal-pv0 Prerequisites Ensure that you have installed Coherence Operator by following the instructions here . Installation Steps Create a local storage class. Using the local-sc.yaml file, create a local storage class called localsc . bash $ kubectl create -f local-sc.yaml console storageclass.storage.k8s.io/localsc created Confirm the creation of the storage class: bash $ kubectl get storageclass console NAME PROVISIONER AGE hostpath (default) docker.io/hostpath 26d localsc kubernetes.io/no-provisioner 31s Create persistent volumes (PV). Note: The PV has the label, coherenceCluster=persistence-cluster , which is used by a nodeSelector to match PV with Coherence clusters. ```bash $ kubectl create -f mylocal-pv0.yaml -n sample-coherence-ns persistentvolume/mylocal-pv0 created $ kubectl create -f mylocal-pv1.yaml -n sample-coherence-ns persistentvolume/mylocal-pv2 created $ kubectl create -f mylocal-pv2.yaml -n sample-coherence-ns persistentvolume/mylocal-pv2 created Confirm the creation of persistent volumes by running the `kubectl` command: bash $ kubectl get pv -n sample-coherence-ns console NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE mylocal-pv0 2Gi RWO Retain Available mylocalsc 1m mylocal-pv1 2Gi RWO Retain Available mylocalsc 14s mylocal-pv2 2Gi RWO Retain Available mylocalsc 9s ``` Note: The number of persistent volumes created must be the same as the Coherence cluster size. For this example we have assumed a cluster size of three. Install the Coherence cluster. Run the following command to install the cluster with persistence enabled, and select the correct storage-class: --set store.persistence.storageClass=mylocalsc - Specifies the storage-class --set store.persistence.selector.matchLabels.coherenceCluster=persistence-cluster - Ensures that the persistent volumes are chosen only where labels match. bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=persistence-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set store.persistence.enabled=true \\ --set store.persistence.storageClass=mylocalsc \\ --set store.persistence.selector.matchLabels.coherenceCluster=persistence-cluster \\ coherence/coherence Check whether the pods are running: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-66f9bb7b75-55msb 1/1 Running 0 23m storage-coherence-0 1/1 Running 0 5m storage-coherence-1 1/1 Running 0 4m storage-coherence-2 1/1 Running 0 3m Ensure that the persistent volumes match the pods: bash $ kubectl get pv -n sample-coherence-ns console NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE mylocal-pv0 2Gi RWO Retain Bound sample-coherence-ns/persistence-volume-storage-coherence-0 mylocalsc 10m mylocal-pv1 2Gi RWO Retain Bound sample-coherence-ns/persistence-volume-storage-coherence-1 mylocalsc 8m mylocal-pv2 2Gi RWO Retain Bound sample-coherence-ns/persistence-volume-storage-coherence-2 mylocalsc 8m Add data to the cluster. a. Connect to the Coherence console using the following command to create a cache: bash $ kubectl exec -it --namespace sample-coherence-ns storage-coherence-0 bash /scripts/startCoherence.sh console b. At the Map (?): prompt, type cache test . This creates a cache in the service, PartitionedCache . c. Use the following to add 100,000 objects of size 1024 bytes, starting at index 0, and using batches of 100. ```bash bulkput 100000 1024 0 100 Mon Apr 15 07:37:03 GMT 2019: adding 100000 items (starting with #0) each 1024 bytes ... Mon Apr 15 07:37:15 GMT 2019: done putting (11578ms, 8878KB/sec, 8637 items/sec) ``` At the prompt, type size and it should show 100000. d. Create a snapshot of the PartitionedCache service which contains the cache test . This is for later use. bash snapshot create test-snapshot Issuing createSnapshot for service PartitionedCache and snapshot empty-service Success bash snapshot list Snapshots for service PartitionedCache test-snapshot To exit the console, type bye . Delete the Coherence cluster by running the following command: bash $ helm delete storage --purge Before continuing, ensure that the pods are deleted. This can be achieved using the following kubectl command: bash $ kubectl get pods -n sample-coherence-ns Ensure that there are no pods running with the name coherence-storage-* Run the following to ensure that the PVC still exists: bash $ kubectl get pv -n sample-coherence-ns NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE mylocal-pv0 2Gi RWO Retain Bound sample-coherence-ns/persistence-volume-storage-coherence-0 mylocalsc 10m mylocal-pv1 2Gi RWO Retain Bound sample-coherence-ns/persistence-volume-storage-coherence-1 mylocalsc 8m mylocal-pv2 2Gi RWO Retain Bound sample-coherence-ns/persistence-volume-storage-coherence-2 mylocalsc 8m Reinstall the Coherence cluster: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=persistence-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set store.persistence.enabled=true \\ --set store.persistence.storageClass=mylocalsc \\ --set store.persistence.selector.matchLabels.coherenceCluster=persistence-cluster \\ coherence/coherence Wait until all three pods are running before you continue to the next step. Ensure that the data added previously still exists. a. Connect to the Coherence console using the following command: bash $ kubectl exec -it --namespace sample-coherence-ns storage-coherence-0 bash /scripts/startCoherence.sh console b. At the Map (?): prompt, type cache test . This create/use a cache in the service PartitionedCache . c. At the prompt, type size and it should show 100000. This shows that the previous data entered has automatically been recovered as the PVC was honoured. d. Clear the cache using clear command and confirm the cache size is zero. Recover the test-snapshot using: bash snapshot recover test-snapshot The size of the cache should now be 100000. To exit the console, type bye . Uninstall the Charts Run the following command to delete the chart and persistent volumes installed in this sample: $ helm delete storage --purge Once the pods are deleted, run the following command to delete the PVC.: $ kubectl delete pvc persistence-volume-storage-coherence-0 persistence-volume-storage-coherence-1 \\ persistence-volume-storage-coherence-2 -n sample-coherence-ns persistentvolumeclaim \"snapshot-volume-storage-coherence-0\" deleted persistentvolumeclaim \"snapshot-volume-storage-coherence-1\" deleted persistentvolumeclaim \"snapshot-volume-storage-coherence-2\" deleted $ kubectl delete pv mylocal-pv0 mylocal-pv1 mylocal-pv2 persistentvolume \"mylocal-pv0\" deleted persistentvolume \"mylocal-pv1\" deleted persistentvolume \"mylocal-pv2\" deleted $ kubectl get pvc -n sample-coherence-ns No resources found. Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Use Specific Persistent Volumes"},{"location":"samples/coherence-deployments/persistence/pvc/#use-specific-persistent-volumes","text":"This sample shows how to use specific persistent volumes (PV) for Coherence when using active persistence mode. Local storage is the recommended storage type for achieving the best performance for active persistence, but this sample can be modified to use any storage class. Note: We only show how to set store.persistence.* chart values which apply for active persistence (/persistence mount point) only. It is equally applicable to the store.snapshot.* chart values that apply to the /snapshot volume. Return to Persistence samples / Return to Coherence Deployments samples / Return to samples","title":"Use Specific Persistent Volumes"},{"location":"samples/coherence-deployments/persistence/pvc/#sample-files","text":"local-sc.yaml - YAML for creating local storage class mylocal-pv0.yaml - YAML for creating persistent volume mylocal-pv0 mylocal-pv1.yaml - YAML for creating persistent volume mylocal-pv0 mylocal-pv2.yaml - YAML for creating persistent volume mylocal-pv0","title":"Sample files"},{"location":"samples/coherence-deployments/persistence/pvc/#prerequisites","text":"Ensure that you have installed Coherence Operator by following the instructions here .","title":"Prerequisites"},{"location":"samples/coherence-deployments/persistence/pvc/#installation-steps","text":"Create a local storage class. Using the local-sc.yaml file, create a local storage class called localsc . bash $ kubectl create -f local-sc.yaml console storageclass.storage.k8s.io/localsc created Confirm the creation of the storage class: bash $ kubectl get storageclass console NAME PROVISIONER AGE hostpath (default) docker.io/hostpath 26d localsc kubernetes.io/no-provisioner 31s Create persistent volumes (PV). Note: The PV has the label, coherenceCluster=persistence-cluster , which is used by a nodeSelector to match PV with Coherence clusters. ```bash $ kubectl create -f mylocal-pv0.yaml -n sample-coherence-ns persistentvolume/mylocal-pv0 created $ kubectl create -f mylocal-pv1.yaml -n sample-coherence-ns persistentvolume/mylocal-pv2 created $ kubectl create -f mylocal-pv2.yaml -n sample-coherence-ns persistentvolume/mylocal-pv2 created Confirm the creation of persistent volumes by running the `kubectl` command: bash $ kubectl get pv -n sample-coherence-ns console NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE mylocal-pv0 2Gi RWO Retain Available mylocalsc 1m mylocal-pv1 2Gi RWO Retain Available mylocalsc 14s mylocal-pv2 2Gi RWO Retain Available mylocalsc 9s ``` Note: The number of persistent volumes created must be the same as the Coherence cluster size. For this example we have assumed a cluster size of three. Install the Coherence cluster. Run the following command to install the cluster with persistence enabled, and select the correct storage-class: --set store.persistence.storageClass=mylocalsc - Specifies the storage-class --set store.persistence.selector.matchLabels.coherenceCluster=persistence-cluster - Ensures that the persistent volumes are chosen only where labels match. bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=persistence-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set store.persistence.enabled=true \\ --set store.persistence.storageClass=mylocalsc \\ --set store.persistence.selector.matchLabels.coherenceCluster=persistence-cluster \\ coherence/coherence Check whether the pods are running: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-66f9bb7b75-55msb 1/1 Running 0 23m storage-coherence-0 1/1 Running 0 5m storage-coherence-1 1/1 Running 0 4m storage-coherence-2 1/1 Running 0 3m Ensure that the persistent volumes match the pods: bash $ kubectl get pv -n sample-coherence-ns console NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE mylocal-pv0 2Gi RWO Retain Bound sample-coherence-ns/persistence-volume-storage-coherence-0 mylocalsc 10m mylocal-pv1 2Gi RWO Retain Bound sample-coherence-ns/persistence-volume-storage-coherence-1 mylocalsc 8m mylocal-pv2 2Gi RWO Retain Bound sample-coherence-ns/persistence-volume-storage-coherence-2 mylocalsc 8m Add data to the cluster. a. Connect to the Coherence console using the following command to create a cache: bash $ kubectl exec -it --namespace sample-coherence-ns storage-coherence-0 bash /scripts/startCoherence.sh console b. At the Map (?): prompt, type cache test . This creates a cache in the service, PartitionedCache . c. Use the following to add 100,000 objects of size 1024 bytes, starting at index 0, and using batches of 100. ```bash bulkput 100000 1024 0 100 Mon Apr 15 07:37:03 GMT 2019: adding 100000 items (starting with #0) each 1024 bytes ... Mon Apr 15 07:37:15 GMT 2019: done putting (11578ms, 8878KB/sec, 8637 items/sec) ``` At the prompt, type size and it should show 100000. d. Create a snapshot of the PartitionedCache service which contains the cache test . This is for later use. bash snapshot create test-snapshot Issuing createSnapshot for service PartitionedCache and snapshot empty-service Success bash snapshot list Snapshots for service PartitionedCache test-snapshot To exit the console, type bye . Delete the Coherence cluster by running the following command: bash $ helm delete storage --purge Before continuing, ensure that the pods are deleted. This can be achieved using the following kubectl command: bash $ kubectl get pods -n sample-coherence-ns Ensure that there are no pods running with the name coherence-storage-* Run the following to ensure that the PVC still exists: bash $ kubectl get pv -n sample-coherence-ns NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE mylocal-pv0 2Gi RWO Retain Bound sample-coherence-ns/persistence-volume-storage-coherence-0 mylocalsc 10m mylocal-pv1 2Gi RWO Retain Bound sample-coherence-ns/persistence-volume-storage-coherence-1 mylocalsc 8m mylocal-pv2 2Gi RWO Retain Bound sample-coherence-ns/persistence-volume-storage-coherence-2 mylocalsc 8m Reinstall the Coherence cluster: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=persistence-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set store.persistence.enabled=true \\ --set store.persistence.storageClass=mylocalsc \\ --set store.persistence.selector.matchLabels.coherenceCluster=persistence-cluster \\ coherence/coherence Wait until all three pods are running before you continue to the next step. Ensure that the data added previously still exists. a. Connect to the Coherence console using the following command: bash $ kubectl exec -it --namespace sample-coherence-ns storage-coherence-0 bash /scripts/startCoherence.sh console b. At the Map (?): prompt, type cache test . This create/use a cache in the service PartitionedCache . c. At the prompt, type size and it should show 100000. This shows that the previous data entered has automatically been recovered as the PVC was honoured. d. Clear the cache using clear command and confirm the cache size is zero. Recover the test-snapshot using: bash snapshot recover test-snapshot The size of the cache should now be 100000. To exit the console, type bye .","title":"Installation Steps"},{"location":"samples/coherence-deployments/persistence/pvc/#uninstall-the-charts","text":"Run the following command to delete the chart and persistent volumes installed in this sample: $ helm delete storage --purge Once the pods are deleted, run the following command to delete the PVC.: $ kubectl delete pvc persistence-volume-storage-coherence-0 persistence-volume-storage-coherence-1 \\ persistence-volume-storage-coherence-2 -n sample-coherence-ns persistentvolumeclaim \"snapshot-volume-storage-coherence-0\" deleted persistentvolumeclaim \"snapshot-volume-storage-coherence-1\" deleted persistentvolumeclaim \"snapshot-volume-storage-coherence-2\" deleted $ kubectl delete pv mylocal-pv0 mylocal-pv1 mylocal-pv2 persistentvolume \"mylocal-pv0\" deleted persistentvolume \"mylocal-pv1\" deleted persistentvolume \"mylocal-pv2\" deleted $ kubectl get pvc -n sample-coherence-ns No resources found. Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/coherence-deployments/sidecar/","text":"Add Application Jars/Config to a Coherence Deployment A common scenario for Coherence deployments is to include specific configuration files, such as cache configuration operational override files, as well as user classes. This can be achieved with coherence-operator by using the sidecar approach. You must ensure that the docker image has the following directories that are copied to the coherence container on startup and are available in the classpath. This sample shows how to create a sidecar image that contains cache configuration, POF configuration and associated portable Person object. A client, SampleClient runs and uses Lambda to modify data on the server side. Return to Coherence Deployments samples / Return to samples Sample files src/main/docker/Dockerfile - Dockerfile for creating a sidecar image from which the configuration and server side JAR will be read at pod startup src/main/resources/conf/storage-cache-config.xml - Cache configuration for storage-enabled tier src/main/resources/conf/storage-pof-config.xml - POF cache configuration for storage-enabled tier src/main/java/com/oracle/coherence/examples/Person.java - Domain class for storing Person src/main/java/com/oracle/coherence/examples/SampleClient.java - Java client to connect via extend and run entry processor. Prerequisites Ensure that you have installed Coherence Operator by following the instructions here . Installation Steps Change to the samples/coherence-deployments/sidecar directory. Ensure that you have your maven build environment set for JDK8, and build the project. bash $ mvn clean install -P docker As a result, the Docker image will be built with the cache configuration files and compiled Java classes with the name in the format sidecar-sample:${version} . For example, bash sidecar-sample:1.0.0-SNAPSHOT`. Note: If you are running against a remote Kubernetes cluster, you must push the above image to your repository accessible to that cluster. You must also prefix the image name in the helm command as shown below. Install the Coherence cluster. bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=sidecar-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=storage-cache-config.xml \\ --set store.pof.config=storage-pof-config.xml \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set userArtifacts.image=sidecar-sample:1.0.0-SNAPSHOT \\ coherence/coherence Once the installation is complete, get the list of pods by using the kubectl command: bash $ kubectl get pods -n sample-coherence-ns All the three storage-coherence pods should be running and ready, as shown in the output: console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 4m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 1m Port-forward Coherence*Extend. Run the following kubectl command to port-forward the default Coherence*Extend port: bash $ kubectl port-forward storage-coherence-0 -n sample-coherence-ns 20000:20000 Run the SampleClient.java class and connect via Coherence*Extend. Run the SampleClient.java class to insert a Person and run a server-side Lambda entry processor to change the name and address to uppercase. The execution of this entry processor shows that the Coherence cluster is aware of the Person object as specified in userArtifacts.image . bash $ mvn exec:java 2019-04-16 13:32:35.835/5.091 Oracle Coherence GE 12.2.1.4.0 <Info> (thread=Proxy:TcpInitiator, member=n/a): Loaded POF configuration from \"file:/Users/timmiddleton/Documents/CoherenceEngineering/github/samples-project/samples/coherence-deployments/sidecar/target/classes/conf/storage-pof-config.xml\" 2019-04-16 13:32:35.859/5.115 Oracle Coherence GE 12.2.1.4.0 <Info> (thread=Proxy:TcpInitiator, member=n/a): Loaded included POF configuration from \"jar:file:/Users/timmiddleton/.m2/repository/com/oracle/coherence/coherence/12.2.1-4-0-73500/coherence-12.2.1-4-0-73500.jar!/coherence-pof-config.xml\" New Person is: Person{Id=1, Name='Tom Jones', Address='123 Hollywood Ave, California, USA'} Person after entry processor is: Person{Id=1, Name='TOM JONES', Address='123 HOLLYWOOD AVE, CALIFORNIA, USA'} ``` ## Uninstall the Charts Run the following command to delete the chart installed in this sample: ```bash $ helm delete storage --purge Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Add Application Jars/Config to a Coherence Deployment"},{"location":"samples/coherence-deployments/sidecar/#add-application-jarsconfig-to-a-coherence-deployment","text":"A common scenario for Coherence deployments is to include specific configuration files, such as cache configuration operational override files, as well as user classes. This can be achieved with coherence-operator by using the sidecar approach. You must ensure that the docker image has the following directories that are copied to the coherence container on startup and are available in the classpath. This sample shows how to create a sidecar image that contains cache configuration, POF configuration and associated portable Person object. A client, SampleClient runs and uses Lambda to modify data on the server side. Return to Coherence Deployments samples / Return to samples","title":"Add Application Jars/Config to a Coherence Deployment"},{"location":"samples/coherence-deployments/sidecar/#sample-files","text":"src/main/docker/Dockerfile - Dockerfile for creating a sidecar image from which the configuration and server side JAR will be read at pod startup src/main/resources/conf/storage-cache-config.xml - Cache configuration for storage-enabled tier src/main/resources/conf/storage-pof-config.xml - POF cache configuration for storage-enabled tier src/main/java/com/oracle/coherence/examples/Person.java - Domain class for storing Person src/main/java/com/oracle/coherence/examples/SampleClient.java - Java client to connect via extend and run entry processor.","title":"Sample files"},{"location":"samples/coherence-deployments/sidecar/#prerequisites","text":"Ensure that you have installed Coherence Operator by following the instructions here .","title":"Prerequisites"},{"location":"samples/coherence-deployments/sidecar/#installation-steps","text":"Change to the samples/coherence-deployments/sidecar directory. Ensure that you have your maven build environment set for JDK8, and build the project. bash $ mvn clean install -P docker As a result, the Docker image will be built with the cache configuration files and compiled Java classes with the name in the format sidecar-sample:${version} . For example, bash sidecar-sample:1.0.0-SNAPSHOT`. Note: If you are running against a remote Kubernetes cluster, you must push the above image to your repository accessible to that cluster. You must also prefix the image name in the helm command as shown below. Install the Coherence cluster. bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=sidecar-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=storage-cache-config.xml \\ --set store.pof.config=storage-pof-config.xml \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set userArtifacts.image=sidecar-sample:1.0.0-SNAPSHOT \\ coherence/coherence Once the installation is complete, get the list of pods by using the kubectl command: bash $ kubectl get pods -n sample-coherence-ns All the three storage-coherence pods should be running and ready, as shown in the output: console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 4m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 1m Port-forward Coherence*Extend. Run the following kubectl command to port-forward the default Coherence*Extend port: bash $ kubectl port-forward storage-coherence-0 -n sample-coherence-ns 20000:20000 Run the SampleClient.java class and connect via Coherence*Extend. Run the SampleClient.java class to insert a Person and run a server-side Lambda entry processor to change the name and address to uppercase. The execution of this entry processor shows that the Coherence cluster is aware of the Person object as specified in userArtifacts.image . bash $ mvn exec:java 2019-04-16 13:32:35.835/5.091 Oracle Coherence GE 12.2.1.4.0 <Info> (thread=Proxy:TcpInitiator, member=n/a): Loaded POF configuration from \"file:/Users/timmiddleton/Documents/CoherenceEngineering/github/samples-project/samples/coherence-deployments/sidecar/target/classes/conf/storage-pof-config.xml\" 2019-04-16 13:32:35.859/5.115 Oracle Coherence GE 12.2.1.4.0 <Info> (thread=Proxy:TcpInitiator, member=n/a): Loaded included POF configuration from \"jar:file:/Users/timmiddleton/.m2/repository/com/oracle/coherence/coherence/12.2.1-4-0-73500/coherence-12.2.1-4-0-73500.jar!/coherence-pof-config.xml\" New Person is: Person{Id=1, Name='Tom Jones', Address='123 Hollywood Ave, California, USA'} Person after entry processor is: Person{Id=1, Name='TOM JONES', Address='123 HOLLYWOOD AVE, CALIFORNIA, USA'} ``` ## Uninstall the Charts Run the following command to delete the chart installed in this sample: ```bash $ helm delete storage --purge Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Installation Steps"},{"location":"samples/coherence-deployments/storage-disabled/","text":"Accessing Coherence via Storage-Disabled Clients Table of Contents Storage-disabled client in cluster via interceptor Storage-disabled client to cluster as separate user image Return to Coherence Deployments samples / Return to samples","title":"Accessing Coherence via Storage-Disabled Clients"},{"location":"samples/coherence-deployments/storage-disabled/#accessing-coherence-via-storage-disabled-clients","text":"","title":"Accessing Coherence via Storage-Disabled Clients"},{"location":"samples/coherence-deployments/storage-disabled/#table-of-contents","text":"Storage-disabled client in cluster via interceptor Storage-disabled client to cluster as separate user image Return to Coherence Deployments samples / Return to samples","title":"Table of Contents"},{"location":"samples/coherence-deployments/storage-disabled/interceptor/","text":"Storage-Disabled Client in Cluster via Interceptor This sample demonstrates deploying 2 tiers, a storage-enabled data tier, and a storage-disabled client tier. The client tier uses an interceptor to start up a Coherence storage-disabled client and perform some processing. The advantage of using this method is that when you run the DefaultCacheServer process, all Prometheus metrics is collected for the storage-disabled member. This is achieved by using two helm install commands, both of which include a sidecar container for the data tier, and a client tier cache configuration as well as the interceptor code. This sample uses only one storage-disabled client. You can change this by setting clusterSize to a value other than one on the client chart install. Return to Storage-Disabled clients samples / Return to Coherence Deployments samples / Return to samples Sample files src/main/docker/Dockerfile - Dockerfile for creating sidecar image from which the configuration and the server side jar will be read at pod startup src/main/resources/conf/storage-cache-config.xml - Cache configuration for storage-enabled tier src/main/resources/conf/interceptor-cache-config.xml - Cache configuration for storage-disabled tier src/main/java/com/oracle/coherence/examples/DemoInterceptor.java - Interceptor that starts our mock client Note that if you want to enable Prometheus or log capture, set the following properties in the helm install command to true . These properties are set to false by default. However, in this sample, these properties have been set to false for completeness. Prometheus: --set prometheusoperator.enabled=true Log capture: --set logCaptureEnabled=true Prerequisites Ensure that you have installed the Coherence Operator by following the instructions here . Installation Steps Change to the samples/coherence-deployments/storage-disabled/interceptor directory. Ensure that you have your maven build environment set for JDK8, and build the project. bash $ mvn clean install -P docker This builds the Docker image with the cache configuration files and compiled Java classes, with the name in the format, interceptor-sample:${version} . For example, bash interceptor-sample:1.0.0-SNAPSHOT Note: If you are running against a remote Kubernetes cluster, you must push the above image to your repository accessible to that cluster. You must also prefix the image name in the helm command as shown below. Install the Coherence cluster. Set the cluster-name to interceptor-cluster for use in later steps. bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=interceptor-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=storage-cache-config.xml \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set userArtifacts.image=interceptor-sample:1.0.0-SNAPSHOT \\ coherence/coherence Once the installation is complete, run the following command to retrieve the list of pods: bash $ kubectl get pods -n sample-coherence-ns All the three storage-coherence pods should be running and ready, as shown in the output: console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 4m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 1m Install the storage-disabled client tier. Set the following properties to ensure that this release connects to the Coherence cluster created by the storage release: --set cluster=interceptor-cluster - Uses the same cluster name --set store.wka=storage-coherence-headless - Ensures that it can contact the cluster --set prometheusoperator.enabled=false - Sets storage to false --set store.cacheConfig=interceptor-cache-config.xml - Uses interceptor cache configuration from the sidecar image bash $ helm install \\ --namespace sample-coherence-ns \\ --set cluster=interceptor-cluster \\ --set clusterSize=1 \\ --set store.wka=storage-coherence-headless \\ --set prometheusoperator.enabled=true \\ --name interceptor-client-tier \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=interceptor-cache-config.xml \\ --set store.storageEnabled=false \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set userArtifacts.image=interceptor-sample:1.0.0-SNAPSHOT \\ coherence/coherence To confirm that the storage-disabled client has joined the cluster, you can look at the logs using the following kubectl commands: bash $ kubectl logs interceptor-client-tier-coherence-0 -n sample-coherence-ns -f This continuously follows the log, and you see messages similar to the following indicating that the storage-disabled client is inserting data: bash 2019-03-20 08:33:43.138/48.454 Oracle Coherence GE 12.2.1.3.0 <Info> (thread=pool-1-thread-1, member=4): Inserted key=40, value=08:33:43 2019-03-20 08:33:44.143/49.459 Oracle Coherence GE 12.2.1.3.0 <Info> (thread=pool-1-thread-1, member=4): Inserted key=41, value=08:33:44 Uninstall the Charts Run the following command to delete both the charts installed in this sample. $ helm delete storage interceptor-client-tier --purge Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Storage-Disabled Client in Cluster via Interceptor"},{"location":"samples/coherence-deployments/storage-disabled/interceptor/#storage-disabled-client-in-cluster-via-interceptor","text":"This sample demonstrates deploying 2 tiers, a storage-enabled data tier, and a storage-disabled client tier. The client tier uses an interceptor to start up a Coherence storage-disabled client and perform some processing. The advantage of using this method is that when you run the DefaultCacheServer process, all Prometheus metrics is collected for the storage-disabled member. This is achieved by using two helm install commands, both of which include a sidecar container for the data tier, and a client tier cache configuration as well as the interceptor code. This sample uses only one storage-disabled client. You can change this by setting clusterSize to a value other than one on the client chart install. Return to Storage-Disabled clients samples / Return to Coherence Deployments samples / Return to samples","title":"Storage-Disabled Client in Cluster via Interceptor"},{"location":"samples/coherence-deployments/storage-disabled/interceptor/#sample-files","text":"src/main/docker/Dockerfile - Dockerfile for creating sidecar image from which the configuration and the server side jar will be read at pod startup src/main/resources/conf/storage-cache-config.xml - Cache configuration for storage-enabled tier src/main/resources/conf/interceptor-cache-config.xml - Cache configuration for storage-disabled tier src/main/java/com/oracle/coherence/examples/DemoInterceptor.java - Interceptor that starts our mock client Note that if you want to enable Prometheus or log capture, set the following properties in the helm install command to true . These properties are set to false by default. However, in this sample, these properties have been set to false for completeness. Prometheus: --set prometheusoperator.enabled=true Log capture: --set logCaptureEnabled=true","title":"Sample files"},{"location":"samples/coherence-deployments/storage-disabled/interceptor/#prerequisites","text":"Ensure that you have installed the Coherence Operator by following the instructions here .","title":"Prerequisites"},{"location":"samples/coherence-deployments/storage-disabled/interceptor/#installation-steps","text":"Change to the samples/coherence-deployments/storage-disabled/interceptor directory. Ensure that you have your maven build environment set for JDK8, and build the project. bash $ mvn clean install -P docker This builds the Docker image with the cache configuration files and compiled Java classes, with the name in the format, interceptor-sample:${version} . For example, bash interceptor-sample:1.0.0-SNAPSHOT Note: If you are running against a remote Kubernetes cluster, you must push the above image to your repository accessible to that cluster. You must also prefix the image name in the helm command as shown below. Install the Coherence cluster. Set the cluster-name to interceptor-cluster for use in later steps. bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=interceptor-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=storage-cache-config.xml \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set userArtifacts.image=interceptor-sample:1.0.0-SNAPSHOT \\ coherence/coherence Once the installation is complete, run the following command to retrieve the list of pods: bash $ kubectl get pods -n sample-coherence-ns All the three storage-coherence pods should be running and ready, as shown in the output: console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 4m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 1m Install the storage-disabled client tier. Set the following properties to ensure that this release connects to the Coherence cluster created by the storage release: --set cluster=interceptor-cluster - Uses the same cluster name --set store.wka=storage-coherence-headless - Ensures that it can contact the cluster --set prometheusoperator.enabled=false - Sets storage to false --set store.cacheConfig=interceptor-cache-config.xml - Uses interceptor cache configuration from the sidecar image bash $ helm install \\ --namespace sample-coherence-ns \\ --set cluster=interceptor-cluster \\ --set clusterSize=1 \\ --set store.wka=storage-coherence-headless \\ --set prometheusoperator.enabled=true \\ --name interceptor-client-tier \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=interceptor-cache-config.xml \\ --set store.storageEnabled=false \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set userArtifacts.image=interceptor-sample:1.0.0-SNAPSHOT \\ coherence/coherence To confirm that the storage-disabled client has joined the cluster, you can look at the logs using the following kubectl commands: bash $ kubectl logs interceptor-client-tier-coherence-0 -n sample-coherence-ns -f This continuously follows the log, and you see messages similar to the following indicating that the storage-disabled client is inserting data: bash 2019-03-20 08:33:43.138/48.454 Oracle Coherence GE 12.2.1.3.0 <Info> (thread=pool-1-thread-1, member=4): Inserted key=40, value=08:33:43 2019-03-20 08:33:44.143/49.459 Oracle Coherence GE 12.2.1.3.0 <Info> (thread=pool-1-thread-1, member=4): Inserted key=41, value=08:33:44","title":"Installation Steps"},{"location":"samples/coherence-deployments/storage-disabled/interceptor/#uninstall-the-charts","text":"Run the following command to delete both the charts installed in this sample. $ helm delete storage interceptor-client-tier --purge Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/coherence-deployments/storage-disabled/other/","text":"Storage-Disabled Client in Cluster as Separate User Image To ensure that a custom made container and Helm Chart can be installed and joins an existing Coherence cluster, perform the following steps for the custom container: Ensure that the correct version of Coherence is included in the package. Set Cluster Name to match the name of the Coherence cluster you are running. Set Well Known Address to match the headless service name of the Coherence cluster you are running. Disable local storage using the setting, -Dcoherence.distributed.localstorage=false This sample shows how a Helidon web application exposes a /query endpoint, allowing CohQL commands to be passed and executed against a Coherence cluster. Return to Storage-Disabled clients samples / Return to Coherence Deployments samples / Return to samples Sample files src/assembly/helm-assembly.xml - Assembly file for Helm src/main/java/com/oracle/coherence/examples/Main.java - Entry point for Helidon web application src/main/helm/ - Helm chart files Note : If you want to enable Prometheus or log capture, set the following properties in the helm install command to true . These properties are set to false by default. However, in this sample, these properties have been set to false for completeness. Prometheus: --set prometheusoperator.enabled=true Log capture: --set logCaptureEnabled=true Prerequisites Ensure that you have installed the Coherence Operator by following the instructions here . Installation Steps Change to the samples/coherence-deployments/storage-disabled/other directory. Ensure that you have your maven build environment set for JDK8, and build the project. bash $ mvn clean install -P docker This builds the Docker image with the cache configuration files, with the name in the format, helidon-sample:${version} . This image is subsequently used by the chart. Note: If you are running against a remote Kubernetes cluster, you must push the above image to your repository accessible to that cluster. You must also prefix the image name in the helm command, as shown below. Install the Coherence cluster. bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=helidon-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ coherence/coherence After the installation is complete, get the list of pods by running the following command: bash $ kubectl get pods -n sample-coherence-ns All the three storage-coherence pods should be running and ready, as shown in the output: console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 4m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 2m Install the Helidon web application. When you install the Helidon web application, ensure that you set the following properties to allow the Helidon application to join the Coherence cluster: --set cluster=helidon-cluster - Uses the same cluster name --set store.wka=storage-coherence-headless - Ensures that it can contact the cluster bash $ helm install \\ --namespace sample-coherence-ns \\ --set wka=storage-coherence-headless \\ --set clusterName=helidon-cluster \\ --name helidon-web-app \\ target/webserver-1.0.0-SNAPSHOT-helm/webserver/ As per the instructions output above, port forward port 8080. bash $ export POD_NAME=$(kubectl get pods --namespace sample-coherence-ns -l \"app=webserver,release=helidon-web-app\" -o jsonpath=\"{.items[0].metadata.name}\") $ kubectl --namespace sample-coherence-ns port-forward $POD_NAME 8080:8080 Forwarding from 127.0.0.1:8080 -> 8080 Forwarding from [::1]:8080 -> 8080 Run CohQL commands. Use the various CohQL commands to create and mutate data in the Coherence cluster. ```bash $ curl -i -w '\\n' -X PUT http://127.0.0.1:8080/query -d '{\"query\":\"create cache foo\"}' HTTP/1.1 200 OK Date: Thu, 18 Apr 2019 06:48:15 GMT transfer-encoding: chunked connection: keep-alive $ curl -i -w '\\n' -X PUT http://127.0.0.1:8080/query -d '{\"query\":\"insert into foo key(\\\"foo\\\") value(\\\"bar\\\")\"}' HTTP/1.1 200 OK Date: Thu, 18 Apr 2019 06:48:40 GMT transfer-encoding: chunked connection: keep-alive $ curl -i -w '\\n' -X PUT http://127.0.0.1:8080/query -d '{\"query\":\"select key(),value() from foo\"}' HTTP/1.1 200 OK Content-Type: application/json Date: Thu, 18 Apr 2019 06:49:15 GMT transfer-encoding: chunked connection: keep-alive {\"result\":\"{foo=[foo, bar]}\"} ``` Uninstall the Charts Run the following command to delete both the charts installed in this sample. $ helm delete storage helidon-web-app --purge Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Storage-Disabled Client in Cluster as Separate User Image"},{"location":"samples/coherence-deployments/storage-disabled/other/#storage-disabled-client-in-cluster-as-separate-user-image","text":"To ensure that a custom made container and Helm Chart can be installed and joins an existing Coherence cluster, perform the following steps for the custom container: Ensure that the correct version of Coherence is included in the package. Set Cluster Name to match the name of the Coherence cluster you are running. Set Well Known Address to match the headless service name of the Coherence cluster you are running. Disable local storage using the setting, -Dcoherence.distributed.localstorage=false This sample shows how a Helidon web application exposes a /query endpoint, allowing CohQL commands to be passed and executed against a Coherence cluster. Return to Storage-Disabled clients samples / Return to Coherence Deployments samples / Return to samples","title":"Storage-Disabled Client in Cluster as Separate User Image"},{"location":"samples/coherence-deployments/storage-disabled/other/#sample-files","text":"src/assembly/helm-assembly.xml - Assembly file for Helm src/main/java/com/oracle/coherence/examples/Main.java - Entry point for Helidon web application src/main/helm/ - Helm chart files Note : If you want to enable Prometheus or log capture, set the following properties in the helm install command to true . These properties are set to false by default. However, in this sample, these properties have been set to false for completeness. Prometheus: --set prometheusoperator.enabled=true Log capture: --set logCaptureEnabled=true","title":"Sample files"},{"location":"samples/coherence-deployments/storage-disabled/other/#prerequisites","text":"Ensure that you have installed the Coherence Operator by following the instructions here .","title":"Prerequisites"},{"location":"samples/coherence-deployments/storage-disabled/other/#installation-steps","text":"Change to the samples/coherence-deployments/storage-disabled/other directory. Ensure that you have your maven build environment set for JDK8, and build the project. bash $ mvn clean install -P docker This builds the Docker image with the cache configuration files, with the name in the format, helidon-sample:${version} . This image is subsequently used by the chart. Note: If you are running against a remote Kubernetes cluster, you must push the above image to your repository accessible to that cluster. You must also prefix the image name in the helm command, as shown below. Install the Coherence cluster. bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=helidon-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ coherence/coherence After the installation is complete, get the list of pods by running the following command: bash $ kubectl get pods -n sample-coherence-ns All the three storage-coherence pods should be running and ready, as shown in the output: console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 4m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 2m Install the Helidon web application. When you install the Helidon web application, ensure that you set the following properties to allow the Helidon application to join the Coherence cluster: --set cluster=helidon-cluster - Uses the same cluster name --set store.wka=storage-coherence-headless - Ensures that it can contact the cluster bash $ helm install \\ --namespace sample-coherence-ns \\ --set wka=storage-coherence-headless \\ --set clusterName=helidon-cluster \\ --name helidon-web-app \\ target/webserver-1.0.0-SNAPSHOT-helm/webserver/ As per the instructions output above, port forward port 8080. bash $ export POD_NAME=$(kubectl get pods --namespace sample-coherence-ns -l \"app=webserver,release=helidon-web-app\" -o jsonpath=\"{.items[0].metadata.name}\") $ kubectl --namespace sample-coherence-ns port-forward $POD_NAME 8080:8080 Forwarding from 127.0.0.1:8080 -> 8080 Forwarding from [::1]:8080 -> 8080 Run CohQL commands. Use the various CohQL commands to create and mutate data in the Coherence cluster. ```bash $ curl -i -w '\\n' -X PUT http://127.0.0.1:8080/query -d '{\"query\":\"create cache foo\"}' HTTP/1.1 200 OK Date: Thu, 18 Apr 2019 06:48:15 GMT transfer-encoding: chunked connection: keep-alive $ curl -i -w '\\n' -X PUT http://127.0.0.1:8080/query -d '{\"query\":\"insert into foo key(\\\"foo\\\") value(\\\"bar\\\")\"}' HTTP/1.1 200 OK Date: Thu, 18 Apr 2019 06:48:40 GMT transfer-encoding: chunked connection: keep-alive $ curl -i -w '\\n' -X PUT http://127.0.0.1:8080/query -d '{\"query\":\"select key(),value() from foo\"}' HTTP/1.1 200 OK Content-Type: application/json Date: Thu, 18 Apr 2019 06:49:15 GMT transfer-encoding: chunked connection: keep-alive {\"result\":\"{foo=[foo, bar]}\"} ```","title":"Installation Steps"},{"location":"samples/coherence-deployments/storage-disabled/other/#uninstall-the-charts","text":"Run the following command to delete both the charts installed in this sample. $ helm delete storage helidon-web-app --purge Before starting another sample, ensure that all pods are removed from the previous sample. To remove coherence-operator , use the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/common/","text":"Common Scripts Overview You will find a number of scripts in the common directory to assist with running these samples. port-forward-grafana.sh - Port forward to Grafana pod port-forward-kibana.sh - Port forward to Kibana pod port-forward-prometheus.sh - Port forward to Prometheus pod Return to samples","title":"Common Scripts"},{"location":"samples/common/#common-scripts","text":"","title":"Common Scripts"},{"location":"samples/common/#overview","text":"You will find a number of scripts in the common directory to assist with running these samples. port-forward-grafana.sh - Port forward to Grafana pod port-forward-kibana.sh - Port forward to Kibana pod port-forward-prometheus.sh - Port forward to Prometheus pod Return to samples","title":"Overview"},{"location":"samples/management/","text":"Management Table of Contents Using Management over REST (12.2.1.4 only) Access management over REST Access management over REST using JVisualVM plugin Enable SSL with management over REST Modify Writable MBeans Access JMX in the Coherence Cluster via JConsole and JVisualVM Access Coherence Console and CohQL on a cluster node Diagnostic Tools Produce and extract a heap dump Produce and extract a Java Flight Recorder (JFR) file Manage and use the Reporter Provide arguments to the JVM that runs Coherence Back to samples","title":"Management"},{"location":"samples/management/#management","text":"","title":"Management"},{"location":"samples/management/#table-of-contents","text":"Using Management over REST (12.2.1.4 only) Access management over REST Access management over REST using JVisualVM plugin Enable SSL with management over REST Modify Writable MBeans Access JMX in the Coherence Cluster via JConsole and JVisualVM Access Coherence Console and CohQL on a cluster node Diagnostic Tools Produce and extract a heap dump Produce and extract a Java Flight Recorder (JFR) file Manage and use the Reporter Provide arguments to the JVM that runs Coherence Back to samples","title":"Table of Contents"},{"location":"samples/management/console-cohql/","text":"Access Coherence Console and CohQL on a Cluster Node The Coherence Console and CohQL (Coherence Query Language) are developer tools used for interacting with a Coherence cluster. This samples shows how to use kubectl exec to connect to any of the pods and start Coherence console or CohQL as a storage-disabled client. Return to Management samples / Return to samples Prerequisites Ensure you have already installed the Coherence Operator using the instructions here . Installation Steps Install the Coherence cluster Install the cluster with Persistence and Snapshot enabled: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=coherence-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ coherence/coherence Ensure the pods are running: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-66f9bb7b75-hqk4l 1/1 Running 0 13m storage-coherence-0 1/1 Running 0 3m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 44s Add data to the cluster using the Coherence console Connect to the console and create a cache: bash $ kubectl exec -it --namespace sample-coherence-ns storage-coherence-0 bash /scripts/startCoherence.sh console At the Map (?): prompt, type cache test . This creates a cache in the service PartitionedCache . Use the following command to add 50000 objects of size 1024 bytes, starting at index 0 and using batches of 100. bash bulkput 50000 1024 0 100 Wed Apr 24 01:17:44 GMT 2019: adding 50000 items (starting with #0) each 1024 bytes ... Wed Apr 24 01:18:11 GMT 2019: done putting (26802ms, 1917KB/sec, 1865 items/sec) At the prompt, type size and it will show 50000. The help command shows all the available command options. Then type bye to exit the console. Add data to the cluster through CohQL Connect to the CohQL client using the following command: bash $ kubectl exec -it --namespace sample-coherence-ns storage-coherence-0 bash /scripts/startCoherence.sh queryplus Use the following command at the CohQL prompt to view and manipulate data: ```bash select count() from 'test'; ... Cluster and service join messages here Results 50000 select value() from 'test' where key() = 0 Results [B@63a5e46c insert into 'test' key(1) value('value 1'); Results [B@7a34b7b8 select value() from 'test' where key() = 1 Results \"value 1\" ``` At the CohQL prompt, type commands to view all the commands that are available, while help shows detailed information. Type quit at the prompt to exit CohQL. Refer to the Coherence Documentation for more information about CohQL. Uninstall the Charts Use the following command to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous sample. If you want to remove the coherence-operator , then include it in the helm delete command.","title":"Access Coherence Console and CohQL on a Cluster Node"},{"location":"samples/management/console-cohql/#access-coherence-console-and-cohql-on-a-cluster-node","text":"The Coherence Console and CohQL (Coherence Query Language) are developer tools used for interacting with a Coherence cluster. This samples shows how to use kubectl exec to connect to any of the pods and start Coherence console or CohQL as a storage-disabled client. Return to Management samples / Return to samples","title":"Access Coherence Console and CohQL on a Cluster Node"},{"location":"samples/management/console-cohql/#prerequisites","text":"Ensure you have already installed the Coherence Operator using the instructions here .","title":"Prerequisites"},{"location":"samples/management/console-cohql/#installation-steps","text":"Install the Coherence cluster Install the cluster with Persistence and Snapshot enabled: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=coherence-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ coherence/coherence Ensure the pods are running: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-66f9bb7b75-hqk4l 1/1 Running 0 13m storage-coherence-0 1/1 Running 0 3m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 44s Add data to the cluster using the Coherence console Connect to the console and create a cache: bash $ kubectl exec -it --namespace sample-coherence-ns storage-coherence-0 bash /scripts/startCoherence.sh console At the Map (?): prompt, type cache test . This creates a cache in the service PartitionedCache . Use the following command to add 50000 objects of size 1024 bytes, starting at index 0 and using batches of 100. bash bulkput 50000 1024 0 100 Wed Apr 24 01:17:44 GMT 2019: adding 50000 items (starting with #0) each 1024 bytes ... Wed Apr 24 01:18:11 GMT 2019: done putting (26802ms, 1917KB/sec, 1865 items/sec) At the prompt, type size and it will show 50000. The help command shows all the available command options. Then type bye to exit the console. Add data to the cluster through CohQL Connect to the CohQL client using the following command: bash $ kubectl exec -it --namespace sample-coherence-ns storage-coherence-0 bash /scripts/startCoherence.sh queryplus Use the following command at the CohQL prompt to view and manipulate data: ```bash select count() from 'test'; ... Cluster and service join messages here Results 50000 select value() from 'test' where key() = 0 Results [B@63a5e46c insert into 'test' key(1) value('value 1'); Results [B@7a34b7b8 select value() from 'test' where key() = 1 Results \"value 1\" ``` At the CohQL prompt, type commands to view all the commands that are available, while help shows detailed information. Type quit at the prompt to exit CohQL. Refer to the Coherence Documentation for more information about CohQL.","title":"Installation Steps"},{"location":"samples/management/console-cohql/#uninstall-the-charts","text":"Use the following command to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous sample. If you want to remove the coherence-operator , then include it in the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/management/diagnostics/","text":"Diagnostics Tools Table of Contents Produce and extract a heap dump Produce and extract a Java Flight Recorder (JFR) file Return to Management samples / Return to samples Overview","title":"Diagnostics Tools"},{"location":"samples/management/diagnostics/#diagnostics-tools","text":"","title":"Diagnostics Tools"},{"location":"samples/management/diagnostics/#table-of-contents","text":"Produce and extract a heap dump Produce and extract a Java Flight Recorder (JFR) file Return to Management samples / Return to samples","title":"Table of Contents"},{"location":"samples/management/diagnostics/#overview","text":"","title":"Overview"},{"location":"samples/management/diagnostics/heap-dump/","text":"Produce and Extract Heap Dump Some of the debugging techniques described in Debugging in Coherence require the creation of files, such as log files and JVM heap dumps, for analysis. You can also create and extract these files in the Coherence Operator. This sample shows how to collect a .hprof file for a heap dump. A single-command technique is also included at the end of this sample. Return to Diagnostics Tools / Return to Management samples / Return to samples Prerequisites Ensure you have installed the Coherence Operator using the instructions here . Installation Steps Install the Coherence cluster Install a Coherence cluster if you don't have one running: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=coherence-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ coherence/coherence Ensure the pods are running: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-66f9bb7b75-hqk4l 1/1 Running 0 13m storage-coherence-0 1/1 Running 0 3m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 44s Open a terminal window in one of the storage nodes: bash $ kubectl exec -it storage-coherence-0 -n sample-coherence-ns -- bash Obtain the PID of the Coherence process. Generally, the PID is 1 . You can also use jps to get the actual PID. bash # /usr/java/default/bin/jps 1 DefaultCacheServer 4230 Jps Use the jcmd command to extract the heap dump: bash $ rm /tmp/heap.hprof $ /usr/java/default/bin/jcmd 1 GC.heap_dump /tmp/heap.hprof $ exit Copy the heap dump to local machine: bash $ kubectl cp sample-coherence-ns/storage-coherence-0:/tmp/heap.hprof heap.hprof Deepending upon whether the Kubernetes cluster is local or remote, this might take some time. Single command usage Assuming that the Coherence PID is 1 , you can use this repeatable single-command technique to extract the heap dump: bash $ (kubectl exec storage-coherence-0 -n sample-coherence-ns -- /bin/bash -c \"rm -f /tmp/heap.hprof; /usr/java/default/bin/jcmd 1 GC.heap_dump /tmp/heap.hprof; cat /tmp/heap.hprof > /dev/stderr\" ) 2> heap.hprof Note that we redirect the heap dump output to stderr to prevent the unsuppressable. bash 1: Heap dump file created Uninstall the Charts Use the following command to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous sample. If you want to remove the coherence-operator , then include it in the helm delete command.","title":"Produce and Extract Heap Dump"},{"location":"samples/management/diagnostics/heap-dump/#produce-and-extract-heap-dump","text":"Some of the debugging techniques described in Debugging in Coherence require the creation of files, such as log files and JVM heap dumps, for analysis. You can also create and extract these files in the Coherence Operator. This sample shows how to collect a .hprof file for a heap dump. A single-command technique is also included at the end of this sample. Return to Diagnostics Tools / Return to Management samples / Return to samples","title":"Produce and Extract Heap Dump"},{"location":"samples/management/diagnostics/heap-dump/#prerequisites","text":"Ensure you have installed the Coherence Operator using the instructions here .","title":"Prerequisites"},{"location":"samples/management/diagnostics/heap-dump/#installation-steps","text":"Install the Coherence cluster Install a Coherence cluster if you don't have one running: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=coherence-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ coherence/coherence Ensure the pods are running: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-66f9bb7b75-hqk4l 1/1 Running 0 13m storage-coherence-0 1/1 Running 0 3m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 44s Open a terminal window in one of the storage nodes: bash $ kubectl exec -it storage-coherence-0 -n sample-coherence-ns -- bash Obtain the PID of the Coherence process. Generally, the PID is 1 . You can also use jps to get the actual PID. bash # /usr/java/default/bin/jps 1 DefaultCacheServer 4230 Jps Use the jcmd command to extract the heap dump: bash $ rm /tmp/heap.hprof $ /usr/java/default/bin/jcmd 1 GC.heap_dump /tmp/heap.hprof $ exit Copy the heap dump to local machine: bash $ kubectl cp sample-coherence-ns/storage-coherence-0:/tmp/heap.hprof heap.hprof Deepending upon whether the Kubernetes cluster is local or remote, this might take some time. Single command usage Assuming that the Coherence PID is 1 , you can use this repeatable single-command technique to extract the heap dump: bash $ (kubectl exec storage-coherence-0 -n sample-coherence-ns -- /bin/bash -c \"rm -f /tmp/heap.hprof; /usr/java/default/bin/jcmd 1 GC.heap_dump /tmp/heap.hprof; cat /tmp/heap.hprof > /dev/stderr\" ) 2> heap.hprof Note that we redirect the heap dump output to stderr to prevent the unsuppressable. bash 1: Heap dump file created","title":"Installation Steps"},{"location":"samples/management/diagnostics/heap-dump/#uninstall-the-charts","text":"Use the following command to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous sample. If you want to remove the coherence-operator , then include it in the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/management/diagnostics/jfr/","text":"Produce and Extract a Java Flight Recorder (JFR) File Java Flight Recorder (JFR) is a tool for collecting diagnostic and profiling data about a running Java application. It is integrated into the Java Virtual Machine (JVM) and does cause any performance overhead and it can be used in heavily loaded production environments. By default, when the Coherence chart is installed, the Management over REST endpoint is exposed at port 30000 on each of the pods. This sample shows how you can create and manage Flight Recordings using the Management over REST endpoint, which is exposed via the following endpoint: http://host:30000/management/coherence/cluster/diagnostic-cmd The Swagger document is available at: http://host:30000/management/coherence/cluster/metadata-catalog The endpoint makes use of jcmd which is described in the Oracle documentation . Note : Use of Management over REST is available only when using the operator with Oracle Coherence 12.2.1.4.0. Return to Diagnostics Tools / Return to Management samples / Return to samples Prerequisites Ensure you have installed the Coherence Operator using the instructions here . Installation Steps Install the Coherence cluster Use the following command to install the cluster: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=coherence-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set coherence.image=your-12.2.1.4.0-Coherence-image \\ coherence/coherence Note: If your version of the Coherence Operator does not default to using Coherence 12.2.1.4.0, then you need to replace your-12.2.1.4.0-Coherence-image with an appropriate 12.2.1.4.0 image. Ensure the pods are running: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 4m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 2m Port forward the Management over REST port: bash $ kubectl port-forward storage-coherence-0 -n sample-coherence-ns 30000:30000 console Forwarding from [::1]:30000 -> 30000 Forwarding from 127.0.0.1:30000 -> 30000 Start a Flight Recording Use curl to start a recording with a name test1 on member 1: bash $ curl -X POST -H 'Content-type: application/json' -v \\ \"http://127.0.0.1:30000/management/coherence/cluster/members/1/diagnostic-cmd/jfrStart?options=name%3Dtest1\" It returns a HTTP 200 OK status. Check the status of the recording: bash $ curl -X POST -H 'Content-type: application/json' -v \\ \"http://127.0.0.1:30000/management/coherence/cluster/members/1/diagnostic-cmd/jfrCheck?options=name%3Dtest1\" The following status line is displayed in the output: json \"status\":\"Recording: recording=1 name=\\\"test1\\\" (running)\\n\"} Dump the Flight Recording to a file Use curl to dump the currently running recording with the name test1 on member 1 to a file called /tmp/test1.jfr : bash $ curl -X POST -H 'Content-type: application/json' -v \\ \"http://127.0.0.1:30000/management/coherence/cluster/members/1/diagnostic-cmd/jfrDump?options=name%3Dtest1,filename%3D/tmp/test1.jfr\" The following status line is displayed in the output: json \"status\":\"Dumped recording \\\"test1\\\", 717.0 kB written to:\\n\\n/tmp/test1.jfr\\n\"} Copy the JFR recording to a local machine: bash $ kubectl cp sample-coherence-ns/storage-coherence-0:/tmp/test1.jfr test1.jfr Depending upon whether your Kubernetes cluster is local or remote, this might take some time. Stop the Flight Recording Use curl to stop the currently running recording with the name test1 on member 1: bash $ curl -X POST -H 'Content-type: application/json' -v \\ \"http://127.0.0.1:30000/management/coherence/cluster/members/1/diagnostic-cmd/jfrStop?options=name%3Dtest1\" The following status line is displayed in the output: json \"status\":\"Stopped recording \\\"test1\\\".\\n\"} Note: The commands in this procedure can be run on all nodes by leaving out /members/1 in the path. For example, http://127.0.0.1:30000/management/coherence/cluster/diagnostic-cmd/jfrStart Uninstall the Charts Use the following command to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous samples. If you want to remove the coherence-operator , then include it in the helm delete command.","title":"Produce and Extract a Java Flight Recorder (JFR) File"},{"location":"samples/management/diagnostics/jfr/#produce-and-extract-a-java-flight-recorder-jfr-file","text":"Java Flight Recorder (JFR) is a tool for collecting diagnostic and profiling data about a running Java application. It is integrated into the Java Virtual Machine (JVM) and does cause any performance overhead and it can be used in heavily loaded production environments. By default, when the Coherence chart is installed, the Management over REST endpoint is exposed at port 30000 on each of the pods. This sample shows how you can create and manage Flight Recordings using the Management over REST endpoint, which is exposed via the following endpoint: http://host:30000/management/coherence/cluster/diagnostic-cmd The Swagger document is available at: http://host:30000/management/coherence/cluster/metadata-catalog The endpoint makes use of jcmd which is described in the Oracle documentation . Note : Use of Management over REST is available only when using the operator with Oracle Coherence 12.2.1.4.0. Return to Diagnostics Tools / Return to Management samples / Return to samples","title":"Produce and Extract a Java Flight Recorder (JFR) File"},{"location":"samples/management/diagnostics/jfr/#prerequisites","text":"Ensure you have installed the Coherence Operator using the instructions here .","title":"Prerequisites"},{"location":"samples/management/diagnostics/jfr/#installation-steps","text":"Install the Coherence cluster Use the following command to install the cluster: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=coherence-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set coherence.image=your-12.2.1.4.0-Coherence-image \\ coherence/coherence Note: If your version of the Coherence Operator does not default to using Coherence 12.2.1.4.0, then you need to replace your-12.2.1.4.0-Coherence-image with an appropriate 12.2.1.4.0 image. Ensure the pods are running: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 4m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 2m Port forward the Management over REST port: bash $ kubectl port-forward storage-coherence-0 -n sample-coherence-ns 30000:30000 console Forwarding from [::1]:30000 -> 30000 Forwarding from 127.0.0.1:30000 -> 30000 Start a Flight Recording Use curl to start a recording with a name test1 on member 1: bash $ curl -X POST -H 'Content-type: application/json' -v \\ \"http://127.0.0.1:30000/management/coherence/cluster/members/1/diagnostic-cmd/jfrStart?options=name%3Dtest1\" It returns a HTTP 200 OK status. Check the status of the recording: bash $ curl -X POST -H 'Content-type: application/json' -v \\ \"http://127.0.0.1:30000/management/coherence/cluster/members/1/diagnostic-cmd/jfrCheck?options=name%3Dtest1\" The following status line is displayed in the output: json \"status\":\"Recording: recording=1 name=\\\"test1\\\" (running)\\n\"} Dump the Flight Recording to a file Use curl to dump the currently running recording with the name test1 on member 1 to a file called /tmp/test1.jfr : bash $ curl -X POST -H 'Content-type: application/json' -v \\ \"http://127.0.0.1:30000/management/coherence/cluster/members/1/diagnostic-cmd/jfrDump?options=name%3Dtest1,filename%3D/tmp/test1.jfr\" The following status line is displayed in the output: json \"status\":\"Dumped recording \\\"test1\\\", 717.0 kB written to:\\n\\n/tmp/test1.jfr\\n\"} Copy the JFR recording to a local machine: bash $ kubectl cp sample-coherence-ns/storage-coherence-0:/tmp/test1.jfr test1.jfr Depending upon whether your Kubernetes cluster is local or remote, this might take some time. Stop the Flight Recording Use curl to stop the currently running recording with the name test1 on member 1: bash $ curl -X POST -H 'Content-type: application/json' -v \\ \"http://127.0.0.1:30000/management/coherence/cluster/members/1/diagnostic-cmd/jfrStop?options=name%3Dtest1\" The following status line is displayed in the output: json \"status\":\"Stopped recording \\\"test1\\\".\\n\"} Note: The commands in this procedure can be run on all nodes by leaving out /members/1 in the path. For example, http://127.0.0.1:30000/management/coherence/cluster/diagnostic-cmd/jfrStart","title":"Installation Steps"},{"location":"samples/management/diagnostics/jfr/#uninstall-the-charts","text":"Use the following command to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous samples. If you want to remove the coherence-operator , then include it in the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/management/jmx/","text":"Access JMX in the Coherence Cluster Using JConsole and VisualVM Java Management Extensions (JMX) is the standard way to inspect and manage enterprise Java applications. Applications that are exposed via JMX do not incur runtime performance penalty for doing so unless a tool is actively connected to the JMX connection, and only then in certain cases. The Java Tutorials provide introduction to JMX . The Coherence documentation describes how to use JMX with Coherence . All the capabilities of JMX with Coherence are also available with the operator. This sample shows how to connect to a Coherence JMX MBean Server when using the Coherence Operator. The option --set store.jmx.enabled=true is used which creates an MBean Server pod from which you can connect to the operator. By default, there is one replica for the MBean Server. You can create more MBean server pods by setting the store.jmx.replicas value. For example, --set store.jmx.replicas=2 . See here for information about connecting to Management over REST endpoint. Return to Management samples / Return to samples Prerequisites Install the Coherence Operator Ensure you have installed the Coherence Operator using the instructions here . Download the JMXMP connector JAR The JMX endpoint does not use RMI, instead it uses JMXMP. This requires an additional JAR on the classpath of the Java JMX client (VisualVM and JConsole). You can use curl to download the required JAR. bash curl http://central.maven.org/maven2/org/glassfish/external/opendmk_jmxremote_optional_jar/1.0-b01-ea/opendmk_jmxremote_optional_jar-1.0-b01-ea.jar \\ -o opendmk_jmxremote_optional_jar-1.0-b01-ea.jar This also can be downloaded as a Maven dependency if you are connecting through a Maven project. xml <dependency> <groupId>org.glassfish.external</groupId> <artifactId>opendmk_jmxremote_optional_jar</artifactId> <version>1.0-b01-ea</version> </dependency> Installation Steps Install the Coherence cluster Install the cluster with one MBean server pod: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=coherence-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set store.jmx.enabled=true \\ --set store.jmx.replicas=1 \\ coherence/coherence Note : There are many other store.jmx.* options which control other aspects of the MBean Server node. Refer to the Coherence Operator values.yaml file for more information. After the chart is installed, instructions are displayed to help you utilize this feature. You can follow these instructions or use the commands. Check whether the MBean server pod is running: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-5899f6444b-tckm4 1/1 Running 0 1h storage-coherence-0 1/1 Running 0 29m storage-coherence-1 1/1 Running 0 28m storage-coherence-2 1/1 Running 0 27m storage-coherence-jmx-54f5d779d-svh29 1/1 Running 0 29m A pod prefixed with storage-coherence-jmx is displayed in the output. Port forward the MBean server pod: ```bash $ export POD_NAME=$(kubectl get pods --namespace sample-coherence-ns -l \"app=coherence,release=storage,component=coherenceJMXPod\" -o jsonpath=\"{.items[0].metadata.name}\") $ kubectl --namespace sample-coherence-ns port-forward $POD_NAME 9099:9099 ``` Access the JMX endpoint at the URL service:jmx:jmxmp://127.0.0.1:9099 . (Optional) Add data to a cache Connect to the Coherence console and create a cache: Note : If you do not carry out this step, then you will not see any CacheMBeans as described below. bash $ kubectl exec -it --namespace sample-coherence-ns storage-coherence-0 bash /scripts/startCoherence.sh console At the Map (?): prompt, type cache test . This will create a cache in the service PartitionedCache . Use the following to add 100,000 objects of size 1024 bytes, starting at index 0 and using batches of 100. ```bash bulkput 100000 1024 0 100 Mon Apr 15 07:37:03 GMT 2019: adding 100000 items (starting with #0) each 1024 bytes ... Mon Apr 15 07:37:15 GMT 2019: done putting (11578ms, 8878KB/sec, 8637 items/sec) ``` At the prompt, type size and it should show 100000. Then, type bye to exit the console . Access through JConsole Ensure you run JConsole with the JMXMP connector on the classpath: bash $ jconsole -J-Djava.class.path=\"$JAVA_HOME/lib/jconsole.jar:$JAVA_HOME/lib/tools.jar:opendmk_jmxremote_optional_jar-1.0-b01-ea.jar\" service:jmx:jmxmp://127.0.0.1:9099 In the console UI, select the MBeans tab and then Coherence Cluster attributes. You should see the Coherence MBeans as below: Access through VisualVM Ensure you run VisualVM with the JMXMP connector on the classpath: bash $ jvisualvm -cp \"$JAVA_HOME/lib/tools.jar:opendmk_jmxremote_optional_jar-1.0-b01-ea.jar\" Note : If you have downloaded VisualVM separatley, then the executable is visualvm . From the Applications tab in the VisualVM UI, right-click on Local and select Add JMX Connection . Enter service:jmx:jmxmp://127.0.0.1:9099 for Connection and click OK . A JMX connection is added. Double-click to open the new connection. You can see the Coherence MBeans under the MBeans tab. If you have installed the Coherence VisualVM plugin, you can also see a Coherence tab. Refer to the Coherence MBean Reference for detailed information about Coherence MBeans. Uninstall the Charts Use the following command to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous samples. If you want to remove the coherence-operator , then include it in the helm delete command.","title":"Access JMX in the Coherence Cluster Using JConsole and VisualVM"},{"location":"samples/management/jmx/#access-jmx-in-the-coherence-cluster-using-jconsole-and-visualvm","text":"Java Management Extensions (JMX) is the standard way to inspect and manage enterprise Java applications. Applications that are exposed via JMX do not incur runtime performance penalty for doing so unless a tool is actively connected to the JMX connection, and only then in certain cases. The Java Tutorials provide introduction to JMX . The Coherence documentation describes how to use JMX with Coherence . All the capabilities of JMX with Coherence are also available with the operator. This sample shows how to connect to a Coherence JMX MBean Server when using the Coherence Operator. The option --set store.jmx.enabled=true is used which creates an MBean Server pod from which you can connect to the operator. By default, there is one replica for the MBean Server. You can create more MBean server pods by setting the store.jmx.replicas value. For example, --set store.jmx.replicas=2 . See here for information about connecting to Management over REST endpoint. Return to Management samples / Return to samples","title":"Access JMX in the Coherence Cluster Using JConsole and VisualVM"},{"location":"samples/management/jmx/#prerequisites","text":"Install the Coherence Operator Ensure you have installed the Coherence Operator using the instructions here . Download the JMXMP connector JAR The JMX endpoint does not use RMI, instead it uses JMXMP. This requires an additional JAR on the classpath of the Java JMX client (VisualVM and JConsole). You can use curl to download the required JAR. bash curl http://central.maven.org/maven2/org/glassfish/external/opendmk_jmxremote_optional_jar/1.0-b01-ea/opendmk_jmxremote_optional_jar-1.0-b01-ea.jar \\ -o opendmk_jmxremote_optional_jar-1.0-b01-ea.jar This also can be downloaded as a Maven dependency if you are connecting through a Maven project. xml <dependency> <groupId>org.glassfish.external</groupId> <artifactId>opendmk_jmxremote_optional_jar</artifactId> <version>1.0-b01-ea</version> </dependency>","title":"Prerequisites"},{"location":"samples/management/jmx/#installation-steps","text":"Install the Coherence cluster Install the cluster with one MBean server pod: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=coherence-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set store.jmx.enabled=true \\ --set store.jmx.replicas=1 \\ coherence/coherence Note : There are many other store.jmx.* options which control other aspects of the MBean Server node. Refer to the Coherence Operator values.yaml file for more information. After the chart is installed, instructions are displayed to help you utilize this feature. You can follow these instructions or use the commands. Check whether the MBean server pod is running: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-5899f6444b-tckm4 1/1 Running 0 1h storage-coherence-0 1/1 Running 0 29m storage-coherence-1 1/1 Running 0 28m storage-coherence-2 1/1 Running 0 27m storage-coherence-jmx-54f5d779d-svh29 1/1 Running 0 29m A pod prefixed with storage-coherence-jmx is displayed in the output. Port forward the MBean server pod: ```bash $ export POD_NAME=$(kubectl get pods --namespace sample-coherence-ns -l \"app=coherence,release=storage,component=coherenceJMXPod\" -o jsonpath=\"{.items[0].metadata.name}\") $ kubectl --namespace sample-coherence-ns port-forward $POD_NAME 9099:9099 ``` Access the JMX endpoint at the URL service:jmx:jmxmp://127.0.0.1:9099 . (Optional) Add data to a cache Connect to the Coherence console and create a cache: Note : If you do not carry out this step, then you will not see any CacheMBeans as described below. bash $ kubectl exec -it --namespace sample-coherence-ns storage-coherence-0 bash /scripts/startCoherence.sh console At the Map (?): prompt, type cache test . This will create a cache in the service PartitionedCache . Use the following to add 100,000 objects of size 1024 bytes, starting at index 0 and using batches of 100. ```bash bulkput 100000 1024 0 100 Mon Apr 15 07:37:03 GMT 2019: adding 100000 items (starting with #0) each 1024 bytes ... Mon Apr 15 07:37:15 GMT 2019: done putting (11578ms, 8878KB/sec, 8637 items/sec) ``` At the prompt, type size and it should show 100000. Then, type bye to exit the console . Access through JConsole Ensure you run JConsole with the JMXMP connector on the classpath: bash $ jconsole -J-Djava.class.path=\"$JAVA_HOME/lib/jconsole.jar:$JAVA_HOME/lib/tools.jar:opendmk_jmxremote_optional_jar-1.0-b01-ea.jar\" service:jmx:jmxmp://127.0.0.1:9099 In the console UI, select the MBeans tab and then Coherence Cluster attributes. You should see the Coherence MBeans as below: Access through VisualVM Ensure you run VisualVM with the JMXMP connector on the classpath: bash $ jvisualvm -cp \"$JAVA_HOME/lib/tools.jar:opendmk_jmxremote_optional_jar-1.0-b01-ea.jar\" Note : If you have downloaded VisualVM separatley, then the executable is visualvm . From the Applications tab in the VisualVM UI, right-click on Local and select Add JMX Connection . Enter service:jmx:jmxmp://127.0.0.1:9099 for Connection and click OK . A JMX connection is added. Double-click to open the new connection. You can see the Coherence MBeans under the MBeans tab. If you have installed the Coherence VisualVM plugin, you can also see a Coherence tab. Refer to the Coherence MBean Reference for detailed information about Coherence MBeans.","title":"Installation Steps"},{"location":"samples/management/jmx/#uninstall-the-charts","text":"Use the following command to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous samples. If you want to remove the coherence-operator , then include it in the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/management/jvmarguments/","text":"Tune JVM Runtime Settings All production enterprise Java application must carefully tune the JVM settings for maximum performance. This sample shows how to set JVM arguments to Coherence running in Kubernetes. Refer to Coherence Performance Tuning documentation for more information about JVM tuning. There are many values in the values.yaml file of the Coherence Helm chart that sets JVM arguments to the JVM that runs Coherence within Kubernetes. The values are described in the following table: --set Arguments Descirption store.maxHeap Heap size arguments to the JVM. The format should be the same as that used for Java's -Xms and -Xmx JVM options. If not set, the JVM defaults are used. store.jmx.maxHeap Heap size arguments passed to the MBean server JVM. Same format and meaning as the preceding row. store.jvmArgs Options passed directly to the JVM running Coherence within Kubernetes store.javaOpts Miscellaneous JVM options to pass to the Coherence store container Return to Management samples / Return to samples Prerequisites Ensure you have already installed the Coherence Operator by using the instructions here . Installation Steps Install the Coherence cluster Install the cluster with the following settings: --set store.maxHeap=1G - Set maxHeap to 1 GB --set store.jvmArgs=\"-Xloggc:/tmp/gc-log -server -Xcomp\" - Set generic options --set store.javaOpts=\"-Dcoherence.log.level=6 -Djava.net.preferIPv4Stack=true - Set Coherence specific arguments bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=coherence-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set store.jvmArgs=\"-Xloggc:/tmp/gc-log -server -Xcomp\" \\ --set store.javaOpts=\"-Dcoherence.log.level=6 -Djava.net.preferIPv4Stack=true\" \\ --set store.maxHeap=1g \\ coherence/coherence Ensure the pods are running: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-66f9bb7b75-hqk4l 1/1 Running 0 13m storage-coherence-0 1/1 Running 0 3m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 0/1 Running 0 44s The JVM arguments include the store. argument and other arguments required by the operator and Coherence. -Xmx1g -Xms1g -Xloggc:/tmp/gc-log -server -Xcomp -Xms8g -Xmx8g -Dcoherence.log.level=6 -Djava.net.preferIPv4Stack=true Inspect the result To inspect the full JVM arguments, you can use kubectl logs storage-coherence-0 -n sample-coherence-ns > /tmp/storage-coherence-0.log and search for one of the arguments that you specified. Uninstall the Charts Use the following command to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous sample. If you want to remove the coherence-operator , then include it in the helm delete command.","title":"Tune JVM Runtime Settings"},{"location":"samples/management/jvmarguments/#tune-jvm-runtime-settings","text":"All production enterprise Java application must carefully tune the JVM settings for maximum performance. This sample shows how to set JVM arguments to Coherence running in Kubernetes. Refer to Coherence Performance Tuning documentation for more information about JVM tuning. There are many values in the values.yaml file of the Coherence Helm chart that sets JVM arguments to the JVM that runs Coherence within Kubernetes. The values are described in the following table: --set Arguments Descirption store.maxHeap Heap size arguments to the JVM. The format should be the same as that used for Java's -Xms and -Xmx JVM options. If not set, the JVM defaults are used. store.jmx.maxHeap Heap size arguments passed to the MBean server JVM. Same format and meaning as the preceding row. store.jvmArgs Options passed directly to the JVM running Coherence within Kubernetes store.javaOpts Miscellaneous JVM options to pass to the Coherence store container Return to Management samples / Return to samples","title":"Tune JVM Runtime Settings"},{"location":"samples/management/jvmarguments/#prerequisites","text":"Ensure you have already installed the Coherence Operator by using the instructions here .","title":"Prerequisites"},{"location":"samples/management/jvmarguments/#installation-steps","text":"Install the Coherence cluster Install the cluster with the following settings: --set store.maxHeap=1G - Set maxHeap to 1 GB --set store.jvmArgs=\"-Xloggc:/tmp/gc-log -server -Xcomp\" - Set generic options --set store.javaOpts=\"-Dcoherence.log.level=6 -Djava.net.preferIPv4Stack=true - Set Coherence specific arguments bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=coherence-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set store.jvmArgs=\"-Xloggc:/tmp/gc-log -server -Xcomp\" \\ --set store.javaOpts=\"-Dcoherence.log.level=6 -Djava.net.preferIPv4Stack=true\" \\ --set store.maxHeap=1g \\ coherence/coherence Ensure the pods are running: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-66f9bb7b75-hqk4l 1/1 Running 0 13m storage-coherence-0 1/1 Running 0 3m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 0/1 Running 0 44s The JVM arguments include the store. argument and other arguments required by the operator and Coherence. -Xmx1g -Xms1g -Xloggc:/tmp/gc-log -server -Xcomp -Xms8g -Xmx8g -Dcoherence.log.level=6 -Djava.net.preferIPv4Stack=true Inspect the result To inspect the full JVM arguments, you can use kubectl logs storage-coherence-0 -n sample-coherence-ns > /tmp/storage-coherence-0.log and search for one of the arguments that you specified.","title":"Installation Steps"},{"location":"samples/management/jvmarguments/#uninstall-the-charts","text":"Use the following command to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous sample. If you want to remove the coherence-operator , then include it in the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/management/reporter/","text":"Manage the Reporter Oracle Coherence reports show key management information over time. The reports identify trends that are valuable for troubleshooting and planning. Reporting is disabled by default and must be explicitly enabled by setting enabled in an operational override file or by using system properties. This approach is valid for all versions of Coherence. This sample shows how to access and manage the reporter over REST, which is available only in Oraccle Coherence 12.2.1.4.0 or later version. Note : To enable the Coherence Reporter using system properties, refer to the section Tune JVM Runtime Settings to use store.javaOpts . The Oracle Reporter documentation explains how to set system propertites. Return to Management over REST samples Return to Management samples / Return to samples Prerequisites Ensure you have installed the Coherence Operator using the instructions here . Installation Steps Install the Coherence cluster Install the cluster with 3 nodes: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=coherence-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set coherence.image=your-12.2.1.4.0-Coherence-image \\ coherence/coherence Note: If your version of the Coherence Operator does not default to using Oracle Coherence 12.2.1.4.0, then you need to replace your-12.2.1.4.0-Coherence-image with an appropriate 12.2.1.4.0 image. Use kubectl get pods -n sample-coherence-ns and wait until all the three pods are running. Port forward the Management over REST port: bash $ kubectl port-forward storage-coherence-0 -n sample-coherence-ns 30000:30000 console Forwarding from [::1]:30000 -> 30000 Forwarding from 127.0.0.1:30000 -> 30000 Access the Reporter endpoint The Reporter is available at the following endpoint: http://127.0.0.1:30000/management/coherence/cluster/reporters The reporter needs to be started only on one node, but is available to be started on all nodes. View the reporter state on node 1: ```bash $ curl http://127.0.0.1:30000/management/coherence/cluster/reporters/1 2> /dev/null| json_pp ``` ```json { \"outputPath\" : \"/.\", \"type\" : \"Reporter\", \"runMaxMillis\" : 0, \"runAverageMillis\" : 0, \"intervalSeconds\" : 60, \"runLastMillis\" : 0, \"state\" : \"Stopped\", \"refreshTime\" : \"2019-04-26T09:02:10.146Z\", \"links\" : [ { \"href\" : \"http://127.0.0.1:30000/management/coherence/cluster/reporters\", \"rel\" : \"parent\" }, { \"href\" : \"http://127.0.0.1:30000/management/coherence/cluster/reporters/1\", \"rel\" : \"self\" }, { \"rel\" : \"canonical\", \"href\" : \"http://127.0.0.1:30000/management/coherence/cluster/reporters/1\" } ], \"nodeId\" : \"1\", \"reports\" : [ \"reports/report-node.xml\", \"reports/report-network-health.xml\", \"reports/report-network-health-detail.xml\", \"reports/report-memory-status.xml\", \"reports/report-service.xml\", \"reports/report-cache-effectiveness.xml\", \"reports/report-proxy.xml\", \"reports/report-proxy-http.xml\", \"reports/report-management.xml\", \"reports/report-flashjournal.xml\", \"reports/report-ramjournal.xml\", \"reports/report-persistence.xml\", \"reports/report-persistence-detail.xml\", \"reports/report-federation-destination.xml\", \"reports/report-federation-origin.xml\" ], \"autoStart\" : false, \"lastReport\" : null, \"lastExecuteTime\" : \"1970-01-01T00:00:00.000Z\", \"currentBatch\" : 0, \"configFile\" : \"reports/report-group.xml\" } ``` Set the reporter output directory: bash $ curl -v -X POST -H 'Content-type: application/json' \\ http://127.0.0.1:30000/management/coherence/cluster/reporters/1 -d '{\"outputPath\": \"/tmp/\"}' Validate the set output path: ```bash $ curl http://127.0.0.1:30000/management/coherence/cluster/reporters/1?fields=outputPath \\ | json_pp | grep outputPath \"outputPath\" : \"/tmp\" ``` Start the reporter: bash $ curl -X POST http://127.0.0.1:30000/management/coherence/cluster/reporters/1/start Confim that the reporter has started: bash $ curl http://127.0.0.1:30000/management/coherence/cluster/reporters/1?fields=currentBatch,lastReport,lastExecuteTime,state,runLastMillis |json_pp The State must be either Started or Sleeping . Sleeping means that the reporter has run reports and is sleeping until the next execution. json { \"lastExecuteTime\" : \"2019-04-26T09:11:53.172Z\", \"currentBatch\" : 2, \"lastReport\" : \"reports/report-federation-origin.xml\", \"runLastMillis\" : 66, \"links\" : [ { \"rel\" : \"parent\", \"href\" : \"http://127.0.0.1:30000/management/coherence/cluster/reporters\" }, { \"rel\" : \"self\", \"href\" : \"http://127.0.0.1:30000/management/coherence/cluster/reporters/1\" }, { \"href\" : \"http://127.0.0.1:30000/management/coherence/cluster/reporters/1\", \"rel\" : \"canonical\" } ], \"state\" : \"Sleeping\" } View the reporter files Execute the following command to exec into the pod and view the Reporter files: ```bash $ kubectl exec -it -n sample-coherence-ns storage-coherence-0 bash $ ls -l /tmp/*.txt -rw-r--r-- 1 root root 618 Apr 26 09:15 /tmp/2019042609-Management.txt -rw-r--r-- 1 root root 1653 Apr 26 09:15 /tmp/2019042609-memory-status.txt -rw-r--r-- 1 root root 1089 Apr 26 09:15 /tmp/2019042609-network-health-detail.txt -rw-r--r-- 1 root root 395 Apr 26 09:15 /tmp/2019042609-network-health.txt -rw-r--r-- 1 root root 1377 Apr 26 09:15 /tmp/2019042609-nodes.txt -rw-r--r-- 1 root root 711 Apr 26 09:15 /tmp/2019042609-persistence-detail.txt -rw-r--r-- 1 root root 559 Apr 26 09:15 /tmp/2019042609-persistence.txt -rw-r--r-- 1 root root 2472 Apr 26 09:15 /tmp/2019042609-report-proxy-http.txt -rw-r--r-- 1 root root 798 Apr 26 09:15 /tmp/2019042609-report-proxy.txt -rw-r--r-- 1 root root 3366 Apr 26 09:15 /tmp/2019042609-service.txt ``` To copy the files to your current directory, use the following command: bash $ kubectl exec -it -n sample-coherence-ns storage-coherence-0 -- bash -c 'cd /tmp && tar cf /tmp/reports.tar *.txt' $ kubectl cp sample-coherence-ns/storage-coherence-0:/tmp/reports.tar reports.tar Uninstall the Charts Use the following commands to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous sample. If you want to remove the coherence-operator , then include it in the helm delete command.","title":"Manage the Reporter"},{"location":"samples/management/reporter/#manage-the-reporter","text":"Oracle Coherence reports show key management information over time. The reports identify trends that are valuable for troubleshooting and planning. Reporting is disabled by default and must be explicitly enabled by setting enabled in an operational override file or by using system properties. This approach is valid for all versions of Coherence. This sample shows how to access and manage the reporter over REST, which is available only in Oraccle Coherence 12.2.1.4.0 or later version. Note : To enable the Coherence Reporter using system properties, refer to the section Tune JVM Runtime Settings to use store.javaOpts . The Oracle Reporter documentation explains how to set system propertites. Return to Management over REST samples Return to Management samples / Return to samples","title":"Manage the Reporter"},{"location":"samples/management/reporter/#prerequisites","text":"Ensure you have installed the Coherence Operator using the instructions here .","title":"Prerequisites"},{"location":"samples/management/reporter/#installation-steps","text":"Install the Coherence cluster Install the cluster with 3 nodes: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=coherence-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set coherence.image=your-12.2.1.4.0-Coherence-image \\ coherence/coherence Note: If your version of the Coherence Operator does not default to using Oracle Coherence 12.2.1.4.0, then you need to replace your-12.2.1.4.0-Coherence-image with an appropriate 12.2.1.4.0 image. Use kubectl get pods -n sample-coherence-ns and wait until all the three pods are running. Port forward the Management over REST port: bash $ kubectl port-forward storage-coherence-0 -n sample-coherence-ns 30000:30000 console Forwarding from [::1]:30000 -> 30000 Forwarding from 127.0.0.1:30000 -> 30000 Access the Reporter endpoint The Reporter is available at the following endpoint: http://127.0.0.1:30000/management/coherence/cluster/reporters The reporter needs to be started only on one node, but is available to be started on all nodes. View the reporter state on node 1: ```bash $ curl http://127.0.0.1:30000/management/coherence/cluster/reporters/1 2> /dev/null| json_pp ``` ```json { \"outputPath\" : \"/.\", \"type\" : \"Reporter\", \"runMaxMillis\" : 0, \"runAverageMillis\" : 0, \"intervalSeconds\" : 60, \"runLastMillis\" : 0, \"state\" : \"Stopped\", \"refreshTime\" : \"2019-04-26T09:02:10.146Z\", \"links\" : [ { \"href\" : \"http://127.0.0.1:30000/management/coherence/cluster/reporters\", \"rel\" : \"parent\" }, { \"href\" : \"http://127.0.0.1:30000/management/coherence/cluster/reporters/1\", \"rel\" : \"self\" }, { \"rel\" : \"canonical\", \"href\" : \"http://127.0.0.1:30000/management/coherence/cluster/reporters/1\" } ], \"nodeId\" : \"1\", \"reports\" : [ \"reports/report-node.xml\", \"reports/report-network-health.xml\", \"reports/report-network-health-detail.xml\", \"reports/report-memory-status.xml\", \"reports/report-service.xml\", \"reports/report-cache-effectiveness.xml\", \"reports/report-proxy.xml\", \"reports/report-proxy-http.xml\", \"reports/report-management.xml\", \"reports/report-flashjournal.xml\", \"reports/report-ramjournal.xml\", \"reports/report-persistence.xml\", \"reports/report-persistence-detail.xml\", \"reports/report-federation-destination.xml\", \"reports/report-federation-origin.xml\" ], \"autoStart\" : false, \"lastReport\" : null, \"lastExecuteTime\" : \"1970-01-01T00:00:00.000Z\", \"currentBatch\" : 0, \"configFile\" : \"reports/report-group.xml\" } ``` Set the reporter output directory: bash $ curl -v -X POST -H 'Content-type: application/json' \\ http://127.0.0.1:30000/management/coherence/cluster/reporters/1 -d '{\"outputPath\": \"/tmp/\"}' Validate the set output path: ```bash $ curl http://127.0.0.1:30000/management/coherence/cluster/reporters/1?fields=outputPath \\ | json_pp | grep outputPath \"outputPath\" : \"/tmp\" ``` Start the reporter: bash $ curl -X POST http://127.0.0.1:30000/management/coherence/cluster/reporters/1/start Confim that the reporter has started: bash $ curl http://127.0.0.1:30000/management/coherence/cluster/reporters/1?fields=currentBatch,lastReport,lastExecuteTime,state,runLastMillis |json_pp The State must be either Started or Sleeping . Sleeping means that the reporter has run reports and is sleeping until the next execution. json { \"lastExecuteTime\" : \"2019-04-26T09:11:53.172Z\", \"currentBatch\" : 2, \"lastReport\" : \"reports/report-federation-origin.xml\", \"runLastMillis\" : 66, \"links\" : [ { \"rel\" : \"parent\", \"href\" : \"http://127.0.0.1:30000/management/coherence/cluster/reporters\" }, { \"rel\" : \"self\", \"href\" : \"http://127.0.0.1:30000/management/coherence/cluster/reporters/1\" }, { \"href\" : \"http://127.0.0.1:30000/management/coherence/cluster/reporters/1\", \"rel\" : \"canonical\" } ], \"state\" : \"Sleeping\" } View the reporter files Execute the following command to exec into the pod and view the Reporter files: ```bash $ kubectl exec -it -n sample-coherence-ns storage-coherence-0 bash $ ls -l /tmp/*.txt -rw-r--r-- 1 root root 618 Apr 26 09:15 /tmp/2019042609-Management.txt -rw-r--r-- 1 root root 1653 Apr 26 09:15 /tmp/2019042609-memory-status.txt -rw-r--r-- 1 root root 1089 Apr 26 09:15 /tmp/2019042609-network-health-detail.txt -rw-r--r-- 1 root root 395 Apr 26 09:15 /tmp/2019042609-network-health.txt -rw-r--r-- 1 root root 1377 Apr 26 09:15 /tmp/2019042609-nodes.txt -rw-r--r-- 1 root root 711 Apr 26 09:15 /tmp/2019042609-persistence-detail.txt -rw-r--r-- 1 root root 559 Apr 26 09:15 /tmp/2019042609-persistence.txt -rw-r--r-- 1 root root 2472 Apr 26 09:15 /tmp/2019042609-report-proxy-http.txt -rw-r--r-- 1 root root 798 Apr 26 09:15 /tmp/2019042609-report-proxy.txt -rw-r--r-- 1 root root 3366 Apr 26 09:15 /tmp/2019042609-service.txt ``` To copy the files to your current directory, use the following command: bash $ kubectl exec -it -n sample-coherence-ns storage-coherence-0 -- bash -c 'cd /tmp && tar cf /tmp/reports.tar *.txt' $ kubectl cp sample-coherence-ns/storage-coherence-0:/tmp/reports.tar reports.tar","title":"Installation Steps"},{"location":"samples/management/reporter/#uninstall-the-charts","text":"Use the following commands to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous sample. If you want to remove the coherence-operator , then include it in the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/management/rest/","text":"Using Management over REST Note: Use of Management over REST is only available when using the operator with Coherence 12.2.1.4. Table of Contents Access management over REST Access management over REST using JVisualVM plugin Enable SSL with management over REST Modify Writable MBeans Return to Management samples / Return to samples","title":"Using Management over REST"},{"location":"samples/management/rest/#using-management-over-rest","text":"Note: Use of Management over REST is only available when using the operator with Coherence 12.2.1.4.","title":"Using Management over REST"},{"location":"samples/management/rest/#table-of-contents","text":"Access management over REST Access management over REST using JVisualVM plugin Enable SSL with management over REST Modify Writable MBeans Return to Management samples / Return to samples","title":"Table of Contents"},{"location":"samples/management/rest/jvisualvm/","text":"Access Management Over REST Endpoint Using VisualVM Plugin This sample shows how to connect the VisualVM Plugin in Coherence 12.2.1.4.0 to a Coherence cluster using REST. Note : Use of management over REST is available only when using the operator with Coherence 12.2.1.4.0. Return to Management over REST samples Return to Management samples / Return to samples Prerequisites Install Coherence Cluster Follow the instructions here to install a Coherence cluster and port forward the management over REST port. Install the Coherence VisualVM plugin Follow the instructions here to install the VisualVM plugin. Installation Steps Start VisualVM bash $ jvisualvm Or, if you have downloaded VisualVM separately, the executable is visualvm . Create the connection If the Coherence Visualvm plugin is correctly installed, then you should see Coherence Clusters item under the Applications tab. Right-click on this and select Add Coherence Cluster . Enter a name for the cluster, and then enter the following URL for management REST: http://127.0.0.1:30000/management/coherence/cluster Connect to the cluster Double-click on the new cluster you created and you can view the Coherence tab in VisualVM. Uninstall the Charts Use the following command to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous samples. If you want to remove the coherence-operator , then include it in the helm delete command.","title":"Access Management Over REST Endpoint Using VisualVM Plugin"},{"location":"samples/management/rest/jvisualvm/#access-management-over-rest-endpoint-using-visualvm-plugin","text":"This sample shows how to connect the VisualVM Plugin in Coherence 12.2.1.4.0 to a Coherence cluster using REST. Note : Use of management over REST is available only when using the operator with Coherence 12.2.1.4.0. Return to Management over REST samples Return to Management samples / Return to samples","title":"Access Management Over REST Endpoint Using VisualVM Plugin"},{"location":"samples/management/rest/jvisualvm/#prerequisites","text":"Install Coherence Cluster Follow the instructions here to install a Coherence cluster and port forward the management over REST port. Install the Coherence VisualVM plugin Follow the instructions here to install the VisualVM plugin.","title":"Prerequisites"},{"location":"samples/management/rest/jvisualvm/#installation-steps","text":"Start VisualVM bash $ jvisualvm Or, if you have downloaded VisualVM separately, the executable is visualvm . Create the connection If the Coherence Visualvm plugin is correctly installed, then you should see Coherence Clusters item under the Applications tab. Right-click on this and select Add Coherence Cluster . Enter a name for the cluster, and then enter the following URL for management REST: http://127.0.0.1:30000/management/coherence/cluster Connect to the cluster Double-click on the new cluster you created and you can view the Coherence tab in VisualVM.","title":"Installation Steps"},{"location":"samples/management/rest/jvisualvm/#uninstall-the-charts","text":"Use the following command to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous samples. If you want to remove the coherence-operator , then include it in the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/management/rest/mbeans/","text":"Modify Writable MBeans Management over REST provides the ability to modify Coherence MBeans that are writable. This sample shows how to modify the log level of a member and the expiryDelay of a cache using Curl. Note : Use of management over REST is available only when using the operator with Oracle Coherence 12.2.1.4.0 version. Return to Management over REST samples Return to Management samples / Return to samples Prerequisites Install Coherence cluster Follow the instructions here to install a Coherence cluster and port forward the management over REST port. Install the Coherence VisualVM plugin Follow the instructions here to install the VisualVM plugin. Installation Steps Retrieve the current loggingLevel : bash $ curl http://127.0.0.1:30000/management/coherence/cluster/members?fields=loggingLevel 2> /dev/null | json_pp | grep \"loggingLevel\" ```console \"loggingLevel\" : 5, \"loggingLevel\" : 5, \"loggingLevel\" : 5, \"loggingLevel\" : 5, ``` This shows all members are running at logging level = 5 (the default). Ensure there are no D6 messages: bash $ kubectl logs storage-coherence-0 -n sample-coherence-ns | grep D6 The command should not return anything. Set the loggingLevel for each member to 9: bash $ for i in 1 2 3 4; do curl -X POST -H 'Content-type: application/json' http://127.0.0.1:30000/management/coherence/cluster/members/$i -d '{\"loggingLevel\": 9}' done {}{}{}{} Query the logging level again: ```bash $ curl http://127.0.0.1:30000/management/coherence/cluster/members?fields=loggingLevel 2> /dev/null | json_pp | grep \"loggingLevel\" \"loggingLevel\" : 9 \"loggingLevel\" : 9 \"loggingLevel\" : 9, \"loggingLevel\" : 9 ``` Add data to the cluster using the Coherence Console. Connect to the Coherence Console and create a cache. This triggers log messages for the joining member. bash $ kubectl exec -it --namespace sample-coherence-ns storage-coherence-0 bash /scripts/startCoherence.sh console At the Map (?): prompt, enter cache test . This creates a cache in the service PartitionedCache . Add an entry to the cache: ```bash put 1 one null ``` Then, type bye to exit the console. Inspect the log files for D6 messages bash $ kubectl logs storage-coherence-0 -n sample-coherence-ns | grep D6 console 2019-04-24 04:58:56.203/3687.142 Oracle Coherence GE 12.2.1.4.0 <D6> (thread=Cluster, member=1): TcpRing connected to Member(Id=5, Timestamp=2019-04-24 04:58:55.99, Address=10.1.4.147:32923, MachineId=30443, Location=site:coherence.sample-coherence-ns.svc.cluster.local,machine:docker-for-desktop,process:6020,member:storage-coherence-0, Role=CoherenceConsole) 2019-04-24 04:58:56.204/3687.144 Oracle Coherence GE 12.2.1.4.0 <D6> (thread=Cluster, member=1): TcpRing connected to Member(Id=5, Timestamp=2019-04-24 04:58:55.99, Address=10.1.4.147:32923, MachineId=30443, Location=site:coherence.sample-coherence-ns.svc.cluster.local,machine:docker-for-desktop,process:6020,member:storage-coherence-0, Role=CoherenceConsole) 2019-04-24 04:58:56.480/3687.420 Oracle Coherence GE 12.2.1.4.0 <D6> (thread=Transport:TransportService, member=1): Registered Connection {Peer=tmb://10.1.4.147:32923.64682, Service=TransportService, Member=5, Not established, State=CONNECTING, peer=tmb://10.1.4.147:32923.64682, state=OPEN, socket=MultiplexedSocket{Socket[addr=/10.1.4.147,port=32923,localport=57374]}, bytes(in=0, out=0), flushlock false, bufferedOut=0B, unflushed=0B, delivered(in=0, out=0), timeout(n/a), interestOps=0, unflushed receipt=0, receiptReturn 0, isReceiptFlushRequired false, bufferedIn(), msgs(in=0, out=0/0)} The level D6 messages are displayed in the output. These particular messages related to the cluster member (console) joining the cluster. Retrieve the current expiryDelay for all members: bash curl http://127.0.0.1:30000/management/coherence/cluster/services/PartitionedCache/caches/test/members?fields=expiryDelay 2> /dev/null | json_pp | grep expiryDelay console \"expiryDelay\" : 0, \"expiryDelay\" : 0, \"expiryDelay\" : 0, \"expiryDelay\" : 0 Set the expiryDelay for each member to 60000 ms: bash $ for i in 1 2 3 4; do curl -X POST -H 'Content-type: application/json' http://127.0.0.1:30000/management/coherence/cluster/services/PartitionedCache/caches/test/members/$i -d '{\"expiryDelay\": 60000}' done {}{}{}{} Query the expiryDelay for all members again: bash curl http://127.0.0.1:30000/management/coherence/cluster/services/PartitionedCache/caches/test/members?fields=expiryDelay 2> /dev/null | json_pp | grep expiryDelay console \"expiryDelay\" : 60000, \"expiryDelay\" : 60000, \"expiryDelay\" : 60000, \"expiryDelay\" : 60000 Note : You can also update highUnits in a similar way to to expiryDelay . Uninstall the Charts Use the following command to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous samples. If you want to remove the coherence-operator , then use the helm delete command.","title":"Modify Writable MBeans"},{"location":"samples/management/rest/mbeans/#modify-writable-mbeans","text":"Management over REST provides the ability to modify Coherence MBeans that are writable. This sample shows how to modify the log level of a member and the expiryDelay of a cache using Curl. Note : Use of management over REST is available only when using the operator with Oracle Coherence 12.2.1.4.0 version. Return to Management over REST samples Return to Management samples / Return to samples","title":"Modify Writable MBeans"},{"location":"samples/management/rest/mbeans/#prerequisites","text":"Install Coherence cluster Follow the instructions here to install a Coherence cluster and port forward the management over REST port. Install the Coherence VisualVM plugin Follow the instructions here to install the VisualVM plugin.","title":"Prerequisites"},{"location":"samples/management/rest/mbeans/#installation-steps","text":"Retrieve the current loggingLevel : bash $ curl http://127.0.0.1:30000/management/coherence/cluster/members?fields=loggingLevel 2> /dev/null | json_pp | grep \"loggingLevel\" ```console \"loggingLevel\" : 5, \"loggingLevel\" : 5, \"loggingLevel\" : 5, \"loggingLevel\" : 5, ``` This shows all members are running at logging level = 5 (the default). Ensure there are no D6 messages: bash $ kubectl logs storage-coherence-0 -n sample-coherence-ns | grep D6 The command should not return anything. Set the loggingLevel for each member to 9: bash $ for i in 1 2 3 4; do curl -X POST -H 'Content-type: application/json' http://127.0.0.1:30000/management/coherence/cluster/members/$i -d '{\"loggingLevel\": 9}' done {}{}{}{} Query the logging level again: ```bash $ curl http://127.0.0.1:30000/management/coherence/cluster/members?fields=loggingLevel 2> /dev/null | json_pp | grep \"loggingLevel\" \"loggingLevel\" : 9 \"loggingLevel\" : 9 \"loggingLevel\" : 9, \"loggingLevel\" : 9 ``` Add data to the cluster using the Coherence Console. Connect to the Coherence Console and create a cache. This triggers log messages for the joining member. bash $ kubectl exec -it --namespace sample-coherence-ns storage-coherence-0 bash /scripts/startCoherence.sh console At the Map (?): prompt, enter cache test . This creates a cache in the service PartitionedCache . Add an entry to the cache: ```bash put 1 one null ``` Then, type bye to exit the console. Inspect the log files for D6 messages bash $ kubectl logs storage-coherence-0 -n sample-coherence-ns | grep D6 console 2019-04-24 04:58:56.203/3687.142 Oracle Coherence GE 12.2.1.4.0 <D6> (thread=Cluster, member=1): TcpRing connected to Member(Id=5, Timestamp=2019-04-24 04:58:55.99, Address=10.1.4.147:32923, MachineId=30443, Location=site:coherence.sample-coherence-ns.svc.cluster.local,machine:docker-for-desktop,process:6020,member:storage-coherence-0, Role=CoherenceConsole) 2019-04-24 04:58:56.204/3687.144 Oracle Coherence GE 12.2.1.4.0 <D6> (thread=Cluster, member=1): TcpRing connected to Member(Id=5, Timestamp=2019-04-24 04:58:55.99, Address=10.1.4.147:32923, MachineId=30443, Location=site:coherence.sample-coherence-ns.svc.cluster.local,machine:docker-for-desktop,process:6020,member:storage-coherence-0, Role=CoherenceConsole) 2019-04-24 04:58:56.480/3687.420 Oracle Coherence GE 12.2.1.4.0 <D6> (thread=Transport:TransportService, member=1): Registered Connection {Peer=tmb://10.1.4.147:32923.64682, Service=TransportService, Member=5, Not established, State=CONNECTING, peer=tmb://10.1.4.147:32923.64682, state=OPEN, socket=MultiplexedSocket{Socket[addr=/10.1.4.147,port=32923,localport=57374]}, bytes(in=0, out=0), flushlock false, bufferedOut=0B, unflushed=0B, delivered(in=0, out=0), timeout(n/a), interestOps=0, unflushed receipt=0, receiptReturn 0, isReceiptFlushRequired false, bufferedIn(), msgs(in=0, out=0/0)} The level D6 messages are displayed in the output. These particular messages related to the cluster member (console) joining the cluster. Retrieve the current expiryDelay for all members: bash curl http://127.0.0.1:30000/management/coherence/cluster/services/PartitionedCache/caches/test/members?fields=expiryDelay 2> /dev/null | json_pp | grep expiryDelay console \"expiryDelay\" : 0, \"expiryDelay\" : 0, \"expiryDelay\" : 0, \"expiryDelay\" : 0 Set the expiryDelay for each member to 60000 ms: bash $ for i in 1 2 3 4; do curl -X POST -H 'Content-type: application/json' http://127.0.0.1:30000/management/coherence/cluster/services/PartitionedCache/caches/test/members/$i -d '{\"expiryDelay\": 60000}' done {}{}{}{} Query the expiryDelay for all members again: bash curl http://127.0.0.1:30000/management/coherence/cluster/services/PartitionedCache/caches/test/members?fields=expiryDelay 2> /dev/null | json_pp | grep expiryDelay console \"expiryDelay\" : 60000, \"expiryDelay\" : 60000, \"expiryDelay\" : 60000, \"expiryDelay\" : 60000 Note : You can also update highUnits in a similar way to to expiryDelay .","title":"Installation Steps"},{"location":"samples/management/rest/mbeans/#uninstall-the-charts","text":"Use the following command to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous samples. If you want to remove the coherence-operator , then use the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/management/rest/ssl/","text":"Enable SSL with Management over REST Endpoint By default, when the Coherence chart is installed, the Management over REST endpoint is exposed at port 30000 (through HTTP) on each of the pods. This sample shows how you can access and configure the management over REST endpoint to use SSL. Note : Use of management over REST endpoint is available only when using the operator with Oracle Coherence 12.2.1.4.0 version. Return to Management over REST samples Return to Management samples / Return to samples Sample files src/main/java/com/oracle/coherence/examples/SampleRESTClient.java - Client to connect to management over REST through SSL src/main/java/com/oracle/coherence/examples/HttpSSLHelper.java - Client to connect to management over Rest through SSL Prerequisites Ensure you have already installed the Coherence Operator using the instructions here . Installation Steps Change to the samples/management/rest/ssl/src/main/resources/certs directory and ensure that you have Maven build environment set for JDK 8 and build the project: bash $ mvn clean compile Note : This sample uses self-signed certificates and simple passwords. They are for sample purposes only and must not be used in a production environment. You must use and generate proper certificates with appropriate passwords. Create the SSL Secret: ```bash $ cd /src/main/resources/certs $ kubectl -n sample-coherence-ns create secret generic ssl-secret \\ --from-file icarus.jks \\ --from-file truststore-guardians.jks \\ --from-literal keypassword.txt=password \\ --from-literal storepassword.txt=password \\ --from-literal trustpassword.txt=secret ``` Install the Coherence cluster: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=rest-ssl-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=true \\ --set logCaptureEnabled=false \\ --set store.management.ssl.enabled=true \\ --set store.management.ssl.secrets=ssl-secret \\ --set store.management.ssl.keyStore=icarus.jks \\ --set store.management.ssl.keyStorePasswordFile=storepassword.txt \\ --set store.management.ssl.keyPasswordFile=keypassword.txt \\ --set store.management.ssl.keyStoreType=JKS \\ --set store.management.ssl.trustStore=truststore-guardians.jks \\ --set store.management.ssl.trustStorePasswordFile=trustpassword.txt \\ --set store.management.ssl.trustStoreType=JKS \\ --set store.management.ssl.requireClientCert=false \\ --set coherence.image=your-12.2.1.4.0-Coherence-image \\ coherence/coherence Note: If your version of the Coherence Operator does not default to using Oracle Coherence 12.2.1.4.0, then you must replace your-12.2.1.4.0-Coherence-image with an appropriate 12.2.1.4.0 image. Confirm that the SSL is applied: bash $ kubectl logs storage-coherence-0 --namespace sample-coherence-ns | grep SSLSocketProviderDependencies console 2019-06-17 03:31:13.256/7.535 Oracle Coherence GE 12.2.1.4.0 <D5> (thread=main, member=1): instantiated SSLSocketProviderDependencies: SSLSocketProvider(auth=two-way, identity=SunX509/file:/coherence/certs/management/icarus.jks, trust=SunX509/file:/coherence/certs/management/truststore-guardians.jks) Port forward the management over REST endpoint port: bash $ kubectl port-forward storage-coherence-0 -n sample-coherence-ns 30000:30000 console Forwarding from [::1]:30000 -> 30000 Forwarding from 127.0.0.1:30000 -> 30000 Access management over REST endpoint Use the following command to run the SampleRESTClient which connects through SSL: bash $ mvn exec:java This results in the output with the additional content from the REST endpoint: console Success, HTTP Response code is 200 Uninstall the Charts Use the following commands to delete the two charts installed in this sample: $ helm delete storage --purge Delete the secret using the following: $ kubectl delete secret ssl-secret --namespace sample-coherence-ns Before starting another sample, ensure that all the pods are removed from previous sample. If you want to remove the coherence-operator , then use the helm delete command.","title":"Enable SSL with Management over REST Endpoint"},{"location":"samples/management/rest/ssl/#enable-ssl-with-management-over-rest-endpoint","text":"By default, when the Coherence chart is installed, the Management over REST endpoint is exposed at port 30000 (through HTTP) on each of the pods. This sample shows how you can access and configure the management over REST endpoint to use SSL. Note : Use of management over REST endpoint is available only when using the operator with Oracle Coherence 12.2.1.4.0 version. Return to Management over REST samples Return to Management samples / Return to samples","title":"Enable SSL with Management over REST Endpoint"},{"location":"samples/management/rest/ssl/#sample-files","text":"src/main/java/com/oracle/coherence/examples/SampleRESTClient.java - Client to connect to management over REST through SSL src/main/java/com/oracle/coherence/examples/HttpSSLHelper.java - Client to connect to management over Rest through SSL","title":"Sample files"},{"location":"samples/management/rest/ssl/#prerequisites","text":"Ensure you have already installed the Coherence Operator using the instructions here .","title":"Prerequisites"},{"location":"samples/management/rest/ssl/#installation-steps","text":"Change to the samples/management/rest/ssl/src/main/resources/certs directory and ensure that you have Maven build environment set for JDK 8 and build the project: bash $ mvn clean compile Note : This sample uses self-signed certificates and simple passwords. They are for sample purposes only and must not be used in a production environment. You must use and generate proper certificates with appropriate passwords. Create the SSL Secret: ```bash $ cd /src/main/resources/certs $ kubectl -n sample-coherence-ns create secret generic ssl-secret \\ --from-file icarus.jks \\ --from-file truststore-guardians.jks \\ --from-literal keypassword.txt=password \\ --from-literal storepassword.txt=password \\ --from-literal trustpassword.txt=secret ``` Install the Coherence cluster: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=rest-ssl-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=true \\ --set logCaptureEnabled=false \\ --set store.management.ssl.enabled=true \\ --set store.management.ssl.secrets=ssl-secret \\ --set store.management.ssl.keyStore=icarus.jks \\ --set store.management.ssl.keyStorePasswordFile=storepassword.txt \\ --set store.management.ssl.keyPasswordFile=keypassword.txt \\ --set store.management.ssl.keyStoreType=JKS \\ --set store.management.ssl.trustStore=truststore-guardians.jks \\ --set store.management.ssl.trustStorePasswordFile=trustpassword.txt \\ --set store.management.ssl.trustStoreType=JKS \\ --set store.management.ssl.requireClientCert=false \\ --set coherence.image=your-12.2.1.4.0-Coherence-image \\ coherence/coherence Note: If your version of the Coherence Operator does not default to using Oracle Coherence 12.2.1.4.0, then you must replace your-12.2.1.4.0-Coherence-image with an appropriate 12.2.1.4.0 image. Confirm that the SSL is applied: bash $ kubectl logs storage-coherence-0 --namespace sample-coherence-ns | grep SSLSocketProviderDependencies console 2019-06-17 03:31:13.256/7.535 Oracle Coherence GE 12.2.1.4.0 <D5> (thread=main, member=1): instantiated SSLSocketProviderDependencies: SSLSocketProvider(auth=two-way, identity=SunX509/file:/coherence/certs/management/icarus.jks, trust=SunX509/file:/coherence/certs/management/truststore-guardians.jks) Port forward the management over REST endpoint port: bash $ kubectl port-forward storage-coherence-0 -n sample-coherence-ns 30000:30000 console Forwarding from [::1]:30000 -> 30000 Forwarding from 127.0.0.1:30000 -> 30000 Access management over REST endpoint Use the following command to run the SampleRESTClient which connects through SSL: bash $ mvn exec:java This results in the output with the additional content from the REST endpoint: console Success, HTTP Response code is 200","title":"Installation Steps"},{"location":"samples/management/rest/ssl/#uninstall-the-charts","text":"Use the following commands to delete the two charts installed in this sample: $ helm delete storage --purge Delete the secret using the following: $ kubectl delete secret ssl-secret --namespace sample-coherence-ns Before starting another sample, ensure that all the pods are removed from previous sample. If you want to remove the coherence-operator , then use the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/management/rest/standard/","text":"Access Management over REST When the Coherence chart is installed, the Management over REST endpoint is exposed on port 30000 on each of the pods by default. This sample shows how you can access the Management over REST endpoint using the following URL: http://host:30000/management/coherence/cluster . You can view the Swagger document at: http://host:30000/management/coherence/cluster/metadata-catalog . Note : Use of Management over REST is available only when using the operator with Oracle Coherence 12.2.1.4.0. Return to Management over REST samples Return to Management samples / Return to samples Prerequisites Ensure you have already installed the Coherence Operator by using the instructions here . Installation Steps Install the Coherence cluster Execute the following command to install the cluster: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=coherence-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set coherence.image=your-12.2.1.4.0-Coherence-image \\ coherence/coherence Note: If your version of the Coherence Operator does not default to using Coherence 12.2.1.4.0, then you need to replace your-12.2.1.4.0-Coherence-image with an appropriate 12.2.1.4.0 image. After the installation completes, list the pods: bash $ kubectl get pods -n sample-coherence-ns NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 4m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 2m Port forward the Management over REST port: bash $ kubectl port-forward storage-coherence-0 -n sample-coherence-ns 30000:30000 console Forwarding from [::1]:30000 -> 30000 Forwarding from 127.0.0.1:30000 -> 30000 Access Management over REST Use Curl to access the endpoint: bash $ curl --noproxy http://127.0.0.1:30000/management/coherence/cluster This returns the top-level JSON. You can access the Swagger endpoint via http://127.0.0.1:30000/management/coherence/cluster/metadata-catalog . You can specify individual attributes via the following: bash $ curl http://127.0.0.1:30000/management/coherence/cluster?fields=clusterName,running,version,clusterSize The output, minus the links element, will be similar to: json { \"links\": [ ... ] \"clusterSize\":3, \"version\":\"12.2.1.4.0\", \"running\":true, \"clusterName\":\"coherence-cluster\"} } Uninstall the Charts Use the following command to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous samples. If you want to remove the coherence-operator , then include it in the helm delete command.","title":"Access Management over REST"},{"location":"samples/management/rest/standard/#access-management-over-rest","text":"When the Coherence chart is installed, the Management over REST endpoint is exposed on port 30000 on each of the pods by default. This sample shows how you can access the Management over REST endpoint using the following URL: http://host:30000/management/coherence/cluster . You can view the Swagger document at: http://host:30000/management/coherence/cluster/metadata-catalog . Note : Use of Management over REST is available only when using the operator with Oracle Coherence 12.2.1.4.0. Return to Management over REST samples Return to Management samples / Return to samples","title":"Access Management over REST"},{"location":"samples/management/rest/standard/#prerequisites","text":"Ensure you have already installed the Coherence Operator by using the instructions here .","title":"Prerequisites"},{"location":"samples/management/rest/standard/#installation-steps","text":"Install the Coherence cluster Execute the following command to install the cluster: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=coherence-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set coherence.image=your-12.2.1.4.0-Coherence-image \\ coherence/coherence Note: If your version of the Coherence Operator does not default to using Coherence 12.2.1.4.0, then you need to replace your-12.2.1.4.0-Coherence-image with an appropriate 12.2.1.4.0 image. After the installation completes, list the pods: bash $ kubectl get pods -n sample-coherence-ns NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 4m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 2m Port forward the Management over REST port: bash $ kubectl port-forward storage-coherence-0 -n sample-coherence-ns 30000:30000 console Forwarding from [::1]:30000 -> 30000 Forwarding from 127.0.0.1:30000 -> 30000 Access Management over REST Use Curl to access the endpoint: bash $ curl --noproxy http://127.0.0.1:30000/management/coherence/cluster This returns the top-level JSON. You can access the Swagger endpoint via http://127.0.0.1:30000/management/coherence/cluster/metadata-catalog . You can specify individual attributes via the following: bash $ curl http://127.0.0.1:30000/management/coherence/cluster?fields=clusterName,running,version,clusterSize The output, minus the links element, will be similar to: json { \"links\": [ ... ] \"clusterSize\":3, \"version\":\"12.2.1.4.0\", \"running\":true, \"clusterName\":\"coherence-cluster\"} }","title":"Installation Steps"},{"location":"samples/management/rest/standard/#uninstall-the-charts","text":"Use the following command to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous samples. If you want to remove the coherence-operator , then include it in the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/operator/","text":"Coherence Operator samples The following samples show how to use and deploy various parts of the Coherence Operator. Table of Contents Logging Enable log capture to view logs in Kiabana Configure user logs and view in Kibana Push logs to your own Elasticsearch Instance Metrics Deploy the operator with Prometheus enabled and view metrics in Grafana Enable SSL for Metrics Scaling a Coherence deployment via kubectl Change image version for Coherence or application container using rolling upgrade Return to samples","title":"Coherence Operator samples"},{"location":"samples/operator/#coherence-operator-samples","text":"The following samples show how to use and deploy various parts of the Coherence Operator.","title":"Coherence Operator samples"},{"location":"samples/operator/#table-of-contents","text":"Logging Enable log capture to view logs in Kiabana Configure user logs and view in Kibana Push logs to your own Elasticsearch Instance Metrics Deploy the operator with Prometheus enabled and view metrics in Grafana Enable SSL for Metrics Scaling a Coherence deployment via kubectl Change image version for Coherence or application container using rolling upgrade Return to samples","title":"Table of Contents"},{"location":"samples/operator/logging/","text":"Logging These samples explain various tasks related to logging within the Coherence Operator and deployed Coherence clusters. Table of Contents Enable log capture to view logs in Kiabana Configure custom logger and view in Kibana Push logs to your own Elasticsearch Instance Return to Coherence Operator samples / Return to samples","title":"Logging"},{"location":"samples/operator/logging/#logging","text":"These samples explain various tasks related to logging within the Coherence Operator and deployed Coherence clusters.","title":"Logging"},{"location":"samples/operator/logging/#table-of-contents","text":"Enable log capture to view logs in Kiabana Configure custom logger and view in Kibana Push logs to your own Elasticsearch Instance Return to Coherence Operator samples / Return to samples","title":"Table of Contents"},{"location":"samples/operator/logging/custom-logs/","text":"Configure Custom Logger and View in Kibana The Coherence Operator manages data logging through the Elasticsearch, Fluentd, and Kibana (EFK) stack. This sample shows how to: * Configure your own application logger named cloud * Capture the logs through Fluentd * Pass the logs into Elasticsearch * Display the logs in Kibana A server-side Interceptor updates values to upper case and writes log messages to the cloud logger. Return to Logging samples / Return to Coherence Operator samples / Return to samples Sample files src/main/docker/Dockerfile - Dockerfile for creating side-car image from which configuration and server side JARs are read from at pod startup. src/main/resources/conf/custom-logging.properties - Custom logging.properties file with cloud logger. src/main/resources/conf/fluentd-cloud.conf - Fluentd configuration for cloud logger. src/main/java/com/oracle/coherence/examples/CustomFileHandler.java - Logger for custom sample logger. src/main/java/com/oracle/coherence/examples/UppercaseLoggingInterceptor.java - Interceptor to update values to upper case and use custom logger. Prerequisites Ensure you have already installed the Coherence Operator with --set logCaptureEnabled=true using the instructions here . Installation Steps Change to the samples/operator/logging/custom-logs directory and ensure you have your Maven build environment set for JDK 8 and build the project: bash mvn clean install -P docker The Docker image is built with the cache and logging configuration, and compiled Java classes. Note: If you are using a remote Kubernetes cluster, you need to push the built Docker image to your repository accessible to that cluster. Also, you need to prefix the image name in the helm install command. Install the Coherence cluster The following additional options are required: --set logCaptureEnabled=true - Enables log catpure. --set userArtifacts.image=custom-logger-sample:1.0.0-SNAPSHOT - Sets custom image with configuration and classes. If you are using a remote Kubernetes cluster, the value of this option must be the Docker image that can be pulled by the cluster. For example, mydockerid/custom-logger-samples:1.0.0-SNAPSHOT . --set store.logging.configFile=custom-logging.properties - Configures custom logger. --set fluentd.application.configFile=/conf/fluentd-cloud.conf - Includes custom fluentd.conf for cloud logger. --set fluentd.application.tag=cloud - Sets the Fluentd application tag. bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=custom-logger-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=storage-cache-config.xml \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=true \\ --set userArtifacts.image=custom-logger-sample:1.0.0-SNAPSHOT \\ --set store.logging.configFile=custom-logging.properties \\ --set fluentd.application.configFile=/conf/fluentd-cloud.conf \\ --set fluentd.application.tag=cloud \\ coherence/coherence After the installation completes, list the pods: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-7f596c6796-9v5m4 2/2 Running 0 58m elasticsearch-5b5474865c-glrl8 1/1 Running 0 58m kibana-f6955c4b9-f959q 1/1 Running 0 58m storage-coherence-0 2/2 Running 0 2m storage-coherence-1 2/2 Running 0 1m storage-coherence-2 2/2 Running 0 1m Ensure that all pods (storage-coherence-0/1/2) are running and ready. Port forward the Coherence proxy port on the storage-coherence-0 pod. bash $ kubectl port-forward -n sample-coherence-ns storage-coherence-0 20000:20000 Connect through CohQL and execute the following command: bash $ mvn exec:java Use the following CohQL commands to insert data into the cluster: sql insert into 'test' key('key-1') value('value-1'); insert into 'test' key('key-2') value('value-2'); insert into 'test' key('key-3') value('value-3'); insert into 'test' key('key-4') value('value-4'); Verify the data: sql select key(), value() from 'test'; Results [\"key-4\", \"VALUE-4\"] [\"key-2\", \"VALUE-2\"] [\"key-1\", \"VALUE-1\"] [\"key-3\", \"VALUE-3\"] From the ouput, you can observe that the server side interceptor has changed the value to uppercase. Confirm that the log message can be viewed: bash $ kubectl exec -it storage-coherence-0 -n sample-coherence-ns -c coherence -- bash -c 'cat /logs/cloud*.log' console 2019-04-29 04:45:03 Cloud 1.0 <INFO> (cluster=custom-logger-cluster, member=storage-coherence-0, thread=PartitionedCacheWorker:0x0000:5): Before, key=key-4, value=value-4 2019-04-29 04:45:03 Cloud 1.0 <INFO> (cluster=custom-logger-cluster, member=storage-coherence-0, thread=PartitionedCacheWorker:0x0000:5): Changed key=key-4 to value=VALUE-4 Note : Depending upon the data distribution, not all members show the messages. Verify Kibana Logs Access Kibana Access Kibana using the instructions here . Create an Index Pattern In Kibana, open Management and click Index Patterns . Click Create index pattern . Set the name to cloud-* . This will show that this matches 1 index, such as cloud-2019.04.29 . Click Next Step and select @timestamp in the Time Filter field name drop-down menu and click Create index pattern . Note: It takes approximately 5 minutes for the data to reach the Elasticsearch instance. View data from the cloud-* index pattern Open Discover and select cloud-* in the drop-down list which shows coherence-cluster-* . When you click Refresh , you can see the data from the custom logger. Uninstall the Charts To delete chart installed in this sample, use the following command: $ helm delete storage --purge Note : If you are using Kubernetes version 1.13.0 or oler version, then you cannot delete the pods. This is a known issue and you need to add the options --force --grace-period=0 to force delete the pods. Refer to https://github.com/kubernetes/kubernetes/issues/45688 . Before starting another sample, ensure that all the pods are removed from previous sample. If you want to remove the coherence-operator , then use the helm delete command.","title":"Configure Custom Logger and View in Kibana"},{"location":"samples/operator/logging/custom-logs/#configure-custom-logger-and-view-in-kibana","text":"The Coherence Operator manages data logging through the Elasticsearch, Fluentd, and Kibana (EFK) stack. This sample shows how to: * Configure your own application logger named cloud * Capture the logs through Fluentd * Pass the logs into Elasticsearch * Display the logs in Kibana A server-side Interceptor updates values to upper case and writes log messages to the cloud logger. Return to Logging samples / Return to Coherence Operator samples / Return to samples","title":"Configure Custom Logger and View in Kibana"},{"location":"samples/operator/logging/custom-logs/#sample-files","text":"src/main/docker/Dockerfile - Dockerfile for creating side-car image from which configuration and server side JARs are read from at pod startup. src/main/resources/conf/custom-logging.properties - Custom logging.properties file with cloud logger. src/main/resources/conf/fluentd-cloud.conf - Fluentd configuration for cloud logger. src/main/java/com/oracle/coherence/examples/CustomFileHandler.java - Logger for custom sample logger. src/main/java/com/oracle/coherence/examples/UppercaseLoggingInterceptor.java - Interceptor to update values to upper case and use custom logger.","title":"Sample files"},{"location":"samples/operator/logging/custom-logs/#prerequisites","text":"Ensure you have already installed the Coherence Operator with --set logCaptureEnabled=true using the instructions here .","title":"Prerequisites"},{"location":"samples/operator/logging/custom-logs/#installation-steps","text":"Change to the samples/operator/logging/custom-logs directory and ensure you have your Maven build environment set for JDK 8 and build the project: bash mvn clean install -P docker The Docker image is built with the cache and logging configuration, and compiled Java classes. Note: If you are using a remote Kubernetes cluster, you need to push the built Docker image to your repository accessible to that cluster. Also, you need to prefix the image name in the helm install command. Install the Coherence cluster The following additional options are required: --set logCaptureEnabled=true - Enables log catpure. --set userArtifacts.image=custom-logger-sample:1.0.0-SNAPSHOT - Sets custom image with configuration and classes. If you are using a remote Kubernetes cluster, the value of this option must be the Docker image that can be pulled by the cluster. For example, mydockerid/custom-logger-samples:1.0.0-SNAPSHOT . --set store.logging.configFile=custom-logging.properties - Configures custom logger. --set fluentd.application.configFile=/conf/fluentd-cloud.conf - Includes custom fluentd.conf for cloud logger. --set fluentd.application.tag=cloud - Sets the Fluentd application tag. bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=custom-logger-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=storage-cache-config.xml \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=true \\ --set userArtifacts.image=custom-logger-sample:1.0.0-SNAPSHOT \\ --set store.logging.configFile=custom-logging.properties \\ --set fluentd.application.configFile=/conf/fluentd-cloud.conf \\ --set fluentd.application.tag=cloud \\ coherence/coherence After the installation completes, list the pods: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-7f596c6796-9v5m4 2/2 Running 0 58m elasticsearch-5b5474865c-glrl8 1/1 Running 0 58m kibana-f6955c4b9-f959q 1/1 Running 0 58m storage-coherence-0 2/2 Running 0 2m storage-coherence-1 2/2 Running 0 1m storage-coherence-2 2/2 Running 0 1m Ensure that all pods (storage-coherence-0/1/2) are running and ready. Port forward the Coherence proxy port on the storage-coherence-0 pod. bash $ kubectl port-forward -n sample-coherence-ns storage-coherence-0 20000:20000 Connect through CohQL and execute the following command: bash $ mvn exec:java Use the following CohQL commands to insert data into the cluster: sql insert into 'test' key('key-1') value('value-1'); insert into 'test' key('key-2') value('value-2'); insert into 'test' key('key-3') value('value-3'); insert into 'test' key('key-4') value('value-4'); Verify the data: sql select key(), value() from 'test'; Results [\"key-4\", \"VALUE-4\"] [\"key-2\", \"VALUE-2\"] [\"key-1\", \"VALUE-1\"] [\"key-3\", \"VALUE-3\"] From the ouput, you can observe that the server side interceptor has changed the value to uppercase. Confirm that the log message can be viewed: bash $ kubectl exec -it storage-coherence-0 -n sample-coherence-ns -c coherence -- bash -c 'cat /logs/cloud*.log' console 2019-04-29 04:45:03 Cloud 1.0 <INFO> (cluster=custom-logger-cluster, member=storage-coherence-0, thread=PartitionedCacheWorker:0x0000:5): Before, key=key-4, value=value-4 2019-04-29 04:45:03 Cloud 1.0 <INFO> (cluster=custom-logger-cluster, member=storage-coherence-0, thread=PartitionedCacheWorker:0x0000:5): Changed key=key-4 to value=VALUE-4 Note : Depending upon the data distribution, not all members show the messages.","title":"Installation Steps"},{"location":"samples/operator/logging/custom-logs/#verify-kibana-logs","text":"Access Kibana Access Kibana using the instructions here . Create an Index Pattern In Kibana, open Management and click Index Patterns . Click Create index pattern . Set the name to cloud-* . This will show that this matches 1 index, such as cloud-2019.04.29 . Click Next Step and select @timestamp in the Time Filter field name drop-down menu and click Create index pattern . Note: It takes approximately 5 minutes for the data to reach the Elasticsearch instance. View data from the cloud-* index pattern Open Discover and select cloud-* in the drop-down list which shows coherence-cluster-* . When you click Refresh , you can see the data from the custom logger.","title":"Verify Kibana Logs"},{"location":"samples/operator/logging/custom-logs/#uninstall-the-charts","text":"To delete chart installed in this sample, use the following command: $ helm delete storage --purge Note : If you are using Kubernetes version 1.13.0 or oler version, then you cannot delete the pods. This is a known issue and you need to add the options --force --grace-period=0 to force delete the pods. Refer to https://github.com/kubernetes/kubernetes/issues/45688 . Before starting another sample, ensure that all the pods are removed from previous sample. If you want to remove the coherence-operator , then use the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/operator/logging/log-capture/","text":"Enable log capture to view logs in Kiabana The Coherence Operator manages data logging through the Elasticsearch, Fluentd and Kibana (EFK) stack. The log capture feature is disabled by default. This sample shows how to enable log capture and access the Kibana user interface (UI) to view the captured logs. Return to Logging samples / Return to Coherence Operator samples / Return to samples Installation Steps When you install coherence-operator and coherence charts, you must specify the following option for helm , for both charts, to ensure that the EFK stack (Elasitcsearch, Fluentd and Kibana) is installed and correctly configured. --set logCaptureEnabled=true Install Coherence Operator Use the following command to install coherence-operator with log capture enabled: Note: If you have already installed the coherence-operator without log capture enabled, you must first delete it using helm delete coherence-operator --purge command and then continue. bash $ helm install \\ --namespace sample-coherence-ns \\ --set imagePullSecrets=sample-coherence-secret \\ --name coherence-operator \\ --set logCaptureEnabled=true \\ --set \"targetNamespaces={sample-coherence-ns}\" \\ coherence/coherence-operator After the installation completes, list the pods: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-7f596c6796-nns9n 2/2 Running 0 41s elasticsearch-5b5474865c-86888 1/1 Running 0 41s kibana-f6955c4b9-4ndsh 1/1 Running 0 41s In the output, you can see the pods for Elasticsearch and Kibana along with the operator. Install Coherence cluster with log capture enabled: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=storage-tier-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set logCaptureEnabled=true \\ coherence/coherence After the installation completes, list the pods: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-7f596c6796-nns9n 2/2 Running 0 22m elasticsearch-5b5474865c-86888 1/1 Running 0 22m kibana-f6955c4b9-4ndsh 1/1 Running 0 22m storage-coherence-0 2/2 Running 0 17m storage-coherence-1 2/2 Running 0 16m storage-coherence-2 2/2 Running 0 16m The coherence-operator and all the coherence pods have two containers. To view the logs, you must specify the container coherence or fluentd . bash $ kubectl logs storage-coherence-0 -n sample-coherence-ns console Error from server (BadRequest): a container name must be specified for pod storage-coherence-0, choose one of: [coherence fluentd] or one of the init containers: [coherence-k8s-utils] bash $ kubectl logs storage-coherence-0 -n sample-coherence-ns coherence | tail -5 console 2019-04-16 01:45:18.316/92.963 Oracle Coherence GE 12.2.1.4.0 <Info> (thread=Proxy, member=1): Member 3 joined Service Proxy with senior member 1 2019-04-16 01:45:18.501/93.148 Oracle Coherence GE 12.2.1.4.0 <Info> (thread=Proxy:MetricsHttpProxy, member=1): Member 3 joined Service MetricsHttpProxy with senior member 1 2019-04-16 01:45:19.281/93.928 Oracle Coherence GE 12.2.1.4.0 <Info> (thread=DistributedCache:PartitionedCache, member=1): Transferring 44B of backup[1] for PartitionSet{172..215} to member 3 2019-04-16 01:45:19.437/94.084 Oracle Coherence GE 12.2.1.4.0 <Info> (thread=DistributedCache:PartitionedCache, member=1): Transferring primary PartitionSet{128..171} to member 3 requesting 44 2019-04-16 01:45:19.650/94.297 Oracle Coherence GE 12.2.1.4.0 <Info> (thread=DistributedCache:PartitionedCache, member=1): Partition ownership has stabilized with 3 nodes Access Kibana Run the port-forward-kibana.sh script in the common directory to view the log messages. Start the port-forward: bash $ ./port-forward-kibana.sh sample-coherence-ns console Forwarding from 127.0.0.1:5601 -> 5601 Forwarding from [::1]:5601 -> 5601 1. Access Kibana using the following URL: http://127.0.0.1:5601/ Note: It takes approximately 5 minutes for the data to reach the Elasticsearch instance. Default Kibana Dashboards There are a number of Kibana dashboards created via the import process. | Dashboard | Options | Description | |--------------------|------------------------|--------------------------------------------------------------------------------------| | Coherence Operator | All Messages | Shows all Coherence Operator messages | | Coherence Cluster | All Messages | Shows all messages | | Coherence Cluster | Errors and Warnings | Shows only errors and warnings | | Coherence Cluster | Persistence | Shows partition related messages | | Coherence Cluster | Message Sources | Allows visualization of messages via the message source (Thread) | | Coherence Cluster | Configuration Messages | Shows configuration related messages | | Coherence Cluster | Network | Shows network related messages, such as communication delays and TCP ring disconnects | Default Queries There are many queries related to common Coherence messages, warnings, and errors that are loaded and can be accessed via the Discover side-bar. Uninstalling the Charts Use the following commands to delete the chart installed in this sample: $ helm delete storage --purge Note : If you are using Kubernetes 1.13.0 or older version, you cannot delete the pods. This is a known issue and you need to add the options --force --grace-period=0 to force delete the pods. Refer to https://github.com/kubernetes/kubernetes/issues/45688 . Before starting another sample, ensure that all the pods are deleted from the previous sample. If you want to remove the coherence-operator , then use the helm delete command.","title":"Enable log capture to view logs in Kiabana"},{"location":"samples/operator/logging/log-capture/#enable-log-capture-to-view-logs-in-kiabana","text":"The Coherence Operator manages data logging through the Elasticsearch, Fluentd and Kibana (EFK) stack. The log capture feature is disabled by default. This sample shows how to enable log capture and access the Kibana user interface (UI) to view the captured logs. Return to Logging samples / Return to Coherence Operator samples / Return to samples","title":"Enable log capture to view logs in Kiabana"},{"location":"samples/operator/logging/log-capture/#installation-steps","text":"When you install coherence-operator and coherence charts, you must specify the following option for helm , for both charts, to ensure that the EFK stack (Elasitcsearch, Fluentd and Kibana) is installed and correctly configured. --set logCaptureEnabled=true Install Coherence Operator Use the following command to install coherence-operator with log capture enabled: Note: If you have already installed the coherence-operator without log capture enabled, you must first delete it using helm delete coherence-operator --purge command and then continue. bash $ helm install \\ --namespace sample-coherence-ns \\ --set imagePullSecrets=sample-coherence-secret \\ --name coherence-operator \\ --set logCaptureEnabled=true \\ --set \"targetNamespaces={sample-coherence-ns}\" \\ coherence/coherence-operator After the installation completes, list the pods: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-7f596c6796-nns9n 2/2 Running 0 41s elasticsearch-5b5474865c-86888 1/1 Running 0 41s kibana-f6955c4b9-4ndsh 1/1 Running 0 41s In the output, you can see the pods for Elasticsearch and Kibana along with the operator. Install Coherence cluster with log capture enabled: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=storage-tier-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set logCaptureEnabled=true \\ coherence/coherence After the installation completes, list the pods: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-7f596c6796-nns9n 2/2 Running 0 22m elasticsearch-5b5474865c-86888 1/1 Running 0 22m kibana-f6955c4b9-4ndsh 1/1 Running 0 22m storage-coherence-0 2/2 Running 0 17m storage-coherence-1 2/2 Running 0 16m storage-coherence-2 2/2 Running 0 16m The coherence-operator and all the coherence pods have two containers. To view the logs, you must specify the container coherence or fluentd . bash $ kubectl logs storage-coherence-0 -n sample-coherence-ns console Error from server (BadRequest): a container name must be specified for pod storage-coherence-0, choose one of: [coherence fluentd] or one of the init containers: [coherence-k8s-utils] bash $ kubectl logs storage-coherence-0 -n sample-coherence-ns coherence | tail -5 console 2019-04-16 01:45:18.316/92.963 Oracle Coherence GE 12.2.1.4.0 <Info> (thread=Proxy, member=1): Member 3 joined Service Proxy with senior member 1 2019-04-16 01:45:18.501/93.148 Oracle Coherence GE 12.2.1.4.0 <Info> (thread=Proxy:MetricsHttpProxy, member=1): Member 3 joined Service MetricsHttpProxy with senior member 1 2019-04-16 01:45:19.281/93.928 Oracle Coherence GE 12.2.1.4.0 <Info> (thread=DistributedCache:PartitionedCache, member=1): Transferring 44B of backup[1] for PartitionSet{172..215} to member 3 2019-04-16 01:45:19.437/94.084 Oracle Coherence GE 12.2.1.4.0 <Info> (thread=DistributedCache:PartitionedCache, member=1): Transferring primary PartitionSet{128..171} to member 3 requesting 44 2019-04-16 01:45:19.650/94.297 Oracle Coherence GE 12.2.1.4.0 <Info> (thread=DistributedCache:PartitionedCache, member=1): Partition ownership has stabilized with 3 nodes","title":"Installation Steps"},{"location":"samples/operator/logging/log-capture/#access-kibana","text":"Run the port-forward-kibana.sh script in the common directory to view the log messages. Start the port-forward: bash $ ./port-forward-kibana.sh sample-coherence-ns console Forwarding from 127.0.0.1:5601 -> 5601 Forwarding from [::1]:5601 -> 5601 1. Access Kibana using the following URL: http://127.0.0.1:5601/ Note: It takes approximately 5 minutes for the data to reach the Elasticsearch instance.","title":"Access Kibana"},{"location":"samples/operator/logging/log-capture/#default-kibana-dashboards","text":"There are a number of Kibana dashboards created via the import process. | Dashboard | Options | Description | |--------------------|------------------------|--------------------------------------------------------------------------------------| | Coherence Operator | All Messages | Shows all Coherence Operator messages | | Coherence Cluster | All Messages | Shows all messages | | Coherence Cluster | Errors and Warnings | Shows only errors and warnings | | Coherence Cluster | Persistence | Shows partition related messages | | Coherence Cluster | Message Sources | Allows visualization of messages via the message source (Thread) | | Coherence Cluster | Configuration Messages | Shows configuration related messages | | Coherence Cluster | Network | Shows network related messages, such as communication delays and TCP ring disconnects |","title":"Default Kibana Dashboards"},{"location":"samples/operator/logging/log-capture/#default-queries","text":"There are many queries related to common Coherence messages, warnings, and errors that are loaded and can be accessed via the Discover side-bar.","title":"Default Queries"},{"location":"samples/operator/logging/log-capture/#uninstalling-the-charts","text":"Use the following commands to delete the chart installed in this sample: $ helm delete storage --purge Note : If you are using Kubernetes 1.13.0 or older version, you cannot delete the pods. This is a known issue and you need to add the options --force --grace-period=0 to force delete the pods. Refer to https://github.com/kubernetes/kubernetes/issues/45688 . Before starting another sample, ensure that all the pods are deleted from the previous sample. If you want to remove the coherence-operator , then use the helm delete command.","title":"Uninstalling the Charts"},{"location":"samples/operator/logging/own-elasticsearch/","text":"Push Logs to Your Elasticsearch Instance The Coherence Operator manages data logging through the ElasticSearch, Fluentd, and Kibana (EFK) stack. This sample explains how to make Fluentd to push data to your own Elasticsearch instance. Return to Logging samples / Return to Coherence Operator samples / Return to samples This will ensure that Elasticsearch and Kibana will be installed and configured. Installation Steps Install the Coherence Operator You must install the Coherence Operator using the instructions that enable log capture and point to the host and port of your existing Elasticsearch instance. Note : If you have a running Coherence Operator, you should uninstall using the command helm delete coherence-operator --purge . Ensure you set the following: elasticsearchEndpoint.host to your Elasticsearch host. elasticsearchEndpoint.port to your Elasticsearch port. Note : If your Elasticsearch host requires username and password, set the following: elasticsearchEndpoint.user to your Elasticsearch username. elasticsearchEndpoint.password to your elasticsearch password. bash $ helm install \\ --namespace sample-coherence-ns \\ --set imagePullSecrets=sample-coherence-secret \\ --name coherence-operator \\ --set logCaptureEnabled=true \\ --set elasticsearchEndpoint.host=my-elastic-host \\ --set elasticsearchEndpoint.port=my-elastic-port \\ --set \"targetNamespaces={sample-coherence-ns}\" \\ coherence/coherence-operator Verify that the Elasticsearch endpoint is set. bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-856d5f8544-kgxgd 2/2 Running 0 8m bash $ kubectl logs coherence-operator-856d5f8544-kgxgd -n sample-coherence-ns -c fluentd | grep -A3 'match coherence-operator' ```console @type elasticsearch host \"my-elastic-host\" port my-elastic-port ``` The host and port value must match the values you supplied in the helm install command. Install the Coherence cluster The following additional options are set: --set logCaptureEnabled=true - This uses the configuration of the operator for the Elasticsearch endpoint for Fluentd. Note : The Coherence Operator provides the Elasticsearch host and port values to install Coherence. bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=storage-tier-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set logCaptureEnabled=true \\ coherence/coherence After the installation completes, list the pods: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-7f596c6796-nns9n 2/2 Running 0 22m storage-coherence-0 2/2 Running 0 17m storage-coherence-1 2/2 Running 0 16m storage-coherence-2 2/2 Running 0 16m The coherence-operator and all the coherence pods have two containers. The second container is for Fluentd. bash $ kubectl logs storage-coherence-0 -n sample-coherence-ns -c fluentd | grep -A3 'match coherence-cluster' <match coherence-cluster> console @type elasticsearch host \"my-elastic-host\" port my-elastic-port The host and port values must match the values you provided to install the coherence-operator . Uninstall the Charts Use the following commands to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous sample. If you want to remove the coherence-operator , then use the helm delete command.","title":"Push Logs to Your Elasticsearch Instance"},{"location":"samples/operator/logging/own-elasticsearch/#push-logs-to-your-elasticsearch-instance","text":"The Coherence Operator manages data logging through the ElasticSearch, Fluentd, and Kibana (EFK) stack. This sample explains how to make Fluentd to push data to your own Elasticsearch instance. Return to Logging samples / Return to Coherence Operator samples / Return to samples This will ensure that Elasticsearch and Kibana will be installed and configured.","title":"Push Logs to Your Elasticsearch Instance"},{"location":"samples/operator/logging/own-elasticsearch/#installation-steps","text":"Install the Coherence Operator You must install the Coherence Operator using the instructions that enable log capture and point to the host and port of your existing Elasticsearch instance. Note : If you have a running Coherence Operator, you should uninstall using the command helm delete coherence-operator --purge . Ensure you set the following: elasticsearchEndpoint.host to your Elasticsearch host. elasticsearchEndpoint.port to your Elasticsearch port. Note : If your Elasticsearch host requires username and password, set the following: elasticsearchEndpoint.user to your Elasticsearch username. elasticsearchEndpoint.password to your elasticsearch password. bash $ helm install \\ --namespace sample-coherence-ns \\ --set imagePullSecrets=sample-coherence-secret \\ --name coherence-operator \\ --set logCaptureEnabled=true \\ --set elasticsearchEndpoint.host=my-elastic-host \\ --set elasticsearchEndpoint.port=my-elastic-port \\ --set \"targetNamespaces={sample-coherence-ns}\" \\ coherence/coherence-operator Verify that the Elasticsearch endpoint is set. bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-856d5f8544-kgxgd 2/2 Running 0 8m bash $ kubectl logs coherence-operator-856d5f8544-kgxgd -n sample-coherence-ns -c fluentd | grep -A3 'match coherence-operator' ```console @type elasticsearch host \"my-elastic-host\" port my-elastic-port ``` The host and port value must match the values you supplied in the helm install command. Install the Coherence cluster The following additional options are set: --set logCaptureEnabled=true - This uses the configuration of the operator for the Elasticsearch endpoint for Fluentd. Note : The Coherence Operator provides the Elasticsearch host and port values to install Coherence. bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=storage-tier-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set logCaptureEnabled=true \\ coherence/coherence After the installation completes, list the pods: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-7f596c6796-nns9n 2/2 Running 0 22m storage-coherence-0 2/2 Running 0 17m storage-coherence-1 2/2 Running 0 16m storage-coherence-2 2/2 Running 0 16m The coherence-operator and all the coherence pods have two containers. The second container is for Fluentd. bash $ kubectl logs storage-coherence-0 -n sample-coherence-ns -c fluentd | grep -A3 'match coherence-cluster' <match coherence-cluster> console @type elasticsearch host \"my-elastic-host\" port my-elastic-port The host and port values must match the values you provided to install the coherence-operator .","title":"Installation Steps"},{"location":"samples/operator/logging/own-elasticsearch/#uninstall-the-charts","text":"Use the following commands to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous sample. If you want to remove the coherence-operator , then use the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/operator/metrics/","text":"Metrics Note: Use of Prometheus and Grafana is only available when using the operator with Coherence 12.2.1.4. Table of Contents Deploy the operator with Prometheus enabled and view in Grafana Enable SSL for Metrics Scrape metrics from your own Prometheus instance Return to Coherence Operator samples / Return to samples","title":"Metrics"},{"location":"samples/operator/metrics/#metrics","text":"Note: Use of Prometheus and Grafana is only available when using the operator with Coherence 12.2.1.4.","title":"Metrics"},{"location":"samples/operator/metrics/#table-of-contents","text":"Deploy the operator with Prometheus enabled and view in Grafana Enable SSL for Metrics Scrape metrics from your own Prometheus instance Return to Coherence Operator samples / Return to samples","title":"Table of Contents"},{"location":"samples/operator/metrics/enable-metrics/","text":"Deploy Coherence Operator with Prometheus Enabled and View Metrics in Grafana The Coherence Operator includes the Prometheus Operator as an optional subchart named prometheusoperator . This sample shows you how to configure the Prometheus Operator and monitor Coherence services through Grafana. Note : Use of Prometheus and Grafana is available only when using the operator with Oracle Coherence 12.2.1.4.0 version. Return to Metrics samples / Return to Coherence Operator samples / Return to samples Installation Steps Install Coherence Operator When you install the coherence-operator chart, you must specify the following additional set value for helm to install subchart prometheusoperator : bash --set prometheusoperator.enabled=true All coherence charts installed in coherence-operator targetNamespaces are monitored by Prometheus. The ServiceMonitor <releasename>-coherence-service-monitor configures Prometheus to scrape all components of coherence-service . Use the following command to install coherence-operator with prometheusoperator enabled: bash $ helm install \\ --namespace sample-coherence-ns \\ --set imagePullSecrets=sample-coherence-secret \\ --name coherence-operator \\ --set prometheusoperator.enabled=true \\ --set prometheusoperator.prometheusOperator.createCustomResource=false \\ --set \"targetNamespaces={sample-coherence-ns}\" \\ coherence/coherence-operator After the installation completes, list the pods: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-66f9bb7b75-q2w8t 1/1 Running 0 34s coherence-operator-grafana-769bb4d5cb-xwm9w 3/3 Running 0 35s coherence-operator-kube-state-metrics-5d5f6855bd-hh7cv 1/1 Running 0 35s coherence-operator-prometh-operator-58bd58ddfd-rldqk 1/1 Running 0 34s coherence-operator-prometheus-node-exporter-n9ls7 1/1 Running 0 35s prometheus-coherence-operator-prometh-prometheus-0 3/3 Running 1 21s You can see grafana and other promethues related pods along with the coherence-operator pod. Install the Coherence cluster with prometheusoperator enabled: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=storage-tier-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=true \\ --set \"targetNamespaces={sample-coherence-ns}\" \\ coherence/coherence Note: If the Coherence Operator chart version does not have the default Coherence image as 12.2.1.4, then you need to set this using --set coherence.image=your-12.2.1.4-image . After the installation completes, list the pods: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-66f9bb7b75-q2w8t 1/1 Running 0 9m coherence-operator-grafana-769bb4d5cb-xwm9w 3/3 Running 0 9m coherence-operator-kube-state-metrics-5d5f6855bd-hh7cv 1/1 Running 0 9m coherence-operator-prometh-operator-58bd58ddfd-rldqk 1/1 Running 0 9m coherence-operator-prometheus-node-exporter-n9ls7 1/1 Running 0 9m prometheus-coherence-operator-prometh-prometheus-0 3/3 Running 1 9m storage-coherence-0 1/1 Running 0 3m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 1m Access Grafana Run the port-forward-grafana.sh script in the common directory to view metrics. Start the port-forward: ```bash $ ./port-forward-grafana.sh sample-coherence-ns Forwarding from 127.0.0.1:3000 -> 3000 Forwarding from [::1]:3000 -> 3000 ``` Access Grafana using the following URL: http://127.0.0.1:3000/d/coh-main/coherence-dashboard-main Username: admin Password: prom-operator Default Dashboards There are a number of dashboard created via the import process: Coherence Dashboard main for inspecting coherence clusters Coherence Cluster Members Summary and Details Coherence Cluster Members Machines Summary Coherence Cache Summary and Details Coherence Services Summary and Details Coherence Proxy Servers Summary and Details Coherence Elastic Data Summary Coherence Cache Persistence Summary Coherence Http Servers Summary Uninstall the Charts Use the following command to delete the charts installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous sample. If you want to remove the coherence-operator , then use the helm delete command. Troubleshooting Helm install of coherence-operator fails creating a custom resource definition (CRD) See Prometheus Operator: helm fails to create CRDs to manually install the Prometheus Operator CRDs and then install the coherence-operator chart with these additional set values. --set prometheusoperator.enabled=true --set prometheusoperator.prometheusOperator.createCustomResource=false No datasource found in Grafana On the Grafana home page, click Create your first data source to create a datasource manually and fill in these fields: Name: Prometheus HTTP URL: http://{release-name}-prometheus:9090/ CLick Save & Test .","title":"Deploy Coherence Operator with Prometheus Enabled and View Metrics in Grafana"},{"location":"samples/operator/metrics/enable-metrics/#deploy-coherence-operator-with-prometheus-enabled-and-view-metrics-in-grafana","text":"The Coherence Operator includes the Prometheus Operator as an optional subchart named prometheusoperator . This sample shows you how to configure the Prometheus Operator and monitor Coherence services through Grafana. Note : Use of Prometheus and Grafana is available only when using the operator with Oracle Coherence 12.2.1.4.0 version. Return to Metrics samples / Return to Coherence Operator samples / Return to samples","title":"Deploy Coherence Operator with Prometheus Enabled and View Metrics in Grafana"},{"location":"samples/operator/metrics/enable-metrics/#installation-steps","text":"Install Coherence Operator When you install the coherence-operator chart, you must specify the following additional set value for helm to install subchart prometheusoperator : bash --set prometheusoperator.enabled=true All coherence charts installed in coherence-operator targetNamespaces are monitored by Prometheus. The ServiceMonitor <releasename>-coherence-service-monitor configures Prometheus to scrape all components of coherence-service . Use the following command to install coherence-operator with prometheusoperator enabled: bash $ helm install \\ --namespace sample-coherence-ns \\ --set imagePullSecrets=sample-coherence-secret \\ --name coherence-operator \\ --set prometheusoperator.enabled=true \\ --set prometheusoperator.prometheusOperator.createCustomResource=false \\ --set \"targetNamespaces={sample-coherence-ns}\" \\ coherence/coherence-operator After the installation completes, list the pods: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-66f9bb7b75-q2w8t 1/1 Running 0 34s coherence-operator-grafana-769bb4d5cb-xwm9w 3/3 Running 0 35s coherence-operator-kube-state-metrics-5d5f6855bd-hh7cv 1/1 Running 0 35s coherence-operator-prometh-operator-58bd58ddfd-rldqk 1/1 Running 0 34s coherence-operator-prometheus-node-exporter-n9ls7 1/1 Running 0 35s prometheus-coherence-operator-prometh-prometheus-0 3/3 Running 1 21s You can see grafana and other promethues related pods along with the coherence-operator pod. Install the Coherence cluster with prometheusoperator enabled: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=storage-tier-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=true \\ --set \"targetNamespaces={sample-coherence-ns}\" \\ coherence/coherence Note: If the Coherence Operator chart version does not have the default Coherence image as 12.2.1.4, then you need to set this using --set coherence.image=your-12.2.1.4-image . After the installation completes, list the pods: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-66f9bb7b75-q2w8t 1/1 Running 0 9m coherence-operator-grafana-769bb4d5cb-xwm9w 3/3 Running 0 9m coherence-operator-kube-state-metrics-5d5f6855bd-hh7cv 1/1 Running 0 9m coherence-operator-prometh-operator-58bd58ddfd-rldqk 1/1 Running 0 9m coherence-operator-prometheus-node-exporter-n9ls7 1/1 Running 0 9m prometheus-coherence-operator-prometh-prometheus-0 3/3 Running 1 9m storage-coherence-0 1/1 Running 0 3m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 1m","title":"Installation Steps"},{"location":"samples/operator/metrics/enable-metrics/#access-grafana","text":"Run the port-forward-grafana.sh script in the common directory to view metrics. Start the port-forward: ```bash $ ./port-forward-grafana.sh sample-coherence-ns Forwarding from 127.0.0.1:3000 -> 3000 Forwarding from [::1]:3000 -> 3000 ``` Access Grafana using the following URL: http://127.0.0.1:3000/d/coh-main/coherence-dashboard-main Username: admin Password: prom-operator","title":"Access Grafana"},{"location":"samples/operator/metrics/enable-metrics/#default-dashboards","text":"There are a number of dashboard created via the import process: Coherence Dashboard main for inspecting coherence clusters Coherence Cluster Members Summary and Details Coherence Cluster Members Machines Summary Coherence Cache Summary and Details Coherence Services Summary and Details Coherence Proxy Servers Summary and Details Coherence Elastic Data Summary Coherence Cache Persistence Summary Coherence Http Servers Summary","title":"Default Dashboards"},{"location":"samples/operator/metrics/enable-metrics/#uninstall-the-charts","text":"Use the following command to delete the charts installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous sample. If you want to remove the coherence-operator , then use the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/operator/metrics/enable-metrics/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"samples/operator/metrics/enable-metrics/#helm-install-of-coherence-operator-fails-creating-a-custom-resource-definition-crd","text":"See Prometheus Operator: helm fails to create CRDs to manually install the Prometheus Operator CRDs and then install the coherence-operator chart with these additional set values. --set prometheusoperator.enabled=true --set prometheusoperator.prometheusOperator.createCustomResource=false","title":"Helm install of coherence-operator fails creating a custom resource definition (CRD)"},{"location":"samples/operator/metrics/enable-metrics/#no-datasource-found-in-grafana","text":"On the Grafana home page, click Create your first data source to create a datasource manually and fill in these fields: Name: Prometheus HTTP URL: http://{release-name}-prometheus:9090/ CLick Save & Test .","title":"No datasource found in Grafana"},{"location":"samples/operator/metrics/own-prometheus/","text":"Scrape Metrics from Your Prometheus Instance You can scrape the metrics from your own Prometheus Operator instance rather than using the prometheusopeartor subchart included with coherence-operator . This sample shows you how to scrape metrics from your own Prometheus instance. Note: Use of Prometheus and Grafana is available only when using the operator with Oracle Coherence 12.2.1.4.0 version. Return to Metrics samples / Return to Coherence Operator samples / Return to samples Installation Steps Install Coherence Operator When you install the coherence-operator , you must ensure to specify --set prometheusoperator.enabled=false or leave out the option completely, which also defaults to false. bash --set prometheusoperator.enabled=false Use the following command to install coherence-operator with prometheusoperator enabled: bash $ helm install \\ --namespace sample-coherence-ns \\ --set imagePullSecrets=sample-coherence-secret \\ --name coherence-operator \\ --set prometheusoperator.enabled=false \\ coherence/coherence-operator After the installation completes, list the pods: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-665489854f-jr7qj 1/1 Running 0 9s There is only a single coherence-operator pod. Install the Coherence cluster bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=storage-tier-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set \"targetNamespaces={sample-coherence-ns}\" \\ coherence/coherence After the installation completes, list the pods: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-665489854f-jr7qj 1/1 Running 0 3m storage-coherence-0 1/1 Running 0 1m storage-coherence-1 1/1 Running 0 1m storage-coherence-2 0/1 Running 0 22s Configure Your Prometheus Operator to Scrape Coherence Pods Refer Prometheus Operator documentation for information about how to configure and deploy a service monitor for your Prometheus Operator installation. This section describes only the service monitor configuration as it relates to the Coherence Helm chart. coherence-service-monitor.yaml fragment: ... spec: selector: matchLabels: component: \"coherence-service\" ... endpoints: - port: 9612 If the parameter service.metricsHttpPort is set when installing the Coherence Helm chart, replace port: 9612 with the new value. If the Coherence Helm chart parameter store.metrics.ssl.enabled is true , add endpoints.scheme value of https to coherence-service-monitor.yaml fragment. There are a number of Coherence Grafana dashboards bundled in the Coherence Operator Helm chart under dashboards folder. While Grafana have to be configured to the location of your Prometheus datasource, you can take advantage of these Coherence dashboards by extracting them from the Coherence Operator Helm chart. Uninstall the Charts Use the following commands to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous sample. If you want to remove the coherence-operator , use the helm delete command.","title":"Scrape Metrics from Your Prometheus Instance"},{"location":"samples/operator/metrics/own-prometheus/#scrape-metrics-from-your-prometheus-instance","text":"You can scrape the metrics from your own Prometheus Operator instance rather than using the prometheusopeartor subchart included with coherence-operator . This sample shows you how to scrape metrics from your own Prometheus instance. Note: Use of Prometheus and Grafana is available only when using the operator with Oracle Coherence 12.2.1.4.0 version. Return to Metrics samples / Return to Coherence Operator samples / Return to samples","title":"Scrape Metrics from Your Prometheus Instance"},{"location":"samples/operator/metrics/own-prometheus/#installation-steps","text":"Install Coherence Operator When you install the coherence-operator , you must ensure to specify --set prometheusoperator.enabled=false or leave out the option completely, which also defaults to false. bash --set prometheusoperator.enabled=false Use the following command to install coherence-operator with prometheusoperator enabled: bash $ helm install \\ --namespace sample-coherence-ns \\ --set imagePullSecrets=sample-coherence-secret \\ --name coherence-operator \\ --set prometheusoperator.enabled=false \\ coherence/coherence-operator After the installation completes, list the pods: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-665489854f-jr7qj 1/1 Running 0 9s There is only a single coherence-operator pod. Install the Coherence cluster bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=storage-tier-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set \"targetNamespaces={sample-coherence-ns}\" \\ coherence/coherence After the installation completes, list the pods: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-665489854f-jr7qj 1/1 Running 0 3m storage-coherence-0 1/1 Running 0 1m storage-coherence-1 1/1 Running 0 1m storage-coherence-2 0/1 Running 0 22s","title":"Installation Steps"},{"location":"samples/operator/metrics/own-prometheus/#configure-your-prometheus-operator-to-scrape-coherence-pods","text":"Refer Prometheus Operator documentation for information about how to configure and deploy a service monitor for your Prometheus Operator installation. This section describes only the service monitor configuration as it relates to the Coherence Helm chart. coherence-service-monitor.yaml fragment: ... spec: selector: matchLabels: component: \"coherence-service\" ... endpoints: - port: 9612 If the parameter service.metricsHttpPort is set when installing the Coherence Helm chart, replace port: 9612 with the new value. If the Coherence Helm chart parameter store.metrics.ssl.enabled is true , add endpoints.scheme value of https to coherence-service-monitor.yaml fragment. There are a number of Coherence Grafana dashboards bundled in the Coherence Operator Helm chart under dashboards folder. While Grafana have to be configured to the location of your Prometheus datasource, you can take advantage of these Coherence dashboards by extracting them from the Coherence Operator Helm chart.","title":"Configure Your Prometheus Operator to Scrape Coherence Pods"},{"location":"samples/operator/metrics/own-prometheus/#uninstall-the-charts","text":"Use the following commands to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous sample. If you want to remove the coherence-operator , use the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/operator/metrics/ssl/","text":"Enable SSL for Metrics By default when metrics are enabled via configuring Prometheus Operator as described here , metrics utilize standard HTTP. This sample shows you how to enable SSL for metrics capture only when configuring an external Prometheus to scrape the metrics. Note: It is not supported to enable SSL for metrics using the out of the box Prometheus installed with the Coherence Operator. Note: Use of Prometheus and Grafana is available only when using the operator with Oracle Coherence 12.2.1.4.0 version. Return to Metrics samples / Return to Coherence Operator samples / Return to samples Sample files src/main/java/com/oracle/coherence/examples/SampleMetricsClient.java - Client connect to management over REST through SSL src/main/java/com/oracle/coherence/examples/HttpSSLHelper.java - Client connect to management over REST through SSL Prerequisites Ensure you have already installed the Coherence Operator using the instructions here . Note: You do not need to enable metrics capture using --set prometheusoperator.enabled=true as an external Prometheus is used. Installation Steps Change to the samples/operator/metrics/ssl directory and ensure you have your Maven build environment set for JDK 8 and build the project: bash $ mvn clean compile Note : This sample uses self-signed certificates and simple passwords. They are for sample purposes only and must NOT use these in a production environment. You must use and generate proper certificates with appropriate passwords. Create the SSL secret: ```bash $ cd /src/main/resources/certs $ kubectl -n sample-coherence-ns create secret generic ssl-secret \\ --from-file icarus.jks \\ --from-file truststore-guardians.jks \\ --from-literal keypassword.txt=password \\ --from-literal storepassword.txt=password \\ --from-literal trustpassword.txt=secret ``` Install the Coherence cluster: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=metrics-ssl-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=true \\ --set logCaptureEnabled=false \\ --set store.metrics.ssl.enabled=true \\ --set store.metrics.ssl.secrets=ssl-secret \\ --set store.metrics.ssl.keyStore=icarus.jks \\ --set store.metrics.ssl.keyStorePasswordFile=storepassword.txt \\ --set store.metrics.ssl.keyPasswordFile=keypassword.txt \\ --set store.metrics.ssl.keyStoreType=JKS \\ --set store.metrics.ssl.trustStore=truststore-guardians.jks \\ --set store.metrics.ssl.trustStorePasswordFile=trustpassword.txt \\ --set store.metrics.ssl.trustStoreType=JKS \\ --set store.metrics.ssl.requireClientCert=true \\ --set coherence.image=your-12.2.1.4.0-Coherence-image \\ coherence/coherence Note: If your version of the Coherence Operator does not default to using Coherence 12.2.1.4.0, then you need to replace your-12.2.1.4.0-Coherence-image with an appropriate 12.2.1.4.0 image. Confirm that SSL is applied: bash $ kubectl logs storage-coherence-0 --namespace sample-coherence-ns | grep SSLSocketProviderDependencies console 2019-06-17 02:15:01.525/11.176 Oracle Coherence GE 12.2.1.4.0 <D5> (thread=main, member=1): instantiated SSLSocketProviderDependencies: SSLSocketProvider(auth=two-way, identity=SunX509/file:/coherence/certs/metrics/icarus.jks, trust=SunX509/file:/coherence/certs/metrics/truststore-guardians.jks) Start port forward for the metrics port: bash $ kubectl port-forward storage-coherence-0 -n sample-coherence-ns 9612:9612 console Forwarding from [::1]:9612 -> 9612 Forwarding from 127.0.0.1:9612 -> 9612 (Optional) Configure Prometheus. Follow the instructions here to configure Prometheus to point to your SSL endpoints. Uninstall the Charts Use the following command to delete the two charts installed in this sample: $ helm delete storage --purge Delete the secret using the following: $ kubectl delete secret ssl-secret --namespace sample-coherence-ns Before starting another sample, ensure that all the pods are removed from previous sample. If you want to remove the coherence-operator , then use the helm delete command.","title":"Enable SSL for Metrics"},{"location":"samples/operator/metrics/ssl/#enable-ssl-for-metrics","text":"By default when metrics are enabled via configuring Prometheus Operator as described here , metrics utilize standard HTTP. This sample shows you how to enable SSL for metrics capture only when configuring an external Prometheus to scrape the metrics. Note: It is not supported to enable SSL for metrics using the out of the box Prometheus installed with the Coherence Operator. Note: Use of Prometheus and Grafana is available only when using the operator with Oracle Coherence 12.2.1.4.0 version. Return to Metrics samples / Return to Coherence Operator samples / Return to samples","title":"Enable SSL for Metrics"},{"location":"samples/operator/metrics/ssl/#sample-files","text":"src/main/java/com/oracle/coherence/examples/SampleMetricsClient.java - Client connect to management over REST through SSL src/main/java/com/oracle/coherence/examples/HttpSSLHelper.java - Client connect to management over REST through SSL","title":"Sample files"},{"location":"samples/operator/metrics/ssl/#prerequisites","text":"Ensure you have already installed the Coherence Operator using the instructions here . Note: You do not need to enable metrics capture using --set prometheusoperator.enabled=true as an external Prometheus is used.","title":"Prerequisites"},{"location":"samples/operator/metrics/ssl/#installation-steps","text":"Change to the samples/operator/metrics/ssl directory and ensure you have your Maven build environment set for JDK 8 and build the project: bash $ mvn clean compile Note : This sample uses self-signed certificates and simple passwords. They are for sample purposes only and must NOT use these in a production environment. You must use and generate proper certificates with appropriate passwords. Create the SSL secret: ```bash $ cd /src/main/resources/certs $ kubectl -n sample-coherence-ns create secret generic ssl-secret \\ --from-file icarus.jks \\ --from-file truststore-guardians.jks \\ --from-literal keypassword.txt=password \\ --from-literal storepassword.txt=password \\ --from-literal trustpassword.txt=secret ``` Install the Coherence cluster: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=metrics-ssl-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=true \\ --set logCaptureEnabled=false \\ --set store.metrics.ssl.enabled=true \\ --set store.metrics.ssl.secrets=ssl-secret \\ --set store.metrics.ssl.keyStore=icarus.jks \\ --set store.metrics.ssl.keyStorePasswordFile=storepassword.txt \\ --set store.metrics.ssl.keyPasswordFile=keypassword.txt \\ --set store.metrics.ssl.keyStoreType=JKS \\ --set store.metrics.ssl.trustStore=truststore-guardians.jks \\ --set store.metrics.ssl.trustStorePasswordFile=trustpassword.txt \\ --set store.metrics.ssl.trustStoreType=JKS \\ --set store.metrics.ssl.requireClientCert=true \\ --set coherence.image=your-12.2.1.4.0-Coherence-image \\ coherence/coherence Note: If your version of the Coherence Operator does not default to using Coherence 12.2.1.4.0, then you need to replace your-12.2.1.4.0-Coherence-image with an appropriate 12.2.1.4.0 image. Confirm that SSL is applied: bash $ kubectl logs storage-coherence-0 --namespace sample-coherence-ns | grep SSLSocketProviderDependencies console 2019-06-17 02:15:01.525/11.176 Oracle Coherence GE 12.2.1.4.0 <D5> (thread=main, member=1): instantiated SSLSocketProviderDependencies: SSLSocketProvider(auth=two-way, identity=SunX509/file:/coherence/certs/metrics/icarus.jks, trust=SunX509/file:/coherence/certs/metrics/truststore-guardians.jks) Start port forward for the metrics port: bash $ kubectl port-forward storage-coherence-0 -n sample-coherence-ns 9612:9612 console Forwarding from [::1]:9612 -> 9612 Forwarding from 127.0.0.1:9612 -> 9612 (Optional) Configure Prometheus. Follow the instructions here to configure Prometheus to point to your SSL endpoints.","title":"Installation Steps"},{"location":"samples/operator/metrics/ssl/#uninstall-the-charts","text":"Use the following command to delete the two charts installed in this sample: $ helm delete storage --purge Delete the secret using the following: $ kubectl delete secret ssl-secret --namespace sample-coherence-ns Before starting another sample, ensure that all the pods are removed from previous sample. If you want to remove the coherence-operator , then use the helm delete command.","title":"Uninstall the Charts"},{"location":"samples/operator/rolling-upgrade/","text":"Change Image Version for Coherence or Application Container Using Rolling Upgrade Samples, such as Storage-disabled client in cluster via interceptor call for the creation of a sidecar docker image. Sidecar docker image provides the application classes to Kubernetes. The docker image is tagged with a version number and this version number is used by Kubernetes to enable safe rolling upgrades. You can read more about safe rolling upgrades in the Helm documentation . The safe rolling upgrade feature allows you to instruct Kubernetes, through the operator, to replace the currently installed version of your application classes with a different one. Kubernetes does not verify whether the classes are new or old. It checks whether the image can be pulled by the cluster and image has a docker tag. The operator also ensures that the replacement is done without data loss or interruption of service. This sample initially deploys version 1.0.0 of the sidecar Docker image and then does a rolling upgrade to version 2.0.0 of the sidecar image which introduces a server side Interceptor to modify data to ensure it is stored as uppercase. Return to Storage-Disabled clients samples / Return to Coherence Deployments samples / Return to samples Sample files src/main/docker-v1/Dockerfile - Dockerfile for creating sidecar which includes only v1 storage-cache-config.xml src/main/docker-v2/Dockerfile - Dockerfile for creating sidecar which includes v2 storage-cache-config.xml src/main/resources/conf/v1/storage-cache-config.xml - Cache configuration version 1 for storage-enabled tier without interceptor src/main/resources/conf/v2/storage-cache-config.xml - Cache configuration version 2 for storage-enabled tier that includes uppercase interceptor src/main/resources/client-cache-config.xml - Client configuration for extend client src/main/java/com/oracle/coherence/examples/UppercaseInterceptor.java - Interceptor that changes all entries to uppercase - version 2.0.0 Prerequisites Ensure you have already installed the Coherence Operator using the instructions here . Installation Steps Change to the samples/operator/rolling-upgrade directory and ensure you have your Maven build environment set for JDK 8 and build the project: bash $ mvn clean install -P docker-v1,docker-v2 The version 1 and version 2 Docker images are created: rolling-upgrade-sample:1.0.0 rolling-upgrade-sample:2.0.0 rolling-upgrade-sample:1.0.0 is the initial image installed in the chart. Note : If you are using a remote Kubernetes cluster, you need to push the created images to your repository accessible to that cluster. You need to prefix the image name in the helm command. Install the Coherence cluster with rolling-upgrade-sample:1.0.0 image as a sidecar: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=rolling-upgrade-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=storage-cache-config.xml \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set userArtifacts.image=rolling-upgrade-sample:1.0.0 \\ coherence/coherence After the installation completes, list the pods: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 4m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 1m All the three storage-coherence-0/1/2 pods are in running state. Port forward the proxy port on the storage-coherence-0 pod: bash $ kubectl port-forward -n sample-coherence-ns storage-coherence-0 20000:20000 Connect via CohQL commands and execute the following command: bash $ mvn exec:java Run the following CohQL commands to insert data into the cluster: ```sql insert into 'test' key('key-1') value('value-1'); insert into 'test' key('key-2') value('value-2'); select key(), value() from 'test'; Results [\"key-1\", \"value-1\"] [\"key-2\", \"value-2\"] ``` Upgrade the helm release to use the rolling-upgrade-sample:2.0.0 image. Use the following arguments to upgrade to version 2.0.0 of the image: --reuse-values - specifies to reuse all previous values associated with the release --set userArtifacts.image=rolling-upgrade-sample:2.0.0 - the new artifact version bash $ helm upgrade storage coherence/coherence \\ --namespace sample-coherence-ns \\ --reuse-values \\ --set imagePullSecrets=sample-coherence-secret \\ --set userArtifacts.image=rolling-upgrade-sample:2.0.0 Check the status of the upgrade. Use the following command to check the status of the rolling upgrade of all pods. Note : The command below will not return until upgrade of all pods is complete. bash $ kubectl rollout status sts/storage-coherence --namespace sample-coherence-ns Waiting for 1 pods to be ready... Waiting for 1 pods to be ready... waiting for statefulset rolling update to complete 1 pods at revision storage-coherence-67b75785f6... Waiting for 1 pods to be ready... Waiting for 1 pods to be ready... waiting for statefulset rolling update to complete 2 pods at revision storage-coherence-67b75785f6... Waiting for 1 pods to be ready... Waiting for 1 pods to be ready... statefulset rolling update complete 3 pods at revision storage-coherence-67b75785f6... Verify the data through CohQL commands. When the upgrade is running, you can execute the following commands in the CohQL session: sql select key(), value() from 'test'; You can note that the data always remains the same. Note : Your port-forward fails when the storage-coherence-0` pod restarts. You have to stop and restart it. In an environment where you have configured a load balancer, then the Coherence*Extend session automatically reconnects when it detects a disconnect. Add new data to confirm the interceptor is now active. ```sql insert into 'test' key('key-3') value('value-3'); select key(), value() from 'test'; Results [\"key-1\", \"value-1\"] [\"key-3\", \"VALUE-3\"] [\"key-2\", \"value-2\"] ``` You can note that the value for key-3 has been converted to uppercase which shows that the server-side interceptor is now active. Verify that the 2.0.0 image on one of the pods. Use the following command to verify that the 2.0.0 image is active: bash $ kubectl describe pod storage-coherence-0 -n sample-coherence-ns | grep rolling-upgrade console Image: rolling-upgrade-sample:2.0.0 Normal Pulled 4m59s kubelet, docker-for-desktop Container image \"rolling-upgrade-sample:2.0.0\" already present on machine The output shows that the version 2.0.0 image is now present. Uninstall the Chart Use the following command: to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous samples. If you want to remove the coherence-operator , then use helm delete command.","title":"Change Image Version for Coherence or Application Container Using Rolling Upgrade"},{"location":"samples/operator/rolling-upgrade/#change-image-version-for-coherence-or-application-container-using-rolling-upgrade","text":"Samples, such as Storage-disabled client in cluster via interceptor call for the creation of a sidecar docker image. Sidecar docker image provides the application classes to Kubernetes. The docker image is tagged with a version number and this version number is used by Kubernetes to enable safe rolling upgrades. You can read more about safe rolling upgrades in the Helm documentation . The safe rolling upgrade feature allows you to instruct Kubernetes, through the operator, to replace the currently installed version of your application classes with a different one. Kubernetes does not verify whether the classes are new or old. It checks whether the image can be pulled by the cluster and image has a docker tag. The operator also ensures that the replacement is done without data loss or interruption of service. This sample initially deploys version 1.0.0 of the sidecar Docker image and then does a rolling upgrade to version 2.0.0 of the sidecar image which introduces a server side Interceptor to modify data to ensure it is stored as uppercase. Return to Storage-Disabled clients samples / Return to Coherence Deployments samples / Return to samples","title":"Change Image Version for Coherence or Application Container Using Rolling Upgrade"},{"location":"samples/operator/rolling-upgrade/#sample-files","text":"src/main/docker-v1/Dockerfile - Dockerfile for creating sidecar which includes only v1 storage-cache-config.xml src/main/docker-v2/Dockerfile - Dockerfile for creating sidecar which includes v2 storage-cache-config.xml src/main/resources/conf/v1/storage-cache-config.xml - Cache configuration version 1 for storage-enabled tier without interceptor src/main/resources/conf/v2/storage-cache-config.xml - Cache configuration version 2 for storage-enabled tier that includes uppercase interceptor src/main/resources/client-cache-config.xml - Client configuration for extend client src/main/java/com/oracle/coherence/examples/UppercaseInterceptor.java - Interceptor that changes all entries to uppercase - version 2.0.0","title":"Sample files"},{"location":"samples/operator/rolling-upgrade/#prerequisites","text":"Ensure you have already installed the Coherence Operator using the instructions here .","title":"Prerequisites"},{"location":"samples/operator/rolling-upgrade/#installation-steps","text":"Change to the samples/operator/rolling-upgrade directory and ensure you have your Maven build environment set for JDK 8 and build the project: bash $ mvn clean install -P docker-v1,docker-v2 The version 1 and version 2 Docker images are created: rolling-upgrade-sample:1.0.0 rolling-upgrade-sample:2.0.0 rolling-upgrade-sample:1.0.0 is the initial image installed in the chart. Note : If you are using a remote Kubernetes cluster, you need to push the created images to your repository accessible to that cluster. You need to prefix the image name in the helm command. Install the Coherence cluster with rolling-upgrade-sample:1.0.0 image as a sidecar: bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=3 \\ --set cluster=rolling-upgrade-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set store.cacheConfig=storage-cache-config.xml \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set userArtifacts.image=rolling-upgrade-sample:1.0.0 \\ coherence/coherence After the installation completes, list the pods: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 4m storage-coherence-1 1/1 Running 0 2m storage-coherence-2 1/1 Running 0 1m All the three storage-coherence-0/1/2 pods are in running state. Port forward the proxy port on the storage-coherence-0 pod: bash $ kubectl port-forward -n sample-coherence-ns storage-coherence-0 20000:20000 Connect via CohQL commands and execute the following command: bash $ mvn exec:java Run the following CohQL commands to insert data into the cluster: ```sql insert into 'test' key('key-1') value('value-1'); insert into 'test' key('key-2') value('value-2'); select key(), value() from 'test'; Results [\"key-1\", \"value-1\"] [\"key-2\", \"value-2\"] ``` Upgrade the helm release to use the rolling-upgrade-sample:2.0.0 image. Use the following arguments to upgrade to version 2.0.0 of the image: --reuse-values - specifies to reuse all previous values associated with the release --set userArtifacts.image=rolling-upgrade-sample:2.0.0 - the new artifact version bash $ helm upgrade storage coherence/coherence \\ --namespace sample-coherence-ns \\ --reuse-values \\ --set imagePullSecrets=sample-coherence-secret \\ --set userArtifacts.image=rolling-upgrade-sample:2.0.0 Check the status of the upgrade. Use the following command to check the status of the rolling upgrade of all pods. Note : The command below will not return until upgrade of all pods is complete. bash $ kubectl rollout status sts/storage-coherence --namespace sample-coherence-ns Waiting for 1 pods to be ready... Waiting for 1 pods to be ready... waiting for statefulset rolling update to complete 1 pods at revision storage-coherence-67b75785f6... Waiting for 1 pods to be ready... Waiting for 1 pods to be ready... waiting for statefulset rolling update to complete 2 pods at revision storage-coherence-67b75785f6... Waiting for 1 pods to be ready... Waiting for 1 pods to be ready... statefulset rolling update complete 3 pods at revision storage-coherence-67b75785f6... Verify the data through CohQL commands. When the upgrade is running, you can execute the following commands in the CohQL session: sql select key(), value() from 'test'; You can note that the data always remains the same. Note : Your port-forward fails when the storage-coherence-0` pod restarts. You have to stop and restart it. In an environment where you have configured a load balancer, then the Coherence*Extend session automatically reconnects when it detects a disconnect. Add new data to confirm the interceptor is now active. ```sql insert into 'test' key('key-3') value('value-3'); select key(), value() from 'test'; Results [\"key-1\", \"value-1\"] [\"key-3\", \"VALUE-3\"] [\"key-2\", \"value-2\"] ``` You can note that the value for key-3 has been converted to uppercase which shows that the server-side interceptor is now active. Verify that the 2.0.0 image on one of the pods. Use the following command to verify that the 2.0.0 image is active: bash $ kubectl describe pod storage-coherence-0 -n sample-coherence-ns | grep rolling-upgrade console Image: rolling-upgrade-sample:2.0.0 Normal Pulled 4m59s kubelet, docker-for-desktop Container image \"rolling-upgrade-sample:2.0.0\" already present on machine The output shows that the version 2.0.0 image is now present.","title":"Installation Steps"},{"location":"samples/operator/rolling-upgrade/#uninstall-the-chart","text":"Use the following command: to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous samples. If you want to remove the coherence-operator , then use helm delete command.","title":"Uninstall the Chart"},{"location":"samples/operator/scaling/","text":"Scaling a Coherence Deployment The Coherence Operator leverages Kubernetes Statefulsets to ensure that scale up and scale down operations are carried out one pod at a time. When scaling down, you must only scale down by one pod at a time and check for HAStatus to continue. This ensures that the cluster nodes have sufficient time to re-balance the cluster data to ensure no data is lost. This sample shows you how to scale up a Statefulset using kubectl and scale down one pod at a time and check HAStatus through VisualVM. Return to Coherence Operator samples / Return to samples Prerequisites Install Coherence Operator Ensure you have already installed the Coherence Operator using the instructions here . Download the JMXMP connector JAR Refer to the instructions here to download the JMXMP connector JAR. Installation Steps Install the Coherence cluster Use the following command to install the cluster with only 2 replicas and 1 MBean Server Pod, which is used to check HAStatus . bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=2 \\ --set cluster=coherence-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set store.jmx.enabled=true \\ --set store.jmx.replicas=1 \\ coherence/coherence Ensure both the pods are running: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-66f9bb7b75-hqk4l 1/1 Running 0 13m storage-coherence-0 1/1 Running 0 3m storage-coherence-1 1/1 Running 0 2m storage-coherence-jmx-54f5d779d-svh29 1/1 Running 0 2m You can see a pod prefixed with storage-coherence-jmx in the output. Add data to the cluster through the Coherence Console. Connect to the Coherence Console using the following command to create a cache: bash $ kubectl exec -it --namespace sample-coherence-ns storage-coherence-0 bash /scripts/startCoherence.sh console At the Map (?): prompt, enter cache test . This creates a cache in the service PartitionedCache . Use the following command to add 50,000 objects of size 1024 bytes, starting at index 0 and using batches of 100. bash bulkput 50000 1024 0 100 console Wed Apr 24 01:17:44 GMT 2019: adding 50000 items (starting with #0) each 1024 bytes ... Wed Apr 24 01:18:11 GMT 2019: done putting (26802ms, 1917KB/sec, 1865 items/sec) At the prompt, type size and it should show 50000. Scale to 4 nodes using kubectl command. Scale the Statefulset to 4 nodes: bash $ kubectl scale statefulsets storage-coherence --namespace sample-coherence-ns --replicas=4 bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 10m storage-coherence-1 1/1 Running 0 9m storage-coherence-2 1/1 Running 0 3m storage-coherence-3 1/1 Running 0 1m Wait for the number of coherence-storage pods to be 4 and all of them in Running status. Check the size of the cache Repeat Step 3 to access the console and to confirm that the cache size is 50000. Port forward to the MBean server pod: Use the instructions here in step 3 to port forward to the MBean server pod. Connect through VisualVM or JConsole: Use the instructions here in step 5 or 6 to connect to the cluster using VisualVM or JConsole. Check HAStatus value of PartitionedCache service. Note: VisualVM is used in this sample. Select the MBean tab in VisualVM and expand Coherence -> PartitionAssignment -> PartitionedCache -> DistributionCoordinator . You can see the value of HAStatus as NODE-SAFE and the ServiceNodeCount as 4 . Ensure the HAStatus is correct to continue further. Scale down 1 node using kubectl bash $ kubectl scale statefulsets storage-coherence --namespace sample-coherence-ns --replicas=3 Wait until the service PartitionedCache has HAStatus other than ENDANGERED and ServiceNodeCount of 3. Then, scale down the replicas to 2 nodes. bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 22m storage-coherence-1 1/1 Running 0 21m Your VisualVM MBeans tab should show the following: Check the size of the cache Access the Coherence console and confirm that the size is still 50000. Ensure you type cache test at the Map: prompt and then size . Verifying Grafana Data If you have enabled Prometheus, access Grafana using the instructions here . Verifying Kibana Logs If you have enabled log capture, access Kibana using the instructions here . Uninstall the Charts Use the following command to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous sample. If you want to remove the coherence-operator , then use the helm delete command.","title":"Scaling a Coherence Deployment"},{"location":"samples/operator/scaling/#scaling-a-coherence-deployment","text":"The Coherence Operator leverages Kubernetes Statefulsets to ensure that scale up and scale down operations are carried out one pod at a time. When scaling down, you must only scale down by one pod at a time and check for HAStatus to continue. This ensures that the cluster nodes have sufficient time to re-balance the cluster data to ensure no data is lost. This sample shows you how to scale up a Statefulset using kubectl and scale down one pod at a time and check HAStatus through VisualVM. Return to Coherence Operator samples / Return to samples","title":"Scaling a Coherence Deployment"},{"location":"samples/operator/scaling/#prerequisites","text":"Install Coherence Operator Ensure you have already installed the Coherence Operator using the instructions here . Download the JMXMP connector JAR Refer to the instructions here to download the JMXMP connector JAR.","title":"Prerequisites"},{"location":"samples/operator/scaling/#installation-steps","text":"Install the Coherence cluster Use the following command to install the cluster with only 2 replicas and 1 MBean Server Pod, which is used to check HAStatus . bash $ helm install \\ --namespace sample-coherence-ns \\ --name storage \\ --set clusterSize=2 \\ --set cluster=coherence-cluster \\ --set imagePullSecrets=sample-coherence-secret \\ --set prometheusoperator.enabled=false \\ --set logCaptureEnabled=false \\ --set store.jmx.enabled=true \\ --set store.jmx.replicas=1 \\ coherence/coherence Ensure both the pods are running: bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE coherence-operator-66f9bb7b75-hqk4l 1/1 Running 0 13m storage-coherence-0 1/1 Running 0 3m storage-coherence-1 1/1 Running 0 2m storage-coherence-jmx-54f5d779d-svh29 1/1 Running 0 2m You can see a pod prefixed with storage-coherence-jmx in the output. Add data to the cluster through the Coherence Console. Connect to the Coherence Console using the following command to create a cache: bash $ kubectl exec -it --namespace sample-coherence-ns storage-coherence-0 bash /scripts/startCoherence.sh console At the Map (?): prompt, enter cache test . This creates a cache in the service PartitionedCache . Use the following command to add 50,000 objects of size 1024 bytes, starting at index 0 and using batches of 100. bash bulkput 50000 1024 0 100 console Wed Apr 24 01:17:44 GMT 2019: adding 50000 items (starting with #0) each 1024 bytes ... Wed Apr 24 01:18:11 GMT 2019: done putting (26802ms, 1917KB/sec, 1865 items/sec) At the prompt, type size and it should show 50000. Scale to 4 nodes using kubectl command. Scale the Statefulset to 4 nodes: bash $ kubectl scale statefulsets storage-coherence --namespace sample-coherence-ns --replicas=4 bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 10m storage-coherence-1 1/1 Running 0 9m storage-coherence-2 1/1 Running 0 3m storage-coherence-3 1/1 Running 0 1m Wait for the number of coherence-storage pods to be 4 and all of them in Running status. Check the size of the cache Repeat Step 3 to access the console and to confirm that the cache size is 50000. Port forward to the MBean server pod: Use the instructions here in step 3 to port forward to the MBean server pod. Connect through VisualVM or JConsole: Use the instructions here in step 5 or 6 to connect to the cluster using VisualVM or JConsole. Check HAStatus value of PartitionedCache service. Note: VisualVM is used in this sample. Select the MBean tab in VisualVM and expand Coherence -> PartitionAssignment -> PartitionedCache -> DistributionCoordinator . You can see the value of HAStatus as NODE-SAFE and the ServiceNodeCount as 4 . Ensure the HAStatus is correct to continue further. Scale down 1 node using kubectl bash $ kubectl scale statefulsets storage-coherence --namespace sample-coherence-ns --replicas=3 Wait until the service PartitionedCache has HAStatus other than ENDANGERED and ServiceNodeCount of 3. Then, scale down the replicas to 2 nodes. bash $ kubectl get pods -n sample-coherence-ns console NAME READY STATUS RESTARTS AGE storage-coherence-0 1/1 Running 0 22m storage-coherence-1 1/1 Running 0 21m Your VisualVM MBeans tab should show the following: Check the size of the cache Access the Coherence console and confirm that the size is still 50000. Ensure you type cache test at the Map: prompt and then size .","title":"Installation Steps"},{"location":"samples/operator/scaling/#verifying-grafana-data","text":"If you have enabled Prometheus, access Grafana using the instructions here .","title":"Verifying Grafana Data"},{"location":"samples/operator/scaling/#verifying-kibana-logs","text":"If you have enabled log capture, access Kibana using the instructions here .","title":"Verifying Kibana Logs"},{"location":"samples/operator/scaling/#uninstall-the-charts","text":"Use the following command to delete the chart installed in this sample: $ helm delete storage --purge Before starting another sample, ensure that all the pods are removed from previous sample. If you want to remove the coherence-operator , then use the helm delete command.","title":"Uninstall the Charts"}]}